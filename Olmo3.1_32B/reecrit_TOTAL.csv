abstract
"We propose a kernel-based framework using a confusion density matrix to categorize uncertain examples flagged by UQ methods into OOD, boundary, and high-misclassification in-distribution classes, offering a new benchmark for evaluating UQ methods."
"Default-ERM’s max-margin inductive bias causes shortcut learning even when a stable feature perfectly predicts labels, but loss functions promoting uniform margins mitigate this by favoring the stable feature."
"We introduce adversarial invariant regularization (AIR), informed by causal reasoning, to improve the robustness of adversarial contrastive learning by enforcing style independence in representations."
"We introduce a role-based node partitioning method grounded in graph isomorphism and Weisfeiler-Leman ideas, validated on a benchmark with stochastically assigned node roles."
"We derive and interpret generic identifiability conditions for the generator of linear SDEs from their solution distributions, and validate them via simulation."
TAILOR is a meta-algorithm that adaptively selects among active learning strategies to improve label efficiency in deep learning with class-balanced sample selection.
"TIDA is a novel OSSL framework that leverages multi-granularity taxonomic relationships to improve representation learning and pseudo-label quality, achieving state-of-the-art results on standard datasets."
"We propose Cluster Information Transfer (CIT), a plug-in mechanism that improves GNN generalization under structure shift by learning invariant node representations via cluster information integration."
We derive sufficient conditions for graph pooling operators to preserve the expressive power of message-passing layers in GNNs and analyze existing operators against these criteria.
"We propose Shared Adversarial Unlearning (SAU), a method that mitigates backdoor attacks by unlearning shared adversarial examples between a backdoored and a purified model, using a bi-level optimization derived from a novel backdoor risk upper bound, achieving state-of-the-art defense performance."
"Normalization in Sharpness-Aware Minimization stabilizes the algorithm and enables movement along a manifold of minima, explaining its robustness and improved performance."
"pFedHR is a federated learning framework that addresses model heterogeneity by dynamically generating personalized models via model-matching optimization and heterogeneous model reassembly, outperforming baselines on diverse datasets."
"We propose an active learning method for semantic segmentation that queries multi-class labels for image superpixels and trains with two new loss functions to resolve class ambiguity, improving performance while reducing annotation cost."
"H-duality establishes a one-to-one correspondence between methods that minimize function values and those that minimize gradient magnitude in convex optimization, yielding new efficient algorithms and revealing symmetries not captured by existing dualities."
"We propose symmetry-breaking constraints and mixed-integer formulations for optimizing over graph neural networks with variable graphs, validated in a molecular design application."
"GALET achieves $\epsilon$-stationary points in $\tilde{\cal O}(\epsilon^{-1})$ iterations for bilevel problems with a convex Polyak-Łojasiewicz lower-level objective, matching the complexity of gradient descent for single-level smooth nonconvex optimization."
We propose an efficient algorithm for fair influence maximization under the welfare fairness notion that achieves a $1-1/e - \varepsilon$ approximation by reducing the problem to a weighted maximum coverage problem using reverse influence sampling and an unbiased estimator for fractional power.
"Training self-supervised models with early, simple visual data from infants yields the best learning outcomes, demonstrating the critical role of early visual experience properties in visual system development."
We present an efficient sampling framework that learns similarity functions across groups with limited expert feedback and provide theoretical and empirical validation.
We propose a non-parametric temporal point process model for dynamic causal mediation analysis in irregularly sampled healthcare time-series to estimate direct and indirect effects of interventions on future outcome trajectories.
We propose a post-hoc method that extracts interpretable distribution decomposition and boundary inference rules to globally explain black-box unsupervised anomaly detection models.
"We propose a max-flow-based method for differentially private counting of distinct elements under unbounded person-level sensitivity, optimizing the sensitivity bound to balance noise and approximation error."
Optimistic NPG is a computationally efficient policy optimization algorithm for online RL that achieves optimal sample complexity $\tilde{\mathcal{O}}(d^2/\epsilon^3)$ in $d$-dimensional linear MDPs and attains polynomial sample complexity with general function approximation.
We analyze how adaptive data collection inflates estimation error for low-dimensional parameters in high-dimensional linear models and propose estimators achieving near-i.i.d. error rates under certain adaptivity conditions.
"We analyze the theoretical guarantees and limitations of prompt tuning in transformer-based models for sequence-to-sequence tasks, showing universality for approximating Lipschitz functions but also demonstrating that limited-depth transformers cannot memorize certain datasets via prompts and require more prompt parameters than LoRA updates in some cases, with multi-layer transformers limited to invertible functions under certain conditions."
"We provide sufficient conditions and a method for identifiably learning a shared causal representation and graph from unpaired, multi-domain marginal distributions in a linear setting."
S³ predicts output sequence lengths to schedule generation and boost throughput by 6.49× compared to systems assuming worst-case cache size.
"Beta diffusion is a multiplicative generative model using beta-distributed transitions optimized with KL-divergence upper bounds, enabling bounded data generation and outperforming traditional diffusion methods on both synthetic and real image datasets."
"We present a model that generates 3D spatial audio from body pose and headset microphone input, validated on a new multimodal dataset with dense microphone coverage."
"We propose an experimental design method for randomized network experiments that balances bias and variance in GATE estimation under 1-hop interference by optimizing the covariance of treatment assignments, outperforming existing methods in simulations with model misspecification."
We propose multi-scale smoothing with a fine-tuned diffusion denoiser to achieve strong certified robustness and high accuracy in denoised smoothing.
"We propose online conformal prediction algorithms with formal guarantees for time series forecasting, improving coverage in real-world datasets including COVID-19 death counts and electricity demand prediction."
"Residual Alignment, characterized by equispaced high-dimensional representations, aligned singular vectors, low residual Jacobian rank, and inversely scaling top singular values in ResNets, emerges due to skip connections and persists across architectures and datasets, revealing a rigid linear progression in intermediate representations."
"We propose stabilized neural differential equations (SNDEs) to enforce arbitrary manifold constraints on neural differential equations by adding a stabilization term that asymptotically stabilizes the constraint manifold, outperforming existing methods and broadening their applicability."
"P-Flow is a fast, data-efficient, zero-shot TTS model that uses speech prompts for speaker adaptation and achieves superior performance with much less data and faster sampling than prior models."
FAMO is an O(1) space and time dynamic weighting method that balances task losses in multitask learning without storing all task gradients.
"We analytically show that both Euclidean and cosine similarity in non-contrastive SSL with linear predictors avoid representational collapse via implicit variance regularization, and propose IsoLoss to equalize convergence rates and enhance robustness without EMA."
"We present new polynomial-time algorithms for optimally training linear- and ReLU-activated neural networks, surpassing previous tractability results."
"We propose an optimal individualized treatment regime using outcome and treatment confounding bridges under proximal causal inference, theoretically and empirically demonstrating its identification and superiority over existing methods."
"We generalize and extend the k-means++ local-search algorithm to allow simultaneous multi-center swaps, achieving a 9+ε approximation while improving practical performance and implementation simplicity."
"The authors propose an automated error classification framework for ImageNet that reveals top-1 accuracy strongly predicts the proportion of error types across models, with severe error rates dropping as accuracy increases."
"We show that matrix factorization with banded matrices improves privacy-utility tradeoffs over prior methods in both federated and centralized settings, enabling practical multi-participation in federated learning and outperforming DP-SGD in most centralized scenarios."
"We present a method that uses LLMs to generate and iteratively correct PDDL domain models for planning, enabling sound planning with external planners and reducing human feedback to the domain model construction phase."
We present a mirror descent-based policy optimization framework that guarantees linear convergence with general parameterizations and improves sample complexity for neural network policies.
"We introduce antithetic termination to graph random features, theoretically reducing variance of 2-regularized Laplacian kernel estimators for any graph and empirically improving accuracy and graph diffusion approximation."
"The paper improves likelihood-based generative models by incorporating data mollification as a simple optimization continuation technique, significantly enhancing sample quality and density estimation without computational overhead."
"Decision Stacks is a generative framework that decomposes goal-conditioned policy agents into three independent generative modules for simulating observations, rewards, and actions, enabling expressive and flexible offline policy optimization in MDP and POMDP environments."
"Transfer learning with large pre-trained models significantly reduces the number of downstream training examples needed to achieve desired accuracy on diverse scientific PDE tasks, with performance gains increasing with model size."
Cyclenet introduces cycle consistency into diffusion models to achieve consistent and high-quality unpaired image-to-image translation with minimal data and resources.
"A regularized conditional Wasserstein GAN rapidly generates high-quality posterior image samples for inverse imaging problems, achieving state-of-the-art results in MRI and inpainting."
"We propose LECI, a method that uses label and environment causal independence with adversarial training to improve graph out-of-distribution generalization, outperforming prior methods on synthetic and real datasets."
"We present a list labeling data structure that leverages predictions to maintain sorted order with strong theoretical guarantees in both worst-case and stochastic error models, and demonstrate its effectiveness on real temporal datasets."
TDGP is a novel deep Gaussian process model that maintains interpretability of kernel lengthscales while effectively learning lower-dimensional input manifolds and demonstrating robust performance with increasing depth.
We propose a method to convert Pre-LN Transformers to more efficient Pre-(C)RMSNorm variants with equivalent performance and up to 10% faster training and inference.
"SparseProp is an event-based algorithm that simulates and trains sparse spiking neural networks with O(log(N)) per spike complexity, enabling exact, large-scale simulations and efficient training."
We prove that linear learners are robust to indiscriminate poisoning when class distributions are well-separated with low variance and the poisoning constraint set is small.
"Fair-ARD improves robust fairness in adversarial robustness distillation by weighting difficult classes according to their robustness difficulty, yielding higher worst-class robustness than existing methods."
"We propose a learning-based method for computing optimal equilibria in extensive-form games by reformulating them as zero-sum games, enabling scalable and convergent algorithms demonstrated on benchmark and sequential auction problems."
"COCOA improves sample efficiency in reinforcement learning by accurately measuring action contributions to future rewards using counterfactual queries, outperforming HCA and reducing bias and variance."
"We propose a manifold flow method that enforces a sparse and/or orthogonal latent basis via minimizing the off-diagonal manifold metric’s $\ell_1$-norm, achieving a more efficient and compact latent representation and lower FID scores than existing methods."
"Epidemic Learning accelerates decentralized model convergence by using randomized communication, achieving up to 1.7× faster convergence and 2.2% higher accuracy than baseline algorithms at the same communication cost."
We show that graph transformations allow standard message passing to strongly or weakly simulate state-of-the-art GNNs with competitive predictive performance on molecular datasets.
"We present a game-theoretic framework for multi-calibration that yields improved error bounds and complexity, and extends to group fairness and multi-distribution learning."
DIFT extracts image features from diffusion models that outperform supervised and weakly-supervised methods in establishing correspondences without additional supervision.
We introduce a multi-policy model that disentangles confounding bias in interactive navigation and demonstrates its effectiveness on a large-scale dataset of multi-room scenes.
"We propose a method that lets large language models actively collect relevant visual evidence based on initial hypotheses, outperforming baselines on knowledge-based VQA with better interpretability."
"We propose algorithms that leverage estimated covariance among dependent arm distributions to more efficiently identify the best arm under fixed confidence, and provide theoretical and numerical evidence of improved performance over the independent arms case."
We propose a framework that sparsifies neural dynamic models and uses them in a mixed-integer predictive controller to achieve efficient and high-performance control for complex physical tasks.
FedInit introduces a stage-wise relaxed initialization in federated learning that mitigates client drift by improving local consistency and achieves state-of-the-art performance without additional cost.
DMSB is a phase-space Schrödinger Bridge method that efficiently reconstructs stochastic trajectories from unlabeled position snapshots by enforcing marginal constraints and outperforms baselines on synthetic and real data.
"DiffTraj, a spatial-temporal diffusion probabilistic model with a Traj-UNet, generates high-fidelity, privacy-preserving GPS trajectories that retain original distributions and outperform existing methods."
"Benign overfitting in regression depends on estimator smoothness rather than dimension, requiring large derivatives, and we show this holds for spiky-smooth kernels and modified wide neural networks with high-frequency activation fluctuations, even in low dimensions."
"We introduce OBJect, a 400K synthetic dataset and 3DIT models for language-guided 3D-aware image editing that maintain physical consistency in complex scenes, with generalization to real images."
"E2PNet is the first learning-based method for event-to-point cloud registration, using a novel Event-Points-to-Tensor module to encode event data into a 2D tensor, enabling robust performance under extreme illumination and fast motion."
Neural-Q-Whittle achieves an $\mathcal{O}(1/k^{2/3})$ finite-time convergence rate for Whittle index-based Q-learning with neural network approximation in restless multi-armed bandits.
"We propose VPGen, an interpretable step-wise text-to-image generation framework using an LM for object and layout reasoning, and VPEval, an explainable evaluation framework using visual modules, improving control and correlation with human judgments in text-to-image tasks."
"FiGURe improves unsupervised node representation learning by introducing filter-based augmentations and dimensionality reduction via random Fourier features, achieving up to 4.4% higher performance than state-of-the-art methods across diverse datasets."
"We propose CF-GNN, a conformal prediction extension for GNNs that provides rigorous, topology-aware uncertainty estimates with significantly reduced prediction set sizes while maintaining target coverage."
A biologically plausible hippocampal model using theta-oscillation-gated learning acts as a Helmholtz machine to infer and generate sensory inputs and exhibits flexible path integration.
"We derive a tighter, assumption-free PAC-Bayes robust generalization bound for DNNs that reveals the gap between robust and standard generalization is not due to mismatch but mathematical factors."
"We analyze how data augmentation affects class-level accuracy in image classification, showing that it mainly harms ambiguous or fine-grained classes by biasing toward related classes, and propose class-conditional augmentation to mitigate these drops."
"We analyze the complexity of derivative-free policy optimization for structured $H_\infty$ control with static output feedback using zeroth-order oracles, providing sample complexity bounds and numerical validation."
"We introduce Star-Shaped DDPM, a generalization of DDPMs enabling diffusion modeling over diverse distributions beyond Gaussian, with efficient training and sampling algorithms and competitive performance on both constrained and image data."
"We present a style-modulated conditional GAN that generates diverse and plausible 3D shape completions from partial point clouds, using learned style codes and multi-scale penalties to avoid mode collapse without ground truth completions."
"We propose partial guidance, which leverages high-quality image properties during reverse diffusion to restore images under complex degradations without modeling the degradation process, outperforming prior diffusion-based and task-specific methods."
"If a Poincaré-style inequality holds on a subset of the state space, MCMC conditional distributions on that subset mix rapidly to the true conditional, even when global mixing is slow, with implications for sampling from mixtures of Gaussians and related problems."
"We analyze a variant of stochastic MGDA (MoDo) in multi-objective learning and show that conflict-avoiding dynamic weighting can impede achieving optimal generalization, highlighting a trade-off among optimization, generalization, and conflict avoidance."
We propose an information-theoretic multi-view clustering method using deep neural networks and Stochastic Gradient Variational Bayes that outperforms state-of-the-art algorithms on diverse datasets.
"We design a Nash incentive compatible mechanism for collaborative normal mean estimation that corrupts shared data based on individual deviation and uses minimax estimators, achieving social loss at most twice the global minimum, with extensions to high dimensions and cases attaining the global minimum."
"We provide a unified theoretical analysis of nonparametric RKHS methods for general loss functions under covariate shift, establishing sharp convergence rates for both types of shift and validating the results numerically."
"HuggingGPT is an LLM-powered agent that uses ChatGPT to orchestrate multiple AI models from Hugging Face to autonomously solve complex, multi-domain AI tasks via natural language interfaces."
"Unlearnable datasets do not reliably prevent neural networks from learning useful features or generalizing, and we demonstrate an orthogonal projection attack that enables learning from such datasets with less complexity than previous methods."
"We propose CBRS extended for class imbalance and PAPA to efficiently select interfering samples in continual learning for intrusion detection, improving performance and reducing training time on multiple benchmarks."
"TimeX is a time series consistency model that trains interpretable surrogates to faithfully mimic pretrained time series models, providing discrete attribution maps and a latent space of explanations that outperform existing methods in interpretability evaluations."
"Adam converges to $\epsilon$-stationary points with $\mathcal{O}(\epsilon^{-4})$ gradient complexity under realistic assumptions, and a variance-reduced variant achieves $\mathcal{O}(\epsilon^{-3})$ complexity."
"CoBO improves latent-space Bayesian optimization by enforcing latent space–input space distance correlation via regularization and loss weighting, reducing the optimization gap for discrete data tasks."
"We derive the first tight upper bound on Adam's iteration complexity under L-smoothness and bounded noise variance, matching known lower bounds by addressing the entanglement between momentum and adaptive learning rates."
We present new exact and approximate algorithms for Marginal MAP inference in credal networks and demonstrate their effectiveness on random and real-world benchmarks.
"SiamMAE, a Masked Autoencoder-based method for learning visual correspondence in videos by asymmetrically masking frames, learns effective object-centric representations and outperforms state-of-the-art self-supervised methods on video tasks without relying on augmentation or tracking-based pretexts."
"Adversarial training causes robust overfitting after learning rate decay due to an imbalance in the minimax game between the model and attacker, which can be mitigated by rebalancing via regularization or stronger attacks."
"InfEmbed, a slice discovery method based on clustering influence embeddings, satisfies coherence and outperforms state-of-the-art baselines on two benchmarks while aiding model debugging."
"DARTS overfits to skip connections due to parametric operations fitting training data while architecture parameters fit validation data, and we propose operation-level early stopping (OLES) to robustify DARTS without overhead."
"We reformulate entropic affinity in t-SNE as an optimal transport problem, enabling symmetric doubly stochastic normalization that improves clustering and robustness to noise, leading to a superior dimensionality reduction algorithm."
"We introduce private everlasting prediction, which achieves private PAC learning and prediction for all concept classes with finite VC dimension—including infinite-domain threshold functions—using a quadratic (in VC dimension) sample size for the initial training set, while protecting the privacy of both the training data and the adaptively chosen queries."
"We propose EDE, a value-based RL method that improves generalization by encouraging exploration of high-uncertainty states via an ensemble of Q-value distributions."
"We propose Curriculum and Structure-aware Optimal Transport (CSOT), a novel optimal transport formulation that leverages global and local data structure to robustly relabel and train with noisy labels, outperforming state-of-the-art methods."
"We propose a Gaussian Process-based method for identifying causal models in multi-context systems with independent mechanism shifts and demonstrate its effectiveness on synthetic, simulated, and real-world data."
WITRAN is a novel framework that captures diverse semantic information and accelerates recurrent computation to improve long-range time series forecasting accuracy.
MoleculeJAE is a self-supervised pretraining method that jointly encodes 2D and 3D molecular information via a diffusion-based model and achieves state-of-the-art results on most geometry-related downstream tasks.
"We propose Color Equivariant Convolutions, a novel layer that shares shape features across hues while retaining color information, improving CNN robustness to color shifts without sacrificing discriminative power."
"OptFeature introduces a hybrid-grained feature interaction selection method for deep sparse networks that efficiently selects interactions at both the feature field and value levels, improving accuracy and efficiency on large sparse datasets."
"BasisFormer is an end-to-end time series forecasting architecture that learns interpretable, data-adaptive bases via contrastive learning and uses them with cross-attention to improve forecasting accuracy over state-of-the-art methods."
Polyner is an unsupervised neural method that models the nonlinear polychromatic CT acquisition process to effectively reduce metal artifacts without requiring external training data.
"We propose Personalized Dictionary Learning (PerDL), a framework for learning disentangled global and local dictionaries from heterogeneous data, and introduce PerMA, a meta-algorithm that efficiently recovers these dictionaries and transfers learning strength across datasets."
"TIES-Merging improves model merging by addressing parameter redundancy and sign disagreement, outperforming existing methods across diverse tasks and settings."
"We present a machine-learned prediction framework for accelerating M-convex, especially laminar convex, function minimization, improving time complexity bounds in discrete optimization."
"Conservative Hamiltonian neural flow GNNs, combining Lyapunov stability, show improved robustness to adversarial attacks compared to other neural flow-based GNNs."
"We introduce QuIP, a two-step LLM post-training quantization method using adaptive rounding and random orthogonal matrix multiplication to induce incoherence, achieving viable two-bit quantization results and providing the first theoretical analysis for an LLM-scale quantization algorithm."
"FaceComposer is a latent diffusion-based generative model that unifies face synthesis, editing, and animation using a large multi-modal dataset and a temporal self-attention module, outperforming state-of-the-art methods on individual and combined tasks."
"We propose a one-shot peer review calibration method using reviewers' predictions that yields more robust rankings under noise and bias than average ratings, with error probability approaching zero as reviewer count increases."
"We propose Multimodal Neural Processes (MNPs), a principled extension of Neural Processes for multimodal uncertainty estimation that achieves state-of-the-art performance with improved robustness and efficiency."
"We show that self-supervised representation learning on synthetic images from Stable Diffusion, using a multi-positive contrastive method (StableRep), outperforms learning from real images, and with language supervision, models trained on synthetic data surpass those trained on larger real datasets."
"The paper introduces HCCRootedTreeFit, an algorithm that bounds the error of rooted tree embeddings in terms of the ultrametricity vector’s $\ell_1$-norm, differing theoretically and empirically from Gromov’s result, and demonstrates that standard hierarchical datasets yield very different tree fits than synthetic tree-like data."
"We propose an alignment-before-generation framework that encodes 3D shapes into a space aligned with images and text, enabling higher-quality and more semantically consistent 3D shape generation from 2D images or texts."
"We propose a lightweight RoBERTa-based text alignment model that outperforms or matches larger LLMs on diverse NLP tasks including entailment, similarity, and factual consistency with far fewer parameters."
"We compare the asymptotic variance of the posterior predictive distribution to the risk of the MAP estimator in random feature regression, showing agreement governed by the signal-to-noise phase transition and sample-to-dimension ratios, with numerical evidence suggesting Gaussian fluctuations."
"Training a one-hidden-layer ReLU network with gradient flow from small initialization on correlated data converges to zero loss and implicitly minimises parameter rank, with numerical experiments supporting this."
"We introduce LC-PFN, a fast and accurate Bayesian learning curve extrapolator using prior-data fitted neural networks, outperforming MCMC and enabling efficient model selection with minimal overhead."
We propose a Bayesian optimization method with a VQE-specific kernel and an EMICoRe acquisition function that efficiently infers the VQE objective using very few evaluations.
"ESPL is an efficient, end-to-end gradient-based method that learns compact, interpretable symbolic policies from scratch, outperforming neural network policies in both single-task and meta-reinforcement learning with improved data efficiency and interpretability."
"We introduce a principled framework analyzing pruning at initialization via effective nodes and paths, and propose a data-agnostic method that outperforms prior approaches and achieves up to 3.4× lower inference FLOPs."
"We propose a sequential prediction algorithm that abstains on adversarial examples and achieves VC-dimension-dependent error for non-adversarial data, even without access to the marginal distribution in some cases."
We evaluate the adversarial vulnerability of open-source large vision-language models under black-box access and demonstrate high success rates in evading safety via targeted visual inputs.
"Relational Conditional Neural Processes (RCNPs) enable scalable incorporation of equivariances into Conditional Neural Processes, improving performance on high-dimensional equivariant tasks."
"SALE learns state-action embeddings to improve reinforcement learning from low-level states, and when integrated into TD3 as TD7, it substantially outperforms TD3 on OpenAI Gym benchmarks."
"We develop a theory of feature-bagging in noisy ridge regression ensembles for equicorrelated data, showing that subsampling shifts the double-descent peak and introducing heterogeneous feature ensembling to mitigate it, while analyzing the noise trade-off in subsampling ensembles on real image feature maps."
"We propose Prune4Rel, a data pruning algorithm that selects a subset maximizing neighborhood prediction confidence to improve re-labeling accuracy and generalization in noisy datasets with label re-labeling."
The authors propose a bi-level optimization framework for fair graph distillation that improves prediction performance-fairness trade-offs by addressing bias in distilled graphs through a novel coherence metric when sensitive attributes are unavailable.
"We introduce Backward Rewarding Mechanisms (BRMs), which overcome the welfare loss inherent in Merit-based Monotone Mechanisms by inducing potential games that align creators' competition with arbitrary welfare objectives."
"We propose a dynamic codebook neural representation that compresses volumetric videos by exploiting spatial and temporal feature redundancy, achieving state-of-the-art rendering quality with improved storage efficiency."
"We propose hierarchical semantic graphs to enable fine-grained, multi-level text control for human motion generation using diffusion models, improving performance and allowing continuous motion refinement."
"We propose a unified sparse subspace recovery framework with an efficient solver for outlier rejection, true model reasoning, and parameter estimation, outperforming state-of-the-art methods on synthetic and real datasets and demonstrating strong results in multi-class multi-model fitting and loop closure detection."
"Deep neural networks develop an early linear representation in initial layers and a later ""tunnel"" that compresses it with minimal impact on performance, affecting out-of-distribution generalization and continual learning."
"Random forests are stable under mild conditions on the response, enabling valid and computationally efficient prediction intervals."
"We prove identifiability and show that, under the few root causes assumption with measurement noise, the true DAG is the global minimizer of the L0-norm of root causes, outperforming prior methods."
CrossGNN is a linear-complexity GNN that models cross-scale and cross-variable interactions in multivariate time series using an adaptive multi-scale identifier and selective edge weighting to improve forecasting accuracy.
"ProCal addresses proximity bias in model calibration, improving calibration across diverse datasets and architectures."
"We propose and train a neural framework with a sampling importance network optimized via reinforcement learning, a latent space encoder replacing handcrafted heuristics, and a denoiser, achieving 1.6x faster rendering with higher visual quality than prior state-of-the-art Monte-Carlo path tracing methods."
"We propose an approximate message-passing algorithm and a spectral method for signal recovery through an inhomogeneous low-rank matrix channel, both achieving the information-theoretic optimal transition."
"HMASD is a hierarchical algorithm that discovers both team and individual skills in multi-agent reinforcement learning, significantly improving performance on sparse-reward benchmarks."
KARD improves small language models’ knowledge-intensive reasoning by fine-tuning them with LLM-generated rationales augmented with externally retrieved knowledge.
RoBOS is a robust Bayesian satisficing algorithm for contextual Bayesian optimization under distributional shift that achieves sublinear regret with and without assumptions on the shift magnitude.
"DyGFormer is a simple Transformer-based architecture for dynamic graph learning that encodes node interaction histories via neighbor co-occurrence and sequence patching, and achieves state-of-the-art results on dynamic link prediction and node classification, while DyGLib standardizes evaluation and reveals baseline inconsistencies."
"We present State2Explanation (S2E), a unified framework that learns concept-based explanations to both accelerate reinforcement learning and improve end-user understanding in sequential decision making tasks."
"We propose two novel imputation-free methods that improve spectral clustering on incomplete data by correcting kernel matrices and learning adaptive affinity matrices, outperforming existing approaches on benchmark datasets."
KL-MS achieves an adaptive worst-case regret bound of $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$ with asymptotic optimality for Bernoulli rewards.
Chain-of-thought improves in-context learning of compositional functions by enabling step-wise data filtering and reducing sample complexity.
"LVM-Med is a self-supervised model family trained on large-scale medical images that outperforms state-of-the-art methods on diverse downstream medical tasks, including brain tumor classification and diabetic retinopathy grading."
"We present a probabilistic inverse optimal control method for partially observable stochastic nonlinear systems with unobserved actions, enabling disentanglement of perceptual and behavioral factors from human and agent data via a computationally efficient likelihood approximation."
SODA improves test-time data adaptation under distribution shifts without accessing model parameters by using high-confidence pseudo-labels and preserving low-confidence data information via zeroth-order optimization.
We introduce consolidation of causal mechanisms to simplify large-scale structural causal models while preserving interventional consistency.
"We propose Thrust, a metric that determines when to retrieve external knowledge for large language models, improving cost-efficiency and performance on most tasks."
"We reformulate continual learning as sequence modeling and demonstrate that Transformers, trained via meta-continual learning, effectively solve continual learning tasks across multiple benchmarks."
CORE integrates a conversational agent with any recommender system via uncertainty minimization to improve recommendations in both cold- and warm-start scenarios.
"niLES combines neural SDEs with an encoder-decoder to model ideal LES, improving statistical accuracy and stability in chaotic flows on unstructured meshes."
FedNAR improves federated learning convergence and accuracy by normalizing and annealing weight decay during gradient updates.
"We present a deep learning model using satellite-derived spatio-temporal data to accurately forecast Global Horizontal Irradiance with uncertainty estimates, demonstrating strong performance and zero-shot generalization to unseen stations."
"We propose SVRS and AccSVRS algorithms for finite-sum distributed optimization under δ-similarity and μ-strong convexity, achieving smoothness-free communication complexities of $\tilde{\mathcal{O}}(n + \sqrt{n}\delta/\mu)$ and $\tilde{\mathcal{O}}(n + n^{3/4}\sqrt{\delta/\mu})$, respectively, with a matching lower bound for AccSVRS."
"We propose an online constrained meta-learning framework that provides theoretical bounds on optimality gaps and constraint violations for sequentially learning tasks under hard constraints, and demonstrate its effectiveness in meta-imitation learning and few-shot image classification."
We analyze how trainable node embeddings enable spatiotemporal graph neural networks to balance global and local modeling for accurate time series forecasting.
FAST is a tree-based estimator that combines trial and observational data using an optimal weighting scheme and a new split criterion to improve accuracy and consistency of heterogeneous treatment effect estimation.
"Multidimensional backtracking is a provably competitive, tuning-free algorithm that extends backtracking line-search to efficiently find good diagonal preconditioners for smooth convex optimization using hyper-gradients and cutting-plane ideas."
We propose a robust natural actor-critic method with scalable uncertainty sets and demonstrate its effectiveness in MuJoCo and real-world robot navigation tasks.
"FedGELA addresses partially class-disjoint data challenges in federated learning by fixing the global classifier as a simplex Equiangular Tight Frame and locally adapting to personal distributions, improving both global and local performance."
"We show that imputing missing data before classification can reduce both fairness and accuracy, and propose adaptive algorithms that preserve missing-pattern information to improve both."
"SuTI enables instant subject-specific text-to-image generation without fine-tuning by using in-context learning with an apprentice model trained on outputs from millions of subject-specific expert models, outperforming optimization-based methods like DreamBooth on standard benchmarks."
We present an unsupervised time series learning technique using Granger causality to recover interpretable latent components from multivariate data.
"ContinuAR, a rank-1 solution to a novel linear fidelity differential equation, offers a scalable, accurate, and efficient framework for multi-fidelity fusion, outperforming existing methods by up to 4× in accuracy and 62,500× in training speed."
"We present a scalable Bayesian causal discovery method that directly samples DAGs from the posterior using SG-MCMC and VI, handling both linear and nonlinear models without DAG regularization."
We propose an adversarially robust average precision maximization method that outperforms state-of-the-art baselines by over 4% in robust AP and 7% in clean AP on CIFAR10 and CIFAR100.
VillanDiffusion is a unified backdoor attack framework for evaluating and analyzing vulnerabilities in diffusion models across various architectures and sampling methods.
"RECODE improves zero-shot visual relation detection by generating composite, component-based description prompts with LLMs and fusing them using a chain-of-thought weighting method."
"We propose a variational construction for the invariant distribution of Markov chains to improve Stein importance sampling, achieving better target approximation than standard methods on most PosteriorDB tasks."
We propose an outlier-robust Wasserstein DRO framework that handles both geometric and adversarial non-geometric data perturbations and provide theoretical guarantees and experimental validation for robust decision-making.
"Markovian reward aggregation is impossible with varying discount factors across objectives, necessitating non-Markovian reward functions for optimal multi-objective agents, which we address with a practical aggregation scheme."
We show that Follow-the-Regularized-Leader with a broad class of regularizers and a new learning rate schedule achieves optimal regret in both stochastic and adversarial multi-armed bandit settings without requiring a unique optimal arm.
We derive closed-form expressions for the denoising mean-squared error of a two-layer tied-weight autoencoder with skip connection in the high-dimensional limit and show its advantage over the PCA-related model using real data learning curves.
We demonstrate a natural exponential family where score matching is computationally and statistically efficient despite maximum likelihood being intractable to optimize.
"Collab is a communication-efficient, distributed algorithm for collaborative least squares estimation under partial feature observation that achieves near-optimal performance without sharing labeled data."
"We propose an optimal asymmetric graph structure for graph-based semi-supervised learning that distinguishes labeled and unlabeled nodes, improving label inference and robustness with theoretical guarantees and empirical validation."
"We propose a language-driven scene synthesis method that integrates text, human motion, and existing objects using a multi-conditional diffusion model, outperforming state-of-the-art benchmarks."
A rigorous analysis shows that a random-feature multi-head attention layer with frozen query and key matrices and trainable value matrices can efficiently learn permutation-invariant functions with improved sample complexity and performance dependent on the query-key weight distribution.
"TRD is a graph-based, interpretable algorithm that recovers and robustly represents local-to-global relationships in images without post-hoc processing, matching or exceeding state-of-the-art performance."
"ConPreDiff enhances diffusion-based image synthesis by incorporating context prediction during training, yielding new state-of-the-art results in text-to-image generation with a zero-shot FID of 6.21 on MS-COCO."
"Infinite-width deep neural networks can be described as Gaussian processes, enabling practical criteria and a numerical test for criticality via partial Jacobians, which show that proper stacking of LayerNorm and residual connections achieves criticality for any initialization and confirm this in ResNet and MLP-Mixer."
"Reducing batch size in value-based deep reinforcement learning with replay memories can improve performance, contrary to the usual trend favoring larger batches in neural network training."
"B-DISTIL decomposes a large pretrained model into an ensemble of smaller student models with strong theoretical guarantees, enabling flexible accuracy–inference cost trade-offs."
"We accelerate diffusion-based image generation by using low-bit activations for the early reverse diffusion stages, where inaccuracies do not affect output quality, and maintaining high-bit precision in later stages."
"We systematically evaluate the robustness, calibration, and anomaly detection of 83 CLIP models across diverse visual factors and shifts, revealing that training source design strongly affects these safety properties and that CLIPs are not inherently better calibrated than other ImageNet models."
"We propose adding a calibration term to the training objective of neural models in simulation-based inference to improve uncertainty quantification, achieving competitive or better calibration performance on benchmark problems."
"This paper proposes a native hybrid query framework that integrates approximate nearest neighbor search and attribute filtering via a composite index and joint pruning, achieving up to 315× speedup over state-of-the-art methods on real-world datasets."
"We provide a total variation learning guarantee for 1-layer ReLU conditional generative models without assumptions on the input distribution, enabling near-linear sample learning of deep generative models from internal activations."
"We propose an extra-gradient difference acceleration algorithm for constrained nonconvex-nonconcave minimax problems that achieves improved complexity $\mathcal{O}(\epsilon^{-2})$, surpassing previous best-known bounds of $\widetilde{\mathcal{O}}(\epsilon^{-4})$ and $\widetilde{\mathcal{O}}(\epsilon^{-3})$."
"We propose RCLUB-WCU and OCCUD, online algorithms that leverage user relations to robustly learn from corrupted behaviors and detect malicious users efficiently in multi-user web systems."
"Restart is a sampling algorithm that balances discretization errors and contraction in diffusion models, achieving higher accuracy and up to tenfold faster sampling than previous methods on CIFAR-10 and ImageNet 64×64."
"We propose a Gaussian process calibration method that uses a modified posterior variance with empirically chosen hyperparameters to yield sharp, calibrated predictive quantiles."
"We propose a persistent homology-based self-supervised learning method for molecular representation that improves embeddings and predictive performance, especially on small datasets."
"We propose a diffusion-based framework that learns the data distribution from highly-corrupted samples by introducing further corruption and training to recover the original, and demonstrate its effectiveness on image and medical datasets."
"We propose Generative Return Decomposition (GRD), a causal, interpretable framework for reward redistribution in reinforcement learning that identifies unobservable Markovian rewards and causal relations to optimize policy in delayed reward settings."
"We propose InfoCD, a contrastive Chamfer distance loss that improves robustness to outliers and achieves state-of-the-art point cloud completion by maximizing a lower bound of mutual information between surfaces."
"We propose Compositional Sculpting, a general framework for composing iterative generative models using classifier guidance and demonstrate its application to GFlowNets and diffusion models with harmonic mean and contrast operations on image and molecular generation."
"We propose a hybrid model, Hyper-HMM, that simultaneously aligns temporal and spatial neural responses to naturalistic stimuli across individuals, enabling mapping of brain activity and stimulus content into a common low-dimensional space."
"The paper establishes convergence rates for the MLE in a multivariate deviated model by introducing a distinguishability condition between the base density and the deviation, and analyzing the effect of the deviation parameter approaching zero."
"We introduce balanced dynamic sparse training (ADAPT) for GANs, using a balance ratio metric to efficiently trade off performance and computational cost."
"We introduce new dimensions that characterize PAC and online learnability in realizable regression, providing instance-optimal learners and resolving an open question on optimal online learning."
"We generate multimodal instruction-following data with GPT-4 and use it to train LLaVA, a vision-language model that achieves strong performance on multimodal tasks and sets a new state-of-the-art on Science QA when fine-tuned."
"GAUCHE is an open-source library providing Gaussian process kernels for structured chemical data to enable black-box optimization in molecular discovery, reaction optimization, and protein design."
We present a multi-timescale model that combines synaptic plasticity and gain modulation to achieve adaptive statistical whitening in sensory neurons.
"RichSem enhances long-tailed object detection by leveraging coarse image-level semantics as soft supervision during training, improving detection, especially for rare categories, without requiring precise bounding boxes or complex procedures."
"We propose EBUCB, a Bayesian bandit algorithm that achieves optimal O(log T) regret with constant bounded inference error measured by two α-divergences, addressing the gap between theory and practice for approximate inference in multi-armed bandits."
"InstructBLIP, a vision-language model fine-tuned on diverse instruction-tuning datasets using an instruction-aware Query Transformer, achieves state-of-the-art zero-shot and finetuned performance on a broad range of vision-language tasks, outperforming BLIP-2 and Flamingo while being open-source."
"FamO2O is a framework that adaptively balances policy improvement and constraint intensities per state during offline-to-online reinforcement learning, improving performance over existing methods on the D4RL benchmark."
"We show that neural net invariance under reparametrization is guaranteed in Riemannian geometry when the metric is explicitly represented and correctly transformed, resolving inconsistencies in flatness, optimization, and density maximization analyses."
"We propose ReBRAC, a minimal offline RL algorithm built on TD3+BC incorporating key design choices, and show it achieves state-of-the-art performance across 51 datasets with extensive ablation and sensitivity analyses."
We reformulate the SCF equation as a time-evolving PCA problem and develop a new online PCA-based algorithm that improves convergence in both synthetic and real electronic structure calculations.
TransHP is a Transformer-based method that uses ancestor-class prompt tokens to improve hierarchical image classification by dynamically focusing on subtle descendant-class differences.
"COMPASS, a reinforcement learning method parameterizing a distribution of diverse policies over a continuous latent space, outperforms state-of-the-art approaches on nine out of eleven benchmark tasks and generalizes better on eighteen transformed distributions for Travelling Salesman, Capacitated Vehicle Routing, and Job-Shop Scheduling problems."
"We propose and theoretically justify a conditional score-based diffusion model for posterior sampling in infinite-dimensional Bayesian linear inverse problems, overcoming limitations of unconditional score-based methods."
"Higher-order gradient play dynamics can locally converge to isolated completely mixed-strategy Nash equilibria in some games but may fail to do so in others, and their convergence in coordination games entails internal instability."
"Scheduled Weight Decay mitigates large gradient norms caused by constant weight decay in Adam, improving convergence and generalization."
"SUMVC is a multi-view clustering method that leverages variational analysis and a sufficient representation lower bound to enhance consistent information and reduce redundancy across views, outperforming existing methods as demonstrated by theoretical and empirical evaluations."
"We introduce Structured Voronoi Sampling (SVS), a gradient-based, theoretically grounded method for controlled text generation that outperforms alternative sampling algorithms in both distributional accuracy and adherence to control targets."
"We propose extremal transport, a scalable algorithm approximating the theoretically optimal unpaired image translation by limiting partial neural optimal transport maps."
"ForkMerge mitigates negative transfer in auxiliary-task learning by periodically forking the model, searching task weights to minimize target validation error, and dynamically merging branches to filter out harmful updates."
"The paper provides a unified framework for uniform recovery guarantees in nonlinear generative compressed sensing with discontinuous or unknown observation models, achieving nearly optimal sample complexity of roughly $\tilde{O}(k/\epsilon^2)$ using generalized Lasso and Lipschitz approximation."
We establish accelerated implicit bias rates for mirror descent and steepest descent by reformulating their optimization dynamics as regularized bilinear games.
"We introduce Layerwise Linear Feature Connectivity (LLFC), a stronger form of linear connectivity where feature maps in every layer of trained networks are linearly connected, and show that LLFC holds whenever Linear Mode Connectivity does, providing new insights into neural network training dynamics."
"MoSo is a data-pruning method that removes least-informative samples by approximating their impact on empirical risk via gradient consistency, outperforming state-of-the-art methods at high pruning ratios."
"We propose an importance-sampling-based method to improve imitation learning with supplementary imperfect data, theoretically and empirically outperforming existing approaches in robotics, Atari, and image classification tasks."
"We propose reinforcement learning with graph neural networks to suggest variable orders for Cylindrical Algebraic Decomposition, outperforming existing heuristics and generalizing across datasets."
"We propose a method for parameterized bandits that leverages correlated auxiliary feedback to reduce regret, and quantify the reduction in terms of the correlation coefficient."
"We propose an NMDA receptor-inspired nonlinear activation in transformers that models hippocampal memory consolidation, showing that it enables long-term reference memory in a navigation task and resides in the feed-forward layers."
"We present a quasi-Newton proximal extragradient method that achieves the optimal NAG rate of $\mathcal{O}(1/k^2)$ for $k=\mathcal{O}(d)$ and a faster rate of $\mathcal{O}(\sqrt{d\log k}/k^{2.5})$ for larger $k$, providing the first provable improvement over NAG for quasi-Newton methods in convex optimization."
We adapt the Stochastic Similar Triangles method with batching for two-point zero-order oracles in non-smooth stochastic convex optimization under infinite noise variance.
"GW-PCZero generalizes path consistency to neural-guided MCTS for real-world tasks with non-zero rewards, improving efficiency and performance on Atari 100k games with a 198% mean human performance using only 25% of EfficientZero's compute."
"Video pretraining via VITO yields more robust and human-aligned visual representations than image pretraining, excelling on both image and video tasks and aligning closely with human judgments."
"We theoretically and empirically show that, under full supervision, k-NN graphs provide no benefit over structure-agnostic baselines for tabular node classification, questioning the utility of such graphs in this setting."
"We propose a method that learns neural algorithmic reasoning from input-output pairs without intermediate supervision, achieving state-of-the-art results on algorithmic reasoning tasks."
"The exploration by optimization method achieves O(d√(T log T)) regret in adversarial bandits and O((d² log T)/Δmin) regret in stochastic bandits, with improved guarantees in multi-armed and multitask settings."
"Reflexion improves language agents by using linguistic feedback and episodic memory for trial-and-error learning without fine-tuning, outperforming GPT-4 on HumanEval."
"MVDiffusion is a correspondence-aware diffusion method that generates consistent multi-view images from text and pixel correspondences without iterative warping, outperforming prior methods on panorama and depth-to-image tasks."
"We generalize the unconstrained features model to multiple non-linear layers and show that its unique global optimum for binary classification exhibits deep neural collapse, explaining experimental evidence and validating the model with empirical results."
"We propose a distributional reinforcement learning method that selects actions via randomized risk criteria without biased exploration, prove its convergence and optimality under weaker assumptions, and demonstrate superior performance on Atari 55 games."
We introduce a unified latent variational diffusion model that reconstructs theoretical kinematic quantities from LHC detector data with significantly improved accuracy compared to existing methods.
"We propose Adaptive Pareto Exploration, a sampling strategy with analyzed sample complexity for efficiently identifying subsets of Pareto optimal arms in multi-objective bandits, demonstrating practical benefits on Covid-19 vaccination strategy selection."
"FNO-DEQ, a deep equilibrium variant of the FNO architecture that leverages fixed-point formulations, outperforms standard FNO-based models in solving steady-state PDEs with fewer parameters and greater robustness to noisy data, and is universally capable of approximating solutions to any steady-state PDE expressible as a fixed point."
"Poppy is a training procedure that learns a population of complementary policies, achieving state-of-the-art reinforcement learning results on traveling salesman, capacitated vehicle routing, and job-shop scheduling problems."
"We propose an Information Bottleneck Distillation method that leverages a robust pre-trained teacher model to enhance adversarial robustness by increasing mutual information between latent features and predictions and reducing mutual information between inputs and latent features, significantly improving performance against adversarial attacks."
"UniControl is a generative foundation model that unifies diverse controllable image generation tasks with arbitrary language prompts within a single framework, outperforming single-task models in zero-shot settings."
"We propose a unified ""normalized depth"" target and a 3D normalized cube depth representation that enable state-of-the-art monocular 3D detection for both vehicles and infrastructure across multiple benchmarks without extra information."
"H2RBox-v2 introduces symmetry-aware self-supervision to improve weakly-supervised oriented object detection using only horizontal box annotations, achieving performance close to fully supervised methods with rotation annotations."
"Egocentric Planning combines symbolic planning and Object-oriented POMDPs to achieve scalable, robust task execution in complex environments, demonstrating a 36.07% unseen success rate on ALFRED and serving as a strong baseline for hybrid task generalization methods."
"We propose Temporal Continual Learning (TCL) with a Prior Compensation Factor (PCF) to improve human motion prediction by better preserving prior information and optimizing for multi-stage prediction, showing effectiveness across multiple datasets."
"Clipped Stochastic Gradient Descent with a $T^{-\alpha}$ learning rate ($\alpha < 1$) is the first online algorithm proven to be simultaneously adaptive to drift, robust to heavy-tailed data and corruptions, and distribution-shift invariant in streaming settings."
We propose a learnable token pruning method that reduces autoregressive transformer inference cost by up to 80% context size with minimal performance loss.
We propose using specific random weight network prototypes as constraints during training to improve image restoration performance across multiple tasks without extra computational cost.
"No SSL algorithm improves upon the minimax-optimal error rates of SL or UL for 2-Gaussian mixtures, despite real-world SSL gains, suggesting the need for tighter constant tracking in theory."
"We propose ERDA, a 3D weakly supervised segmentation method that uses entropy and distribution alignment regularization to effectively utilize all unlabeled data and outperform fully supervised baselines with only 1% supervision."
"We propose FGWMixup, a graph mixup method based on Fused Gromov-Wasserstein optimal transport that jointly augments graph structure and signals, improving GNN generalizability and robustness."
"We establish quantitative approximation rates for complex-valued neural networks with smooth but non-polyharmonic activations, showing the error scales as $m^{-k/(2n)}$ and that curse of dimensionality is unavoidable for $C^k$ function approximation."
"PriorBand is an HPO algorithm for deep learning that leverages expert beliefs and proxy tasks, demonstrating efficiency and robustness across DL benchmarks."
"IMRCs exploit forward and backward learning to improve classification accuracy on evolving sequential tasks with limited samples, with performance gains analytically linked to task similarity and sequence length."
"We propose HN-GFN, a multi-objective Bayesian optimization method using a preference-conditioned hypernetwork to efficiently sample diverse molecular candidates with improved quality and sample efficiency."
"We introduce energy-based sliced Wasserstein (EBSW), a parameter-free sliced Wasserstein variant using an energy-based slicing distribution defined via the projected one-dimensional Wasserstein distance, and demonstrate its favorable theoretical and empirical performance."
"DSN is a neuron-wise task incremental learning method that selects a small set of neurons for knowledge transfer between tasks without using past data, alleviating catastrophic forgetting and enabling bidirectional knowledge transfer."
"Annotator is a voxel-centric active learning method for LiDAR semantic segmentation that efficiently selects informative voxels for annotation, achieving high performance with far fewer labels across various adaptation settings."
"Mechanic is an automatic learning rate scale factor tuner that, across diverse deep learning tasks, performs comparably to or better than manual tuning."
"HyenaDNA is a genomic foundation model using implicit convolutions to achieve long-context (up to 1 million single-nucleotide tokens) pretraining on the human genome, outperforming prior Transformers on multiple genomic benchmarks with fewer parameters and faster training."
We formalize semantic independence in text embeddings using partial orthogonality and prove the existence of embeddings that preserve conditional independence structures.
We present a method enabling text-to-image diffusion models to run on mobile devices in under 2 seconds with improved quality and reduced computational cost.
DAFNO enables Fourier neural operators to model responses on irregular and evolving domains by incorporating a smoothed characteristic function and achieving state-of-the-art accuracy in material and fracture simulations.
We propose a projected-gradient-based algorithm that achieves fairness constraints with total costs of order $\sqrt{T}$ in contextual bandit problems with knapsacks.
"CL-NeRF enables efficient continual NeRF adaptation to scene changes using a lightweight adaptor and conflict-aware distillation, preserving unchanged regions with minimal forgetting and high training efficiency."
We propose learning neural implicit 3D representations from multi-view RGBD images using an attentive depth fusion prior based on TSDF to improve geometry inference by volume rendering.
"We systematically study combining self-training and contrastive learning, showing significant complementary gains in domain adaptation but not in semi-supervised learning, with theoretical insights explaining their synergy under distribution shift."
IMP is a scalable multimodal Transformer encoder that uses alternating gradient descent and mixture-of-experts to achieve state-of-the-art zero-shot video classification with significantly reduced computational cost.
"We propose a self-supervised pre-training method for model-free RL that enables fast transfer to new tasks by implicitly modeling long-horizon dynamics, validated on manipulation and locomotion domains."
"We propose a data-dependent sampling method for neural network weights that achieves universal approximation and Barron function approximation rates without iterative optimization, yielding accuracy comparable to trained networks but with much faster construction."
Log-precision transformers can be equivalently expressed as first-order logic sentences augmented with majority-vote quantifiers.
"DSIR efficiently selects pretraining data using importance resampling with hashed n-gram features, matching or surpassing expert curation and baseline methods in downstream performance."
"HUGE enables reinforcement learning to effectively use noisy, asynchronous, non-expert human feedback to direct exploration and learn complex tasks without reward design or exploration bonuses."
We derive sample complexity bounds for score-matching estimation with deep ReLU networks and apply them to analyze causal discovery and generative modeling.
We propose a debiased estimator using adaptive linear estimating equations that achieves asymptotic normality for sequential data while preserving non-asymptotic performance in multi-armed bandits.
"A method is presented that maps subgraph densities to stochastic block model parameters in polynomial time, enabling direct inference from subgraph densities."
"We prove tight inapproximability results for maximin share allocation with submodular and subadditive costs, and show constant approximations for bin packing and job scheduling under these cost functions."
"We analyze and characterize the high-confidence, star-like equi-confidence regions in vision models, proposing a Level Set Traversal algorithm that reveals their connectivity and extent via gradient-based exploration."
"FairLISA improves fairness in user modeling with limited sensitive attribute data by leveraging both known and unknown sensitive information in an adversarial framework, maintaining accuracy across varying data scenarios."
"We propose a method that uses attention and gradient information to focus vision transformers on class-related entities in few-shot image classification, improving performance across different architectures and pre-training methods."
"We introduce and analyze malicious data update attacks during the selective forgetting (machine unlearning) process, both in static and sequential settings, demonstrating their feasibility and effectiveness through theoretical analysis and experiments."
AIRBO is a robust Bayesian optimization algorithm that models arbitrary input uncertainty using Gaussian Processes with Maximum Mean Discrepancy and achieves state-of-the-art performance under input uncertainty.
"We rigorously show that large stepsizes benefit sparse recovery in SGD but hinder it in GD for overparameterized linear networks, with effects amplified near the divergence threshold."
"CQL (ReDS) improves offline reinforcement learning by using reweighted data to relax uniform distribution constraints, enabling state-dependent policy adherence within the support of the behavior policy."
We propose a differentially private method to learn prompts for large language models that achieves performance close to non-private prompting while maintaining privacy and compatibility with commercial APIs.
RIVAL aligns latent distributions during diffusion-based image generation to produce high-quality real-world image variations with improved semantic and perceptual fidelity.
"MADG is a novel adversarial domain generalization method using a margin loss-based discrepancy metric that learns domain-invariant features and improves generalization to unseen target domains, with theoretical guarantees and strong empirical performance across multiple benchmarks."
A graph denoising diffusion model using amino acid replacement matrices generates diverse amino acid sequences for a given protein backbone and outperforms baseline methods.
"We present a dimensionality reduction framework that identifies participating neuronal populations, infers signal flow directions, and tracks their temporal evolution, revealing area-specific communication patterns related to retinotopy in macaque visual cortex."
"Weighted Banzhaf values, particularly the uniquely robust semi-value derived from Kronecker noise, provide the most robust data valuation under stochasticity compared to the standard Banzhaf value."
"We propose PlanE, a scalable framework inspired by the Hopcroft–Tarjan planar graph isomorphism algorithm, that learns complete invariants for planar graphs and achieves state-of-the-art results on planar graph benchmarks."
"We show that knowledge distillation in linear and deep linear models can be interpreted as a partial stochastic variance reduction method that reduces but does not fully eliminate gradient noise, with convergence analysis highlighting the importance of distillation loss weighting."
"We propose a new Influence Function approximation that removes linearization and uses Geometric Ensemble to better capture nonlinear influences, outperforming existing methods with less computation."
"We propose instance-dependent threshold functions for pseudo-labeling in semi-supervised learning that use per-instance ambiguity and error rates to set higher thresholds for more uncertain samples, providing a probabilistic correctness guarantee for assigned pseudo-labels."
"Fed-FA detects backdoor clients in federated NLP by modeling data divergence with f-divergence and Hessian-based indicators, outperforming parameter distance methods."
"DinoSR combines masked language modeling, self-distillation, and online clustering to learn speech representations that outperform previous state-of-the-art models on downstream tasks and reveal learned discrete phone-like units."
"Many stability definitions in learning theory are equivalent within distribution-dependent and distribution-independent families, including privacy, replicability, and divergence-based notions."
"EBML is a coherent meta-learning framework that detects and adapts out-of-distribution tasks using expressive energy functions, improving reliability and adaptability over existing Bayesian meta-learning methods."
"We propose Disentangled Counterfactual Learning (DCL), a plug-and-play method that disentangles static and dynamic latent factors and augments physical reasoning via counterfactual interventions to improve audiovisual commonsense inference."
"We analyze and improve neural policy learning for NetHack, achieving state-of-the-art results but showing that scaling alone cannot match symbolic agents or expert humans."
"We propose using conformal e-values with side information weighting to stabilize conformal inference, reducing randomness and preserving power in novelty detection."
SafeDICE is a hyperparameter-free offline safe imitation learning algorithm that learns a policy satisfying safety constraints by directly estimating stationary distribution corrections using non-preferred demonstrations.
"Dream-OOD uses diffusion models to generate photo-realistic outlier images from in-distribution data, improving out-of-distribution detection when used for model training."
ADD-THIN is a probabilistic denoising diffusion model for temporal point processes that outperforms state-of-the-art models in long-term forecasting.
"SPACE is an efficient federated learning participant contribution evaluation method that uses Federated Knowledge Amalgamation and Prototype-based Model Evaluation to enable single-round, validation-set-size-independent assessment with improved speed and correlation compared to Shapley value-based methods."
"Fictitious Play in two-agent identical-payoff potential games can require exponential time to reach a Nash equilibrium, even though every approximate Nash equilibrium is close to the unique pure Nash equilibrium in ℓ₁-distance."
Mean-field Langevin dynamics enables two-layer neural networks to learn k-sparse parity functions with sample complexity where the sparsity k does not appear in the dimension dependence exponent.
"We evaluate several efficient training algorithms for BERT and T5 under a fixed computation budget and find no gains in training, validation, or downstream performance compared to a standard baseline with learning rate decay."
"GlyphControl improves visual text generation in Stable-Diffusion using glyph conditioning without retraining, outperforming DeepFloyd IF on OCR accuracy, CLIP score, and FID."
We propose a spike sorting-free neural decoding method using a time-varying mixture of Gaussians over spike features that outperforms conventional spike sorting and thresholding methods.
"SheetCopilot is an LLM-based agent that automates spreadsheet tasks using atomic actions and a state machine planning framework, achieving 44.3% single-generation success on 221 benchmarked tasks."
"We introduce ATM, a gradient-based method that generates perturbations to text prompts, causing text-to-image models to fail at generating desired subjects with high success rates (91.1% short, 81.2% long)."
"FACE is a Fourier-based metric that effectively quantifies the human-model language gap and correlates with model size, decoding methods, and human judgments."
"We propose a curriculum reinforcement learning method that automatically defines a semantic goal space using VQ-VAE and graphs, and selects uncertainty- and temporal-distance-aware curriculum goals, improving data efficiency and performance in goal-reaching tasks with raw observations."
"Tracr compiles human-readable programs into decoder-only transformer models with known internal structure, enabling controlled experiments and ground-truth evaluation of interpretability methods."
"InfoGating learns minimal task-relevant information by differentiably controlling signal-to-noise ratios, improving robustness and generalization in representation learning and control tasks."
"We propose a method that exploits semantic relations in natural language labels via a Language Semantic Graph to enhance data efficiency in transfer and semi-supervised learning across image, video, and audio modalities, improving and accelerating model training."
"ForecastPFN, a zero-shot forecasting model trained solely on synthetic data, achieves more accurate and faster predictions than state-of-the-art methods even when those methods are given extensive additional in-distribution data."
"Tuning the complexity of discrete neural representations by entropy improves few-shot finetuning performance, and users can select the optimal complexity using visualizations of representations."
"We propose an image-computable model of human visual motion processing that outperforms state-of-the-art computer vision models in predicting human motion perception, as validated by neurophysiological and psychophysical comparisons."
"D$^2$CSG is a dual-branch neural network that learns compact, high-quality CSG representations of 3D CAD shapes by modeling both the main cover and complementary subtracted components with provable generalization and dropout-induced compactness."
"We introduce a non-stationary multi-armed bandit algorithm with an auto-regressive reward model and dynamic exploration-exploitation and restarting mechanisms, achieving near-optimal regret and demonstrating effectiveness in real-world time series prediction."
OPAX is an active exploration algorithm that uses calibrated probabilistic models and optimistic information gain maximization to efficiently learn dynamics for zero-shot multi-task planning.
"Neuro-symbolic predictive models often exploit unintended reasoning shortcuts due to flaws in their learning objectives, undermining their interpretability and reliability despite their theoretical advantages."
"We introduce ISQA, a multi-round interactive sketch-based question answering task, and an EC system balancing accuracy, drawing complexity, and human interpretability, showing that interaction improves communication efficiency between agents."
"Parallel-mentoring improves robust model-based optimization by enabling proxies to mutually mentor each other via consensus-based pairwise supervision and adaptive soft-labeling, enhancing performance on out-of-distribution designs."
ORDER introduces a discrete proxy representation to enhance offline RL robustness to partial observability during execution without requiring mask information at training time.
"We introduce 3S Testing, a generative framework that outperforms traditional methods in estimating model performance on minority subgroups and under distributional shifts by generating synthetic test data and providing reliable confidence intervals, suggesting a potential paradigm shift from real to synthetic test datasets."
"DiffPreT pre-trains a protein encoder using joint sequence-structure diffusion modeling and SiamDiff improves it by modeling conformational correlations, achieving state-of-the-art results on protein structure tasks."
"We address supervision starvation in latent graph inference by identifying and reconnecting pivotal starved nodes to restore meaningful graph structure and improve generalization, especially under limited supervision."
"We propose and analyze a simple, nonparametric method for individually calibrated regression quantiles that achieves finite-sample statistical consistency and serves as a theoretical benchmark for regression calibration."
"$\eta\psi$-Learning improves exploration efficiency by conditioning policies on past episodic experience to maximize state visitation entropy, enhancing state coverage with limited samples."
"We provide convergence guarantees for single-call stochastic extragradient methods under weak assumptions and arbitrary sampling, addressing mini-batching and step-size selection for generalized variational inequality problems."
"We propose a federated learning method that directly maximizes the AUC for imbalanced data, formulating it as a compositional minimax problem with theoretical complexity bounds and demonstrating its efficacy experimentally."
"We propose PPi, a pretraining-based model for patient-independent SEEG seizure detection that uses self-supervised learning and domain generalization techniques to address patient variability and brain region differences."
We present the first algorithm to directly verify safety of decision-tree controlled continuous-time systems using set-based propagation through decision nodes.
"We establish the interrelations between group fairness and diversity in center selection in clustering, showing that constant approximation for one implies a bounded-loss solution for both, but not vice versa, and that both can be incompatible with other distance-based fairness notions."
We present improved league training for StarCraft II AI that achieves superhuman performance with far fewer resources by using goal-conditioned exploiters and opponent modeling.
"Vanilla-SVD correctly recovers all clusters in the symmetric stochastic block model, answering an open question by Van Vu."
We present the first provably replicable reinforcement learning algorithms for parallel value iteration and episodic R-Max.
"We propose Inversion Influence Function (I²F), a scalable tool that links recovered images to private gradients in Deep Gradient Leakage attacks, enabling analysis of privacy leakage in distributed learning."
"We propose BECAT, a CAT framework that selects questions to closely approximate the gradient of full-response ability estimation, achieving equivalent accuracy with 15% fewer questions."
"We propose a reinforcement learning-based fine-grained reward maximization framework that directly optimizes segmentation model calibration for uncertainty estimation in safety-critical vision tasks, outperforming state-of-the-art methods on calibration metrics while preserving segmentation accuracy."
"Mixture-of-Modality Adaptation (MMA) enables efficient vision-language adaptation of LLMs using lightweight adapters and a routing mechanism, yielding competitive performance with minimal training cost."
"We propose a learnable neuron permutation-based rewiring method with cached wirings and alignment for continual reinforcement learning, improving adaptivity and stability across diverse tasks."
"JSP-GFN approximates the joint posterior over Bayesian Network structure and parameters using a GFlowNet that generates the DAG and then samples its parameters, enabling flexible, nonlinear local models and outperforming existing methods."
"The paper proposes Uni-Code with DCID and MM-EMA for cross-modal generalization via fine-grained, shared discrete latent space pre-training across multiple modalities."
"We introduce a jump diffusion generative model that jointly models state and dimensionality, enabling sampling of data with varying dimensions and demonstrating superior performance on molecular and video datasets compared to fixed-dimensional models."
"We propose QuadAttac$K$, a low-cost quadratic programming method that significantly increases successful ordered top-$K$ targeted attacks on ImageNet-1k models up to $K=20$ while improving $K=5$ attack rates."
We propose a black-box membership inference attack using quantile regression on confidence scores that is as effective as shadow model methods but requires less computation and no knowledge of the target model's architecture.
"We propose Subtree Attention (STA), a multi-hop graph attention mechanism that efficiently captures long-range and fine-grained local graph information and outperforms existing graph neural networks on node classification tasks."
We introduce a temporal feature similarity loss using pre-trained self-supervised features that achieves state-of-the-art object-centric representation learning on MOVi datasets and scales to unconstrained YouTube-VIS videos.
"We present a decorrelation lemma and associated techniques to derive new, recover existing, and unify information-theoretic generalization bounds for learning algorithms."
We propose projection-free algorithms achieving sub-linear regret for online geodesically convex optimization on Riemannian manifolds using separation or linear optimization oracles.
"We theoretically and empirically analyze within-class feature variability in GNNs for node-wise classification, revealing partial neural collapse under stricter graph conditions than in instance-wise classification, and contrasting this with spectral methods."
"We show that preference-based RL can be reduced to reward-based RL or multiagent RL with minimal additional cost, and instantiate this for various MDP settings."
Meta-in-context learning recursively improves large language models' in-context learning abilities and performance across diverse tasks without traditional fine-tuning.
"We derive tight non-asymptotic upper bounds for random design linear regression with dependent $\beta$-mixing data that recover the variance predicted by the Central Limit Theorem, even under misspecification, without inflating the leading term by mixing time factors after a burn-in."
"We present a large-scale deep learning framework that tokenizes and models neural population dynamics across diverse primate recordings, enabling rapid adaptation to new sessions with minimal labels."
"CEIL is a general imitation learning algorithm that learns a hindsight embedding and a contextual policy, outperforming state-of-the-art baselines in sample efficiency for online tasks and achieving competitive results for offline tasks."
"We generalize Sharpness-Aware Minimization to Riemannian manifolds, deriving Lorentz SAM and unifying prior variants, and show its improved generalization in knowledge graph and translation tasks."
"RNNs exhibit a ""simplicity bias,"" sequentially reusing attractors and dynamics when performing multiple tasks, with new attractors emerging mainly due to task demands or architecture, and shared attractor geometry reflecting input similarity, implying modularity does not naturally arise without incentives."
"We introduce SwitchBack, an int8 linear layer that accelerates CLIP ViT-Huge training by 13–25% without performance loss, and propose an AdamW-Adafactor hybrid to stabilize training and prevent loss spikes."
"We introduce HASSOD, a hierarchical adaptive self-supervised object detection method that learns object composition and count without supervision, improving detection performance on LVIS and SA-1B."
"We propose DP-FEST and DP-AdaFEST, which preserve gradient sparsity and reduce gradient size by $10^6 \times$ during differentially private training of large embedding models without sacrificing accuracy."
"LEFT introduces a unified, logic-based framework that enables flexible reasoning and grounding across 2D images, 3D scenes, human motions, and robotic manipulation by learning to execute domain-specific programs from LLM-generated logic."
"BPQL is a novel actor-critic algorithm for delayed feedback environments that avoids state-space explosion by evaluating critic values in the original state space, outperforming conventional methods in sample efficiency and handling long delays."
"We extract semantic correspondences from text-to-image diffusion models by optimizing prompt embeddings to attend to regions of interest, matching state-of-the-art supervised results on PF-Willow and outperforming unsupervised methods on multiple datasets."
"We propose a discriminator with dynamic feature masking that detects and adapts to changes in generated data distributions, improving GAN training stability and outperforming state-of-the-art methods."
"We introduce a NeRF-based method for real-world audio-visual scene synthesis that generates realistic, spatially coherent audio-visual content from novel viewpoints."
"We propose convex-function-based training objectives for text generation that outperform MLE, especially in closed-ended tasks, by sharpening the predicted output distribution and improving performance across autoregressive and non-autoregressive models as well as large language models."
"Offline RL's surprising robustness to incorrect reward labels arises from the interplay between algorithmic pessimism and biased data coverage, inducing a survival instinct that constrains policies to the data support."
"We propose Feature Discrimination Alignment (FD-Align), a fine-tuning method that preserves consistency of spurious features during fine-tuning to enhance generalizability in few-shot learning without sacrificing downstream performance."
"We apply deep learning to design revenue-optimal data markets with obedience and incentive constraints, replicating known solutions and extending to complex settings."
"We unify and compare sample-efficient offline RL algorithms using a new data diversity notion, showing their comparable guarantees and introducing a novel model-free PS-based algorithm with frequentist bounds."
RAM-MIL integrates Optimal Transport into Multiple Instance Learning to maintain state-of-the-art performance when test data come from a different domain than training data.
"We introduce VAST-27M, a large-scale omni-modality video caption dataset integrating vision, audio, subtitle, and text, and train VAST, a foundational model that achieves new state-of-the-art results across vision, audio, subtitle, and multi-modal video-text tasks."
We propose a deep hashing framework using product quantization with a softmax-based branch that learns class-specific compact codes and demonstrates superior performance on large-scale datasets up to 360K classes and 17 million samples.
"We introduce clipped softmax and gated attention to reduce outliers in transformer activations, enabling full INT8 quantization without performance loss."
"DreamSparse leverages pre-trained diffusion models and a geometry module to synthesize high-quality, geometry- and identity-consistent novel view images from sparse inputs without fine-tuning."
"DiffPack is a torsional diffusion model that autoregressively predicts protein side-chain dihedral angles, improving packing accuracy by 11.9–13.5% with much smaller models than previous methods."
"We propose a post-hoc interpretability framework using sparse, stochastic layer activations and vision-text models to generate textual descriptions of individual neuron functions in vision networks, enabling efficient and interpretable analysis of their decision process."
"For finite-arm bandits with linear function approximation, global convergence of policy gradient methods depends on how the policy update interacts with representation properties, not on approximation error, with distinct conditions required for natural policy gradient and Softmax policy gradient."
WS-AVS is a novel weakly-supervised framework that leverages multi-scale multiple-instance contrastive learning for audio-visual segmentation using instance-level annotations.
"We introduce the Minimum Conditional Dependence (MCD) criterion, which outperforms MMI-based methods by up to 13.7% F1 by selecting causal rationales that d-separate non-causal features from the label using KL-divergence."
We propose training conditional GFlowNets on Markov decision process formulations of combinatorial optimization problems to efficiently sample high-quality solutions.
"We improve offline bisimulation-based RL by addressing missing transitions and reward scaling, yielding better performance on D4RL and Visual D4RL."
"Cocktail enhances text-conditional diffusion models by fusing multiple modalities and spatial guidance for precise, high-fidelity image generation."
"We propose a causal space defined by a probability space and causal kernels to rigorously axiomatize causality, addressing limitations such as cycles and latent variables in existing frameworks."
"COMET is a hierarchical framework that learns effective representations for medical time series by leveraging contrastive learning at multiple data levels, outperforming baselines especially with limited labels."
"We propose a projection-free OCO algorithm using self-concordant barriers that computes the inverse Hessian only rarely, achieving state-of-the-art regret bounds."
"This paper gives a concise error analysis of vector-valued random feature ridge regression, establishing strong consistency under misspecification and minimax-optimal rates without random matrix concentration, and shows that the required parameter and sample complexities match Monte Carlo intuition and are free of logarithmic factors."
"We present a framework characterizing the curvature and regularity of deep neural network loss landscapes with normalization and skip connections, and show it explains why skip connections accelerate training."
"ELDEN introduces an intrinsic reward based on uncertainty in local entity dependencies to improve exploration and policy learning in environments with complex, factored state spaces."
We propose a twin-structure framework that mitigates bias between internal and cross-links in GNN-based link prediction and improves overall accuracy by fusing debiased node embeddings learned with augmented supervision into existing GNN models.
"VOCE is an offline safe RL algorithm that uses variational inference and pessimistic Q-estimation to improve policy optimization and safety on offline datasets, especially outperforming existing methods in safety-critical tasks."
"We propose a quantization method for diffusion models that reduces computation and memory via low-bit networks while mitigating activation oscillation and quantization error, achieving strong performance (e.g., 4-bit models with 7.8× speedup and FID 5.17 on CIFAR-10)."
"SutraNets improve long-sequence time series forecasting by decomposing univariate prediction into multivariate prediction over sub-series, reducing error accumulation and enhancing accuracy."
"We propose DRaT, a meta-reinforcement learning method that addresses sparse-reward and dynamics mismatch by using a doubly robust augmented estimator with interval-based importance weighting to transfer informative samples across tasks, significantly outperforming existing hindsight-based approaches on MuJoCo locomotion tasks."
"We propose RH-BrainFS, a novel method addressing regional heterogeneity between structural and functional connectivity in multimodal brain networks, improving performance on neuroscience tasks."
"We propose an adversarial neural degradation model that, when trained with a restoration network under a minmax criterion, generates diverse complex degradations and generalizes better to real-world unseen degradation for improved image super-resolution."
"DeepSoftLog, a principled neuro-symbolic framework using probabilistic semantics for soft-unification, outperforms prior methods on benchmarks by satisfying key properties like non-redundancy, well-defined scores, and non-sparse gradients."
"We propose Prompt-based Continued Pre-training (PCP), which enhances prompt-based fine-tuning by pre-training on task-related texts and prompt templates, consistently improving performance across multiple tasks and settings."
"We derive a PAC-Bayesian generalization bound for structured prediction that depends on both the number and size of structured examples, under the assumption that data are generated by a Knothe-Rosenblatt rearrangement of a factorizing reference measure."
"We introduce Focused Transformer (FoT), a contrastive learning-based method that fine-tunes large language models to extend effective context length by improving (key, value) space structure and reducing distraction from irrelevant context."
We propose an optimal transport-based self-supervised graph learning method that aligns node representation transport plans and outperforms contrastive methods without using edge information or distinguishing positive/negative samples.
"We propose a more accurate federated frequency estimation sketch algorithm for multiple communication rounds and a two-phase approach to adapt sketch size to problem simplicity, with differential privacy support, and validate it on real datasets."
We propose an environment-aware representation for egocentric video that outperforms standard clip features on human-centric tasks and achieves state-of-the-art results on Ego4D NLQ by training on simulated environments and transferring to real-world videos.
"We present a randomized algorithm that computes a low-rank, low-precision factorization of large matrices, achieving high compression with provable error bounds and competitive performance in image compression and embedding tasks."
MindEye is a novel fMRI-to-image model with specialized retrieval and reconstruction submodules that achieves state-of-the-art performance in both tasks and can accurately retrieve and reconstruct images even from large-scale databases.
"We find that neural networks often implement tasks via modular, composable subnetworks rather than simple template matching, suggesting learned compositionality."
DäRF enhances NeRF's robustness with few viewpoints by integrating and adapting monocular depth estimation via online complementary training and addressing depth ambiguity.
PointGPT adapts the GPT architecture to point clouds via auto-regressive pre-training and achieves new state-of-the-art results on 3D classification and few-shot learning benchmarks.
"The sample complexity of PAC learning noisy multi-layered sigmoid recurrent neural networks with w weights grows as O(w log(T/σ)), whereas for the non-noisy case it is Ω(wT), showing an exponential gap in dependence on T that persists even for very small noise σ."
"We introduce ConRad, a neural radiance field variant that reconstructs 3D objects from a single RGB image by leveraging pretrained diffusion models to preserve input appearance and achieve improved, faithful 3D reconstructions."
"AdaPlanner is a closed-loop LLM agent framework that adaptively refines its plans using environmental feedback and few-shot exemplars, outperforming baselines with fewer demonstrations in ALFWorld and MiniWoB++."
"Time discretization in continuous-time RL for LQR systems introduces a trade-off between approximation and statistical error in value estimation, enabling improved policy evaluation efficiency by optimally choosing temporal resolution given finite data."
KOPI is a new knockoff-based method that controls the actual proportion of false discoveries and improves power and specificity in high-dimensional data analysis compared to existing methods.
"We derive and analyze generalization error bounds for multi-channel neural networks with t-product layers, showing that transformed low-rank parameterization and adversarial training improve robust generalization."
"DreamHuman generates animatable, high-fidelity, text-driven 3D human avatars with diverse appearances and consistent anthropometry by integrating text-to-image models, neural radiance fields, and statistical body models."
"We develop and apply partial identification methods to evaluate how changes in peer-review assignment algorithms affect review quality using randomized assignments, finding that emphasizing text similarity improves quality and that randomization only slightly reduces it."
"We develop a discounted occupation time framework for continuous-time, continuous-space RL with SDE dynamics and apply it to derive performance guarantees for PG and TRPO/PPO methods, demonstrating their effectiveness in numerical experiments."
"We propose the Parallel Spiking Neuron (PSN) and its variants, which reformulate spike-based dynamics without reset to enable parallel simulation, achieving high efficiency and accuracy in sequence modeling."
"Chain-of-thought reasoning in language models is effective when training data consists of locally structured dependencies, enabling step-by-step inference across overlapping variable clusters and improving data efficiency."
"We introduce ATP, an adaptive test-time personalization method for federated learning that learns module-wise adaptation rates from source domain distribution shifts, outperforming existing TTA methods on diverse distribution shifts without requiring client labels at test time."
"Adaptive Swarm Mesh Refinement (ASMR) formulates Adaptive Mesh Refinement as a Swarm MDP with message passing, enabling scalable, efficient, and high-quality mesh refinement that achieves up to 30-fold speedup over uniform refinement and matches error-based AMR without error oracles."
"Layer normalization with nonlinear activations biases the penultimate Gram matrix toward the identity at an exponential rate with depth at initialization, quantified via the Hermite expansion of the activation."
"We propose formulating Gaussian mixture model-based robot policy optimization as a Wasserstein gradient flow constrained by L²-Wasserstein distance and using Riemannian optimization on the Bures-Wasserstein manifold, demonstrating improved task success and lower variance compared to baseline policy optimization methods in reaching, collision avoidance, and multi-goal tasks."
PRED is an occlusion-aware image-assisted pre-training framework for outdoor point clouds that uses BEV-conditioned semantic rendering and high-ratio point masking to improve 3D perception tasks by addressing LiDAR incompleteness.
"We propose FourierHashNet, an LSH-based retrieval method using a Fourier-transformed dominance similarity for efficient, data-sensitive indexing of embedded set elements under hinge distance."
We propose a NeuralODE training method using synchronization and homotopy optimization that reduces training epochs and improves extrapolation without changing the model architecture.
We propose a leverage score-based sampling algorithm that learns a two-layer GCN regression model efficiently by observing only $O(nd\epsilon^{-2}\log n)$ entries of the adjacency matrix $A$ with near-optimal accuracy.
"We analyze continuous-time anchor acceleration, characterize its convergence rate via the anchor coefficient, and propose an adaptive method with proven effectiveness."
"We prove that preconditioned gradient descent variants (ScaledGD and AltScaledGD) achieve global convergence in logarithmic iterations for low-rank matrix factorization, with AltScaledGD not requiring small learning rates or initialization."
"ARTIC3D is a self-supervised framework that reconstructs high-fidelity, per-instance 3D shapes from sparse, in-the-wild images with occlusions and truncation, leveraging skeleton-based surfaces and diffusion model guidance for robust and realistic results."
"AWRM and its implementation GALILEO improve learned environment dynamics models by addressing selection bias via adversarial data sampling, yielding better counterfactual prediction and downstream performance in synthetic and real-world tasks."
We define formal operational classes of locality to clarify and test biologically plausible synaptic plasticity models in the brain.
"Invariant Dropout enables adaptive, runtime sub-model offloading in federated learning to mitigate straggler-induced performance bottlenecks without sacrificing accuracy."
"We show that enforcing Dale's Law in recurrent neural networks degrades spectral properties of the recurrent weight matrix, impairing learning, and that this effect is more pronounced in small networks with few inhibitory units."
"We propose a ""first-encoding-then-separation"" framework with residual vector quantization and a task-agnostic self-supervised objective to learn molecular representations invariant and robust to distribution shifts, outperforming baselines on 18 molecular datasets."
"MolGroup predicts optimal auxiliary molecule datasets for a target dataset by combining graph structure and task similarity via a routing mechanism optimized with bi-level meta-learning, improving model performance by 4.41%/3.47% on average."
"We integrate SMT solvers into DNNs to leverage non-differentiable symbolic reasoning, reducing sample requirements, improving robustness to covariate shift, and producing interpretable, knowledge-consistent representations."
"TexQ is a zero-shot quantization method that uses texture feature energy distribution calibration and mixup knowledge distillation to synthesize high-quality samples, achieving state-of-the-art accuracy for ultra-low bit width quantization on CIFAR10/100 and ImageNet."
"We propose an adaptive tensorial factorization framework with differentiable tomographic modeling and regularization for efficient, high-quality 3D reconstruction from tilt-series cryo-ET data."
We propose meta-algorithms for online meta-learning with bandit feedback that improve task-averaged regret in multi-armed bandits and bandit linear optimization by tuning initializations and hyperparameters using task similarity.
GUT is a tracking-based decentralized learning method that improves test accuracy by 1-6% on heterogeneous data without communication overhead.
"We propose a learnable landscape surrogate to accelerate and stabilize the training of learned optimizers for challenging optimization problems, achieving superior performance with fewer solver calls."
"We introduce Iterative Markovian Fitting and the Diffusion Schrödinger Bridge Matching algorithm, a scalable numerical method for Schrödinger bridges that recovers recent transport methods and outperforms existing approaches."
"We propose a reinforcement learning algorithm that learns compressible action sequences via sequence priors, achieving faster learning and higher returns in continuous control tasks."
"We present a recurrent neural network that integrates gaze-contingent visual input and behavioral variables to accurately predict mouse primary visual cortex activity during natural behavior, revealing mixed selectivity for behavioral signals in V1."
"We show that DSIC and BIC mechanisms are strongly robust to correlated priors under total variation distance, enabling approximation of revenue-optimal and correlation-robust mechanisms and recovering or extending several prior results."
"<projektor> is a two-stage framework that predicts model performance and guides data selection from partially revealed data sources using Optimal Transport and a parameter-free scaling law-inspired extrapolation, outperforming existing methods in accuracy, efficiency, and selection effectiveness across diverse tasks."
TabMT is a masked transformer that generates high-quality synthetic tabular data with strong privacy guarantees and state-of-the-art performance across dataset sizes.
"Photoswap is a training-free method that enables seamless, personalized subject swapping in images by manipulating self- and cross-attention in pre-trained diffusion models, outperforming baselines in swapping quality and background preservation."
"Adaptive algorithms for recovering low-rank matrices from linear measurements require at least $\Omega(\log n / \log\log n)$ rounds when using $n^{2-\beta}$ measurements per round, even at the edge of the noise level."
We introduce a pathologist-guided framework that uses human feedback to train generative models for producing clinically plausible synthetic medical images.
"We propose a scalable Bayesian method using sparse variational Gaussian processes to model covariate-dependent variability in neural spiking, outperforming baselines on mouse and rat navigation data."
"We establish that the decentralized stochastic gradient descent ascent (D-SGDA) algorithm achieves algorithmic stability and generalization comparable to vanilla SGDA in both convex-concave and nonconvex-nonconcave settings, with generalization bounds affected by network topology."
"We prove that gradient descent with small random initialization converges to the ground truth in rank-1 symmetric matrix completion, entering the local convergence region in logarithmic iterations, with initialization bounds improving as sample size increases, due to implicit regularization preventing unbalanced entry growth."
"We present a universal neural audio compression method that achieves ~90x compression at 8 kbps with high fidelity by combining advances in audio generation and image-domain vector quantization, outperforming existing methods."
"We prove and demonstrate that in transformers, the rank of the difference between trained and initial weights increases during incremental learning, even under realistic conditions."
"Self-guidance enables precise image attribute control in diffusion models using internal representations, enabling complex manipulations without extra training or models."
"We propose convolutional visual prompts (CVP), a label-free, low-parameter test-time adaptation method that improves robustness of vision models to out-of-distribution samples by up to 5.87%."
We incorporate graph random-walk affinity features into message passing GNNs to improve expressivity and achieve state-of-the-art performance on node and graph prediction tasks with fewer message passing steps.
"Autoregressive Neural TensorNet (ANTN) is a novel architecture that combines tensor networks and autoregressive neural networks to more expressively and symmetrically parameterize quantum many-body states, outperforming both components on the 2D J₁–J₂ Heisenberg model."
"We present Value-Guided Data Filtering (VGDF), which improves online domain adaptation by filtering source domain data based on value target similarity, outperforming prior methods on environments with dynamics mismatch."
"Parsel is a framework that improves LLM performance on hierarchical multi-step reasoning tasks by automatically decomposing tasks and validating implementations with tests, significantly boosting pass rates on code and planning benchmarks."
"We investigate and improve adapter routing in parameter-efficient multi-task fine-tuning, showing that finer-grained routing (MHR) boosts performance and that adapting the average pre-trained adapter (MHR-μ) is effective for single-adapter and zero-shot transfer."
"We propose 1R2R, a model-based offline RL method that uses ensemble-estimated epistemic and aleatoric uncertainty and risk-aversion to jointly prevent distributional shift and achieve risk-aversion in stochastic environments."
We propose learning intrinsic rewards via bi-level optimization to improve conversational recommender system performance by maximizing success rate and minimizing conversation turns.
"We derive a gradient flow conservation law for GATs explaining poor trainability and worse performance in deeper models, and propose an initialization that enables training deeper GATs and speeds convergence."
"CoPriv reduces secure 2PC inference communication by jointly optimizing the protocol and DNN architecture, achieving up to 9.98× online and 3.88× total communication reduction with improved accuracy compared to existing methods."
"LambdaBeam is a neural-guided search algorithm that constructs arbitrary lambda functions to synthesize longer, more general programs with iterative loops and higher-order functions, outperforming prior neural, symbolic, and LLM-based methods in integer list manipulation."
"Wavelet Graph Diffusion Model (Wave-GD) addresses oversmoothing in graph generation by modeling node and edge dependencies in the spectral domain with a score-based diffusion model, producing synthetic graphs with realistic high-frequency characteristics."
"GESS improves generalized fMRI-to-image reconstruction by reducing the semantic gap via expanded semantics from CLIP and structural guidance, outperforming state-of-the-art methods."
"Gradient flow in two-layer ReLU networks, in the presence of weakly correlated cluster means, favors generalizing but non-robust solutions despite the existence of robust alternatives."
"We prove that a modified offline goal-conditioned reinforcement learning algorithm achieves near-optimal sample complexity with general function approximation under single-policy concentrability and realizability, and empirically outperforms the original in real-world environments without minimax optimization."
"We propose a neural framework for answering complex logical queries on eventuality-centric knowledge graphs, incorporating implicit temporal constraints and enhancing query encoding with memory mechanisms."
"We analyze the asymptotic distribution and power of the Hilbert-Schmidt independence criterion (HSIC) when covariate dimensions grow at different rates, revealing how its ability to detect nonlinear dependence varies with sample size and dimensionality."
"We introduce monitor-guided decoding, which uses static analysis to improve code generation by language models, especially for repository-level context, outperforming larger models and generalizing across languages and constraints."
Read and Reward improves Atari RL efficiency by using instruction manuals to provide auxiliary rewards via information extraction and reasoning.
"TriRE is a continual learning paradigm inspired by brain mechanisms that concurrently retains, revises, and promotes neurons for tasks, significantly reducing forgetting and outperforming isolated approaches."
"We present a non-parametric Bayesian stochastic decision tree model using variational inference, competitive in regression and applicable to causal inference, with a fully vectorized PyTorch implementation."
FedICON is a federated learning framework using contrastive learning to mitigate test-time feature shifts by leveraging inter-client heterogeneity for invariant feature extraction and test-time adaptation.
"We present new sketching-based algorithms and lower bounds for sparse dictionary learning and k-means clustering in data streams, achieving near-optimal space complexities and extending PTAS results to sparse dictionary learning while highlighting fundamental limitations."
"We propose PoNoS, a nonmonotone line search method for SGD/Adam that achieves faster convergence and improved generalization without requiring monotone decrease, and introduce a resetting technique that minimizes backtracking while maintaining large initial step sizes."
"We show that while bounded-depth Transformers cannot directly solve basic arithmetic and dynamic programming tasks without super-polynomial growth, they can solve these tasks with Chain-of-Thought prompting by generating step-by-step solutions, explaining the effectiveness of CoT in LLMs."
"We analyze gradient descent with linearly correlated noise, providing tighter and exact characterizations for both convex and non-convex settings, and demonstrate improved differentially private optimization via novel matrix factorizations."
"We propose VLATTACK, a framework that generates image and text perturbations to effectively attack black-box fine-tuned vision-language models across multiple downstream tasks, outperforming existing baselines."
We prove that complex-valued neurons learn real-valued and complex-valued neurons faster than real-valued networks can learn complex-valued neurons.
"SDAC is a trust region-based safe RL algorithm that efficiently handles multiple risk-averse constraints in robotic tasks, achieving significantly fewer constraint violations and faster constraint satisfaction compared to baselines."
"We introduce Discontinuous ReLU networks to enable deep learning-based automated design of optimal contracts by modeling utility as a discontinuous piecewise affine function and solving for incentive-compatible, utility-maximizing contracts via linear programming."
"We introduce Proxy Value Propagation, a reward-free method that uses a proxy value function learned from human demonstrations and interventions to optimize policies, enabling efficient imitation learning across continuous and discrete control tasks with minimal algorithmic modification."
TripleEagle is a novel algorithmic framework for designing budget-feasible mechanisms for submodular valuations that achieves better approximation ratios and linear complexity with obvious strategyproofness.
"TSDiff is an unconditional diffusion model for time series with a self-guidance mechanism enabling task-specific conditioning at inference, outperforming or matching conditional models on forecasting, refinement, and synthetic data generation tasks."
Diff-Foley is a latent diffusion model for video-to-audio synthesis that uses contrastive audio-visual pretraining and double guidance to achieve state-of-the-art synchronization and audio-visual relevance in generated audio.
"DiET adapts black-box models to be robust to distractor erasure, producing faithful and discriminative feature attributions that closely approximate original model behavior and ground-truth explanations."
"We propose SubSelNet, a non-adaptive, attention-based subset selection framework that uses a neural gadget to approximate model predictions and enables fast, architecture-agnostic subset sampling, outperforming existing methods on real datasets."
We train deep neural networks to predict human cortical responses to natural scene images and use explainable AI techniques to map interpretable visual feature selectivity across specific brain regions.
Neural k-Opt (NeuOpt) introduces a learning-to-search solver for routing problems that explores both feasible and infeasible regions via a novel GIRE scheme and outperforms existing solvers on TSP and CVRP.
"We prove identifiability of causal representations from unknown single-node Gaussian latent interventions with general mixing functions, using high-dimensional geometric analysis of precision matrices and a contrastive algorithm for latent variable identification."
"We propose a theoretically grounded regularization method, SPQR, that enforces Q-ensemble independence via spectral distribution matching and demonstrates improved performance in online and offline reinforcement learning."
"We show that maximum likelihood estimation for probabilistic principal component analysis is consistent in a quotient space, resolving parameter identifiability issues caused by rotational symmetry."
"StateMask is a novel reinforcement learning explainer that identifies critical states for an agent’s final reward by masking state inputs during execution without degrading performance, outperforming existing methods in fidelity and utility."
We derive tighter f-DP bounds for differentially private shuffling and DP-GD with random initialization by analyzing the effects of randomness on privacy guarantees and introducing an inequality that establishes joint convexity of F-divergences.
FASTEN is a 3D mask detection framework using flow and attention-based spatio-temporal aggregation that achieves high accuracy with only five frames and is deployable on mobile devices.
We give a PTAS for learning random constant-depth networks with poly or quasi-poly time and sample complexity in the network size and error parameter.
"GraphPatcher is a model-agnostic test-time augmentation framework that improves GNNs' performance on low-degree nodes by progressively reconstructing predictions under node corruption, without degrading performance on high-degree nodes."
Self-AIXI is a universal agent that maximizes learning by self-predicting its action stream and converges to AIXI while inheriting properties like maximal intelligence and self-optimization.
"We propose FLIP, a label-only backdoor attack that achieves near-perfect attack success with minimal label corruption and negligible clean accuracy loss on standard datasets and architectures."
"DPMORL extends multi-objective reinforcement learning to optimize policies over distributions of returns, enabling discovery of distributional Pareto-optimal policies that better handle uncertainty and diverse preferences in real-world tasks."
"We propose variational score distillation (VSD), a particle-based framework for text-to-3D generation that models the 3D parameter as a random variable, addressing over-saturation, over-smoothing, and low diversity in score distillation sampling, and introduce improvements like distillation time schedule and density initialization to achieve high-fidelity, diverse 3D outputs."
"We reformulate Quantum Variational Monte Carlo using Wasserstein gradient flow in the space of Born distributions, yielding faster convergence to molecular ground states than traditional methods."
"We propose π-KRVI, an optimistic least-squares value iteration method using RKHS with first-order optimal regret for large state-action spaces and non-smooth kernels, achieving sublinear regret where previous methods failed."
"We propose a method that makes arbitrary base models equivariant to any group by coupling them with a small equivariant network parameterizing a learned probability distribution, achieving equivariance and universal approximation with competitive performance and improved sample efficiency."
"FedFed addresses data heterogeneity in federated learning by globally sharing performance-sensitive features while keeping performance-robust features local, improving model performance."
We show that combining recurrent networks with hypernetworks yields stronger meta-reinforcement learning performance than specialized methods.
"We introduce the Blackwell discount factor γ_bw for MDPs and show that discount-optimal policies with discount factor above γ_bw are Blackwell- and average-optimal, providing a general upper bound on γ_bw and polynomial-time algorithms without additional assumptions."
"We show that sample complexity for matrix-parameterized linear predictors and neural networks, under Frobenius norm constraints, can differ from scalar cases and yields new learnability results."
"We propose BiSRNet, a binarized spectral-redistribution convolutional network that efficiently reconstructs hyperspectral images from compressive measurements with superior performance and low hardware requirements."
SGFD improves visual RL generalization by decorrelating features via saliency-guided sample reweighting using Random Fourier Functions.
"Free-Bloom generates temporally and semantically coherent text-to-video by using LLMs to create prompt sequences and LDMs to produce frames, with novel reverse-process modifications, achieving high-quality zero-shot video generation without training or video data."
"We analyze the trade-off between labeled and unlabeled sample sizes in binary classification with mixture distributions under MMD separation, and empirically confirm this trade-off in Higgs boson detection and CIFAR-10/DDPM image detection tasks."
We propose a framework that synthesizes programs from partially observed robot environments using environment embeddings and graph-based information flow for improved generalization and robustness.
"Smooth fictitious play achieves both consensus within and equilibrium across interacting sub-populations in multi-agent systems, with consensus crucial for equilibrium selection."
"We propose a principled weight initialization for Input-Convex Neural Networks (ICNNs) that leverages non-negative weights and non-decreasing convex activations, enabling faster learning, better generalization, and effective training without skip-connections in ICNNs applied to molecular latent space exploration."
"RETVec is an efficient, resilient, multilingual text vectorizer that uses novel character encoding and pre-training with pair-wise metric learning to improve robustness to typos and adversarial attacks, outperforming state-of-the-art methods."
"CorresNeRF improves NeRF performance in sparse-view scenarios by integrating off-the-shelf image correspondences as supervision during training, enhancing both novel view synthesis and surface reconstruction across various NeRF models."
"AURORA introduces a lightweight parameter-efficient framework for cross-modal transfer learning that significantly reduces trainable parameters and enhances modality alignment, outperforming state-of-the-art and full fine-tuning methods on six cross-modal benchmarks."
UniHOI leverages Vision-Language foundation models and large language models to achieve superior open-world human-object interaction detection via prompt-guided high-level relation extraction and linguistic interpretation.
"VCD improves camera-only 3D object detection by introducing an apprentice-friendly multi-modal expert and trajectory-based distillation, achieving 63.1% NDS on nuScenes."
"ESSEN is a novel framework that measures temporal network evolution using von Neumann entropy and thermodynamic temperature, incorporating entropy-aware attention and a Mixture of Thermodynamic Experts decoder, and demonstrates improved link prediction performance over state-of-the-art baselines."
"KaRR quantifies LLM factual knowledge by estimating the probability of correct answer generation across diverse prompts, revealing that model size increases knowledge while instruction tuning can reduce factual reliability."
"PRODIGY is a pretraining framework that enables in-context learning on graphs via a prompt graph representation and graph neural network objectives, outperforming contrastive pretraining and standard fine-tuning baselines on citation and knowledge graph tasks."
"We extend cumulants to reproducing kernel Hilbert spaces via a computationally tractable kernel trick, recovering classical statistics and demonstrating practical advantages by using higher-degree objects with minimal overhead."
"We propose Variance-UCB, an active learning algorithm for estimating group means that achieves optimal $O(T^{-2})$ regret for finite $p$-norm variance and $O(T^{-1.5})$ for the infinite norm, with matching lower bounds."
"We propose and analyze single- and tri-level non-smooth weakly-convex finite-sum coupled compositional optimization algorithms, and demonstrate their effectiveness in deep learning for two-way partial AUC maximization."
We propose history encoders inspired by PID controllers that improve robustness and performance in deep RL for control tasks with unobservable states.
"We show that for two-layer neural networks trained on data in a low-dimensional linear subspace, standard training produces non-robust networks with large gradients orthogonal to the data subspace, but reducing initialization scale or adding L2 regularization increases robustness to orthogonal adversarial perturbations."
We propose an exploration strategy for learning controllers in nonlinear dynamical systems that targets the most task-relevant system parameters and provably reduces controller loss.
"We propose prompt retrieval methods to automatically select effective visual in-context examples, significantly improving in-context learning for large vision models without requiring access to their internal weights."
"Given a probabilistic causal graph, identifying the most plausible identifiable subgraph for a specific causal effect reduces to an NP-hard combinatorial optimization problem, for which we propose efficient approximation algorithms and provide empirical evaluations."
"FlatMatch improves semi-supervised learning by aligning learning performance between labeled and unlabeled data via a cross-sharpness minimization approach, yielding state-of-the-art results."
"We show that simple algebraic transformations can effectively translate representations between latent spaces of different neural networks, enabling zero-shot stitching of encoders and decoders across architectures and modalities."
"We propose and theoretically analyze a consistent, robust estimator of the Bayes error rate for multi-class classification, validated on synthetic and real data under label noise and outlier conditions."
"Rewritten abstract in one short sentence:

We introduce a continuously trainable graph convolutional network depth parameter, enabling TEDGCN to automatically adapt to both homophily and heterophily in graphs without prior knowledge."
"VALOR improves audio-visual event recognition in the unaligned setting with weak video-level labels by using contrastively pre-trained modality-independent teachers, achieving a substantial gain over prior methods on the Look, Listen, and Parse dataset and generalizing to event localization."
"Decomposing the text-to-SQL task into sub-tasks and feeding their solutions to LLMs significantly improves performance, achieving new state-of-the-art execution accuracy on Spider (85.3%) and BIRD (55.9%)."
"We show that retraction-based stochastic Riemannian optimization methods almost surely avoid strict saddle points under mild assumptions, ensuring that their limit is a local minimizer."
"We explain how GPT-2 small uses a specific circuit to compute ""greater-than"" relationships in year prediction tasks and show this mechanism generalizes across contexts."
"SELF-ALIGN enables self-supervised alignment of large language models using minimal human supervision by generating diverse prompts, applying a few principles via in-context learning, fine-tuning, and refinement, resulting in an AI assistant that outperforms strongly supervised models on benchmark tasks."
"We propose integrating a differentiable, invertible normalizing flow with DDPG to efficiently enforce action constraints in ACRL, reducing constraint violations and improving speed without optimization-based projection."
"TempBalance, a layer-wise learning rate method inspired by Heavy-Tailed Self-Regularization Theory, improves neural network test performance across multiple architectures and datasets compared to standard SGD and advanced optimizers."
"We adapt an image CLIP model to video using self-supervised learning with language-guided action concepts, improving action recognition performance."
"MIMONets enable deep neural networks to process multiple inputs simultaneously with fixed-width representations, achieving significant speedups with minimal accuracy loss."
"We derive tight bounds for KL divergence between multivariate Gaussians, showing approximate symmetry for small divergence and a relaxed triangle inequality, with dimension-independent results and applications in deep learning and reinforcement learning."
We propose a linear-time multi-swap local search algorithm for k-means that achieves a $(50(1+\frac{1}{t})+\epsilon)$-approximation and improves upon previous results with practical speedups and recombination.
"We propose deep mutual learning to enhance Bayesian Neural Networks by increasing parameter and feature diversity, significantly improving classification accuracy, negative log-likelihood, and calibration error over traditional methods."
"We achieve $\tilde{\mathcal{O}}(\sqrt{T})$ regret for adversarial linear contextual bandits without a simulator and with efficient computation when the action set is small, answering an open question for the stochastic sleeping bandits case and handling additive misspecification."
We introduce SCH stability and a neighboring-hypothesis matrix to achieve sharper information-theoretic generalization bounds that overcome limitations in stochastic convex optimization.
Point-In-Context is a novel in-context learning framework for 3D point clouds that addresses token masking and position embedding issues and outperforms individually trained models with effective prompt selection.
"We study the multi-fidelity multi-armed bandit problem, providing cost complexity bounds and algorithms for best arm identification and regret minimization with new definitions and tight bounds."
SPIE is a reinforcement learning exploration method that combines prospective and retrospective state information to improve efficient exploration in sparse-reward environments.
"We propose Disentanglement based Cognitive Diagnosis (DCD), a model that disentangles student proficiency, exercise difficulty, and concept label distributions using a tree-like concept structure to effectively diagnose cognition from limited exercise labels."
"We present Maximum Manifold Capacity Representations (MMCR), a computationally efficient method to optimize a novel linear-separability-based efficiency metric for self-supervised learning, showing competitive performance with state-of-the-art models and insights into manifold compression and class separability."
"We propose a computationally efficient algorithm for the multinomial logistic bandit problem achieving $\tilde{\mathcal{O}}(K\sqrt{T})$ regret with constant per-round complexity, outperforming prior methods in both regret and computational cost."
"Graph Neural Networks exhibit performance disparity between homophilic and heterophilic nodes due to differences in aggregated feature distance and homophily ratios, as theoretically and empirically analyzed with a new PAC-Bayesian generalization bound."
"FedGCN is a federated learning algorithm for graph convolutional networks that reduces communication overhead by limiting inter-client exchange to a single pre-training step, achieving faster convergence and significantly less communication than prior methods while maintaining accuracy."
"We prove a Statistical Query lower bound showing that learning mixtures of linear classifiers under Gaussian covariates requires sample complexity $n^{\mathrm{poly}(1/\Delta)\log(r)}$, where $\Delta$ is the minimum pairwise $\ell_2$ separation of the unit vectors."
"We propose Fine-grained Goal Prompting, which uses high-resolution goal image features as prompts to enhance goal understanding and observation attention, significantly improving image-goal navigation performance on Gibson, MP3D, and HM3D, especially achieving an 8% higher success rate than the state-of-the-art on Gibson with minimal model size."
"We introduce TFLEX, the first temporal complex query embedding framework for multi-hop logical reasoning on temporal knowledge graphs that uses fuzzy logic to model both entity set and temporal timestamp set operations."
"We introduce poLinUCB, an algorithm for contextual bandits with post-serving contexts that achieves tight regret using a robustified Elliptical Potential Lemma, and show its superior performance empirically."
"We present a novel method that uses a reduced-dimension enumeration oracle within a Frank-Wolfe variant to learn polyhedral facets and generate strong cutting planes for large-scale integer programming, demonstrated on the multidimensional knapsack problem."
We propose a softmax output approximation method that reduces attention-based model activation memory by up to 84% during training while maintaining or improving performance.
"We analyze self-supervised learning through the lens of the Laplace operator, connecting augmentation-based inductive bias to low-rank matrix completion to explain convergence and downstream performance."
"Self-supervised training using unbiased gradient estimates achieves supervised-level performance but requires more data than supervised training, with the gap closing at a problem-dependent rate as sample size increases."
"We propose KARI and BEP, a grammar-based method that improves temporal action segmentation performance and interpretability by inducing context-free grammars from action data and parsing frame-level predictions accordingly."
"We propose Constrained Neural Fields (CNF), a framework that enforces hard constraints via linear operators on neural fields, addressing limitations of standard deep learning in constrained optimization and enabling efficient model and constraint specification for real-world applications."
"We propose GWG, a generalized ParVI framework using a broader class of regularized Wasserstein gradient flows for KL divergence with strong convergence and adaptive metric selection, demonstrating improved effectiveness and efficiency."
"BHMC is a Hamiltonian Monte Carlo method for sampling on a Riemannian manifold using a barrier-derived metric, featuring an involution checking step to remove bias and ensure reversibility, as demonstrated on polytopes."
"We propose a bi-level optimization method for offline inverse reinforcement learning that jointly models environment uncertainty and expert policy conservatism, improving reward estimation and outperforming state-of-the-art baselines on MuJoCo and D4RL benchmarks."
"We prove that DiskANN with slow preprocessing achieves constant approximation and poly-logarithmic query time for bounded intrinsic dimension, while DiskANN with fast preprocessing, HNSW, and NSG can require linear query time to find a reasonable number of nearest neighbors in certain instances."
"We propose UniT, a unified training framework that certifies robustness in word embedding space and improves base model robustness via decoupled regularization, enhancing certified robust accuracy on text classification tasks."
"DreamRec reframes sequential recommendation as a generative task using a guided diffusion model to directly produce the oracle item from user interaction history, avoiding negative sampling and better capturing true user preferences."
"We propose a PPO variant that adaptively integrates analytical gradients using an α-policy and gradient quality metrics, outperforming baselines in function optimization, physics, and traffic control tasks."
"Directional diffusion models improve unsupervised graph representation learning by using anisotropic, data-dependent noise in the forward diffusion process, outperforming baselines on multiple graph datasets."
"CUOLR unifies off-policy learning to rank under a general stochastic click model using offline reinforcement learning, outperforming state-of-the-art methods without requiring click model assumptions or debiasing."
We propose Distributional Maximum Entropy Policy Evaluation with state aggregation and progressive factorization to balance representation bias and complexity in distributional reinforcement learning policy evaluation.
"We present an SVD-based algorithm for recovering unbalanced communities in the stochastic block model, achieving nearly optimal recovery (up to poly-log factors) for constant edge probabilities without restrictions on cluster size gaps or the number of clusters, and enabling sublinear-time detection of large clusters even when many small clusters are present."
"We introduce regression with cost-based rejection and show that optimal models should reject predictions when the variance exceeds the rejection cost under mean squared error, with a surrogate loss ensuring consistency."
"We propose a diffusion-based label-noise learning method with neighbor-consistency pseudo-labeling that achieves new state-of-the-art results on real-world datasets, with significant accuracy gains when using pre-trained models like CLIP."
"We introduce Recursion in Recursion (RIR), a two-level nested recursive neural network that achieves logarithmic-depth computation and strong length generalization on ListOps while remaining scalable for long sequences from LRA, outperforming Transformers and matching Structured State Space Models in language tasks."
"We use Quality Diversity algorithms to train Neural Cellular Automata generators for scalable, pattern-consistent, arbitrarily large multi-robot warehouse environments, supporting up to 2,350 robots."
"We propose RES, a certifiably robust randomized edgedrop smoothing technique that ensures and certifies the robustness of any Graph Contrastive Learning model against adversarial attacks."
"We show that conditional independence tests on exchangeable data from independent causal mechanism generative processes identify unique causal structures, unlike i.i.d. data which only yields Markov equivalence classes."
"We show that, under a generalized faithfulness assumption, the latent causal model can be identified from unpaired observational and interventional data with unobserved causal variables, enabling prediction of unseen intervention effects and being implemented via autoencoding variational Bayes for genomics."
"We formalize and demonstrate that irregular observation policies outperform regular ones in continuous-time control with costly observations, highlighting the need for new solution methods."
"We show that fairness interventions can increase predictive multiplicity, necessitating methods like an ensemble algorithm to improve prediction consistency."
"We propose decoupling and independently learning the mean and covariance in GP conditionals, improving regression and Bayesian optimization performance while maintaining scalability."
"The paper improves off-policy evaluation data-efficiency by learning a state-action encoder using an OPE-tailored behavioral similarity metric, enabling more accurate and robust fitted q-evaluation under distribution shifts."
"We demonstrate that in synthetic additive noise model data, sorting variables by increasing coefficient of determination ($R^2$) yields a causal order, enabling an efficient causal discovery algorithm that outperforms existing methods."
We propose a method that extracts minor-class knowledge from all training samples using re-balanced attention and labels to improve facial expression recognition under class imbalance.
"CLIP4HOI improves zero-shot HOI detection by using a disentangled two-stage approach with independent human and object detection and a fine-grained HOI classifier based on CLIP, achieving state-of-the-art results on rare and unseen categories."
"We introduce Truncated Affinity Maximization (TAM), which leverages one-class homophily in graphs to learn node representations that enhance local node affinity, significantly improving unsupervised graph anomaly detection."
"The Marginal Ratio (MR) estimator reduces variance in off-policy evaluation for contextual bandits compared to IPW and DR, and outperforms MIPS in both theory and practice."
"We propose a differentiable deep feature selection method using Dirichlet Energy and Optimal Transport to learn k-NN graphs, validated on synthetic and real datasets."
"Sharpness-Aware Minimization (SAM) with constant perturbation and gradient normalization often fails to converge to global minima or stationary points except in specific convex settings, where deterministic SAM achieves optimal rates but stochastic SAM only converges to neighborhoods of optima due to unavoidable error terms."
"NIM, a noisy image modeling variant of masked image modeling, improves adversarial robustness in downstream classifiers via denoising and offers tunable trade-offs comparable to adversarial training."
We present a post-processing method that resamples synthetic data to improve user-defined utility while preserving privacy and quality.
"We introduce Lion, a simple, memory-efficient optimizer discovered via program search, which outperforms Adam and other optimizers on diverse deep learning tasks, especially at large batch sizes."
"We use the replica trick to show that row correlations in the first layer of Gaussian features improve generalization, while structure in later layers is harmful, in a solvable many-layer model."
"We propose using diffusion models to compress a QD-RL policy archive into a single generative model, achieving 13x compression while retaining 98% of original rewards and 89% of coverage."
"We propose a neural method that predicts principal components of the posterior uncertainty in image restoration tasks in a single forward pass, providing instance-adaptive uncertainty quantification far faster than sampling-based methods."
"We stabilize Regret Matching+ with restarting and orthant-chopping, achieving O(T^{1/4}) individual and O(1) social regret in normal-form games and improved performance in extensive-form games."
"We propose and validate a fairness-aware weak supervision method that improves both accuracy and fairness, outperforming baselines and achieving state-of-the-art results on multiple datasets."
"We propose a federated learning method that enables successive freezing and training of model parameters on resource-constrained devices, improving model accuracy by 52.4 percentage points over state-of-the-art subset training approaches while efficiently utilizing distributed computation."
FedGame defends federated learning against strategic backdoor attackers by modeling defender-attacker interactions as a minimax game and empirically outperforms existing defenses.
"RegBN is a parameter-free, regularization-based batch normalization method that normalizes heterogeneous multimodal features to mitigate confounding and dependencies, improving performance across diverse modalities and architectures."
"We propose a descent-style algorithm that recovers a low-rank matrix from noisy linear measurements in a semi-random matrix sensing model with adversarial matrices, addressing the limitations of non-convex optimization by adaptively reweighting inputs based on current estimates."
"SNK is a zero-shot, reconstruction-based method for non-rigid shape matching that predicts unsupervised functional maps and generates realistic deformations without requiring training or ground truth data."
"We present a differentiable clustering method using stochastic perturbations of minimum-weight spanning forests, enabling end-to-end training with efficient gradients and effective performance on noisy and complex data, including learning from partial labels."
"Boundless DAS discovers that Alpaca implements an interpretable causal model with two boolean variables for numerical reasoning, and that this causal alignment is robust to input changes."
"We introduce a weak learning condition for multiclass classification and a boosting algorithm with complexity independent of the number of classes, and apply it to show equivalence to weak PAC learning and to simplify list PAC learning algorithms and proofs."
DBE is a federated learning framework that reduces domain discrepancy to mitigate representation degeneration and significantly improves generalization and personalization performance over state-of-the-art methods.
"We compress optimizer states to 4-bit using improved quantization that leverages row and column information and a linear quantizer without zero point, achieving full-precision accuracy with better memory efficiency across multiple tasks."
We propose a spatio-temporal pre-training framework with a mask autoencoder and adaptive masking to improve downstream traffic prediction models by learning robust spatio-temporal dependencies and region relationships.
"We propose PORTAL and Ada-PORTAL, sample-efficient algorithms with theoretical guarantees for reinforcement learning in episodic low-rank nonstationary MDPs with unknown representations."
"We introduce a stable, efficient, and information-rich vector space representation for multiparameter persistent homology that enables integration with machine learning and validate it with numerical experiments on real data sets."
"We propose graph structural distributional shifts via node properties and show that existing graph models struggle with them, revealing a trade-off between base task performance and distribution separability."
"PromptRestorer uses perceived degradation features via a prompting branch and gating mechanism to guide restoration, achieving state-of-the-art results on four image restoration tasks."
"We generalize H-consistency bounds for surrogate losses in multi-class classification using error transformations, providing tighter, more general bounds for constrained and comp-sum losses under realistic hypothesis sets, and correcting a recent weak bound for cross-entropy."
"The dynamics gap between predictor evaluation on fixed datasets and interactive real-world driving causes a significant disparity in trajectory prediction performance, necessitating an interactive, task-driven evaluation protocol."
"We present a regret-optimal, low-burn-in, model-free algorithm for tabular infinite-horizon discounted MDPs that uses variance reduction and adaptive policy switching."
"We propose a Bayesian, instance-dependent label correction method with theoretical guarantees, validated on benchmark and real-world datasets."
We present architectural modifications and a novel contrastive learning method that enable synchrony-based models to unsupervisedly discover multiple objects in color datasets with more than three simultaneous representations.
"Public-private learnability of a distribution class is equivalent to the existence of a sample compression scheme and is linked to list learning, yielding new sample complexity bounds for mixtures of Gaussians and showing that at least $d$ public samples are necessary for private learning of Gaussians in $\mathbb{R}^d$."
"We propose FouriDown, a learnable Fourier-domain downsampling method that adaptively addresses aliasing and outperforms static methods in image restoration tasks."
"Regularized no-regret learning in finite N-player games converges to sets of strategies that are closed under better replies, with entropic regularization achieving geometric convergence and projection-based methods converging in finitely many steps even with bandit feedback."
"We propose Low-Rank Graph Neural Network (LRGNN), which leverages the low rank of the signed graph's label relationship matrix to effectively model both homophilous and heterophilous graphs."
LoReTTa is a self-supervised pre-training strategy that enables multimodal transformers to model and infer previously unseen modality combinations using only pairwise modality data.
"We extend Haim et al.'s sample reconstruction method from binary MLPs to multiclass and convolutional networks, generalize it to various loss functions, and show that weight decay and neuron-to-sample ratio affect reconstructability."
EAGLE improves out-of-distribution generalization in dynamic graph neural networks by modeling and inferring latent environments and extracting invariant spatio-temporal patterns.
"We introduce Clifford Group Equivariant Neural Networks that achieve O(n)- and E(n)-equivariance via a subgroup acting on the Clifford algebra, enabling expressive, dimension-agnostic equivariant layers and state-of-the-art performance across 3D n-body, 4D Lorentz, and 5D convex hull tasks."
"We formalize learning from explanation constraints and show that models satisfying such constraints in expectation can reduce Rademacher complexity, with empirical improvements demonstrated for linear models and two-layer neural networks."
"We introduce a more realistic heterogeneity model and derive tighter learning error lower and upper bounds for robust distributed gradient descent, showing improved alignment with empirical results."
"We propose Distributionally Robust AUC (DRAUC), a tractable and label-unbiased surrogate for robust AUC optimization under distribution shift, and show its effectiveness empirically and theoretical guarantees for generalization."
"We introduce min-SWGG, a computationally efficient upper bound of Wasserstein distance based on one-dimensional projections, with closed-form expressions and an associated transport plan, and demonstrate its effectiveness in gradient flows and shape matching."
"2Direction is a new accelerated distributed convex optimization method that achieves improved communication complexity over prior and vanilla AGD methods, with theoretical guarantees and experimental validation."
"We show that, without additional distributional assumptions, no mean estimator can asymptotically beat the sub-Gaussian error rate, even when beneficial features exist, and introduce a ""neighborhood optimality"" framework where median-of-means is nearly optimal up to constant factors."
"The Thinker algorithm enables reinforcement learning agents to autonomously plan and act using a learned world model, achieving state-of-the-art results in Sokoban and competitive results in Atari, and is the first to show RL agents can learn to plan with a learned world model in complex environments."
"We propose a three-step method for reliable off-policy learning of optimal dosage combinations in personalized medicine, addressing joint treatment effects and limited overlap in covariate-treatment space."
We propose an efficient sampling algorithm from a stochastic differential equation using a positive semi-definite probability model that achieves i.i.d. sampling with error ε in Wasserstein-1 distance and sub-curse-of-dimensionality complexity under sufficient solution regularity.
"We present a NeRF method that renders raw, time-resolved single-photon lidar histograms from novel views by modeling transient light transport, enabling improved geometry and appearance recovery from few viewpoints and expanding NeRFs to transient timescales."
"We provide a theoretical framework showing that there exist generative models with sufficient expressivity, polynomial parameterization, and benign optimization landscapes for broad combinatorial problems, and introduce a novel regularization for gradient descent to mitigate vanishing gradients and escape poor stationary points."
We develop formal tools for non-parametric decomposition of spurious effects in Markovian and semi-Markovian models and prove their identification under sufficient conditions.
"LQAE aligns images and text unsupervisedly by quantizing image embeddings into text tokens using a pretrained language model, enabling few-shot multimodal learning with large language models and outperforming baselines on image classification and VQA with minimal paired data."
"We propose a first-principles auditing framework for differentially private machine learning using randomized canaries and lifted differential privacy, achieving improved sample efficiency and compatibility with advanced canary designs."
"We show that for symmetric distributions, including product Cauchy and elliptical distributions, the mean can be robustly estimated with optimal error and sample complexity without moment assumptions, using a generalized filtering technique with Huber loss and sum-of-squares proofs."
"We present recursive cutting-plane algorithms that achieve a provable memory–oracle trade-off for constrained feasibility and convex optimization in the sub-polynomial regime, outperforming gradient descent when $\epsilon \leq d^{-\Omega(d)}$."
"We characterize exactly computable reward functionals in finite-horizon undiscounted MDPs, showing only generalized means are optimizable, while Distributional RL allows approximate evaluation with error bounds, advancing theory on risk-aware strategies."
"Encoding time series as digit strings enables large language models to forecast future values with performance rivaling specialized time series models, due to their ability to model multimodal distributions and handle missing data and side information, though model size and tokenization affect results."
"LightSim is a neural lighting camera simulation system that generates realistic, controllable urban scene datasets under diverse illumination for improved training of image-based robot perception systems."
SEENN dynamically adjusts the number of timesteps per input in spiking neural networks using confidence thresholding or reinforcement learning to reduce inference time while maintaining accuracy.
"ReinMax approximates gradients for discrete latent variables using a second-order method with negligible overhead, outperforming the Straight-Through heuristic and current state-of-the-art methods."
"We theoretically show that deep neural networks find it harder to learn interactive concepts involving more input variables, explaining why complex concepts are less likely to be learned."
Clustered conformal prediction improves class-conditional coverage for multi-class problems with limited labeled data per class by clustering similar classes before prediction.
"Representation Distinction (RD) improves offline RL by differentiating in-sample and out-of-distribution representations and dynamically adjusting to prevent collapse, significantly boosting policy performance on D4RL continuous control tasks."
We propose a hierarchical Gaussian mixture task generative model that jointly addresses multi-source task distributions and unseen novel tasks in meta-learning via density estimation and end-to-end learning.
"InfoPrompt improves soft prompt tuning by maximizing mutual information between prompts and model parameters, leading to faster convergence and better performance."
We propose a few-shot weakly supervised multiple instance learning framework for pathology WSI classification using prompt learning and GPT-4 to integrate language priors for both patch and slide-level classification with limited labeled data.
"OWL-ST enables Web-scale self-training for open-vocabulary object detection, significantly improving rare-class AP by leveraging pseudo-box annotations from image-text pairs."
"We propose SACL-XD, a self-supervised contrastive learning method that enables lightweight models to achieve high accuracy without requiring a pre-trained teacher or excessive computational resources."
We introduce a robustness notion accounting for task equivariance and provide architecture-agnostic certification via equivariance-preserving randomized smoothing and architecture-specific graph edit distance certificates.
"We present a polynomial-time tester-learner for testable learning of halfspaces under Gaussian distributions with adversarial label noise, achieving error close to the best halfspace and enabling efficient active learning with polylogarithmically many labels."
BirDRec is a model-agnostic framework that theoretically and practically addresses unreliable input-target pairs in sequential recommendation by rectifying both unreliable inputs and targets during training.
"The independent natural policy gradient algorithm asymptotically finds an $\epsilon$-Nash Equilibrium in multi-agent Markov potential games in $\mathcal{O}(1/\epsilon)$ iterations, improving upon previous $\mathcal{O}(1/\epsilon^2)$ bounds and matching the single-agent case."
"We propose an unsupervised face animation method that combines a global affine motion model with a novel learned motion refinement module to better capture fine local facial motions, outperforming state-of-the-art baselines on standard benchmarks."
"We introduce label informativeness, a graph characteristic that better correlates with GNN performance than existing homophily measures, which suffer from significant definitional drawbacks."
"We propose using dataset-dependent priors to canonicalize inputs for large pretrained models, achieving equivariance and improved robustness to transformations without sacrificing performance."
We propose a graph pre-training framework that selects representative data using predictive uncertainty to achieve better performance with fewer training samples.
"We propose list-level alignment for invariant representation learning in ranking, deriving the first theoretical generalization bound for domain adaptation in ranking and demonstrating improved unsupervised transfer performance."
"We propose a cubic regularization-based method using stochastic relaxation to optimize hyper-parameters when hyper-gradients are unavailable, theoretically and empirically demonstrating improved convergence and avoidance of local optima."
"We propose a self-supervised method that discovers unknown categories at test time by assigning minimal-length codes to data instances, enabling efficient, hierarchical category discovery with theoretical optimality guarantees."
"StyleTTS 2 achieves human-level text-to-speech synthesis using style diffusion and adversarial training with large speech language models, outperforming prior models on single- and multi-speaker benchmarks."
"DiffComplete is a diffusion-based 3D shape completion method that improves realism, fidelity, and multi-modality by using hierarchical feature aggregation and occupancy-aware fusion, achieving state-of-the-art results and strong generalization without retraining."
"Diffusion generative models exhibit a symmetry-breaking phase transition that separates reversible early dynamics from data-manifold attraction, enabling a Gaussian late initialization that improves generation quality and diversity."
"We propose CARE, a probabilistic graph ODE model with a context variable for time-varying interacting dynamical systems, and show its effectiveness on four datasets."
"We propose H-InDex, a human hand–informed visual representation learning framework that outperforms baseline and foundation models on challenging dexterous manipulation tasks using minimal parameter updates."
We propose a classifier-based conditional mutual information test with local null sampling that efficiently controls type I error and achieves high power for CI testing even with high-dimensional conditioning and small datasets.
"We propose SPA, a graph spectral alignment framework with spectral regularization and neighbor-aware self-training that improves unsupervised domain adaptation by aligning domain graphs in eigenspaces and enhancing target domain discriminability."
"ProtoDiff is a meta-learning framework that uses a task-guided diffusion model to robustly generate task-specific prototypes from few examples, improving few-shot classification performance."
"We propose a distributed optimization and federated learning method with variance reduction, partial participation, and communication compression that achieves optimal oracle and communication complexity without requiring bounded gradients."
"We propose RCL, the first ML-augmented algorithm with provable robustness guarantees for SOCO with multi-step switching costs and feedback delay, which combines ML predictions with an expert algorithm via constrained projection and improves both robustness and average performance."
"We provide tight Hessian-dependent sample complexity bounds and a Hessian-independent algorithm achieving them for stochastic zeroth-order optimization of quadratic objectives, with robustness to heavy-tailed noise."
KGRL with KIAN enables flexible integration of multiple external knowledge policies for efficient and exploratory reinforcement learning.
"We propose the G-triple-correlation layer, a complete and robust group-invariant layer for G-CNNs that outperforms max pooling on multiple groups and datasets."
"A single-pass semi-streaming variant of the Pivot algorithm for Correlation Clustering achieves a (3+ε)-approximation using O(n/ε) words of memory, improving on previous space bounds with a simple algorithm and analysis."
"SSL models often memorize image-specific details rather than learning generalizable features, posing privacy risks and evading standard detection methods."
We derive sharp non-asymptotic bounds for the test error of finite-rank kernel ridge regression that are tighter and valid for any regularization.
We propose a two-model unsupervised framework using point- and sequence-based reconstruction to compute an induced anomaly score that outperforms state-of-the-art methods on time series anomaly detection.
"Transformers can implement and select standard machine learning algorithms in-context without parameter updates, achieving near-optimal predictions and adaptive algorithm selection on diverse tasks."
"We introduce the ""Re-identify Any Animal in the Wild"" (ReID-AW) task and a universal ReID model, UniReID, with a wildlife dataset (Wildlife-71) and dynamic visual prompts to enable cross-species re-identification for unseen wildlife categories."
"Stable Diffusion features, when simply post-processed and fused with DINOv2, yield improved zero-shot correspondences and enable effective instance swapping across multiple images."
"We propose Context-Aware PoseFormer, which uses off-the-shelf 2D pose detector features to improve 3D pose estimation without temporal information or finetuning, outperforming state-of-the-art methods even when they use hundreds of frames."
"Selfmem improves retrieval-augmented generation by iteratively using the model’s own outputs as expandable memory, achieving state-of-the-art results on translation and summarization tasks."
"DoReMi uses a small proxy model with Group DRO to derive optimal domain weights for efficient pretraining of a larger language model, improving perplexity and downstream performance without task-specific tuning."
"We introduce a neural implicit sampler trained via KL or Fisher divergence minimization that efficiently generates high-quality samples from unnormalized target distributions, outperforming MCMC by more than 100× in high dimensions."
"We propose Progressive Active Learning (PAL), a sampling scheme that balances selection of informative in-distribution and out-of-distribution instances to improve open-set active learning performance."
"We derive adaptive regret bounds for unbounded OCO with time-varying comparators using sparse coding, measuring complexity via comparator energy and sparsity on a user-chosen dictionary."
Rewarded soup improves multi-objective alignment in foundation models by linearly interpolating independently fine-tuned networks for diverse rewards.
"MSTH is a novel hash-based encoding that efficiently reconstructs dynamic 3D scenes by using a learnable mask to separate static and dynamic space-time regions, reducing hash collisions and memory usage while achieving superior results with minimal training time."
X-Prompt extends LLM prompting with imaginary OOD-robust words learned via context-augmented learning to enable more descriptive and customizable instructions.
"We introduce Wusi, a 3D multi-person sports dataset, and a cognition-inspired framework using behavioral cloning and generative adversarial imitation learning to predict strategic human social interactions in team sports."
"We propose D3FG, a functional-group-based diffusion model that generates realistic protein pocket-specific molecules by representing molecules as functional groups and linkers, yielding improved 3D structures, binding affinities, and drug properties."
We propose an energy-based model framework for adaptive context control in text-to-image diffusion models that improves semantic alignment and enables zero-shot compositional generation.
"We present a memory-constrained algorithm for hypothesis selection that achieves a near-optimal tradeoff between memory usage and sample complexity, with bit-sample product $O(n \log n)$."
"We show that no-regret learning algorithms can efficiently converge to equilibria with near-full efficiency in large games under smoothness or vanishing strategic sensitivity, and achieve improved welfare guarantees using clairvoyant mirror descent."
Parameter-free Differentiable Pruning (PDP) achieves state-of-the-art accuracy and efficiency across diverse vision and language models for multiple pruning types with minimal complexity.
"Unlimiformer enables pretrained transformer models to process unlimited-length inputs by replacing cross-attention with kNN distance queries, achieving strong performance on long-document summarization without input truncation."
Gold-YOLO introduces an advanced Gather-and-Distribute mechanism and MAE-style pretraining to improve multi-scale feature fusion and detection performance in real-time object detection.
"The paper shows that Stable Diffusion's CLIP model can convert images to text prompts instantly using a closed-form linear projection, with performance improved by minimal fine-tuning, enabling flexible image-to-text tasks."
AutoGO is a framework that optimizes deep neural networks at the computation graph level by mutating segments of primitive operations to improve performance and hardware efficiency without introducing new primitives.
"FIT is a new measure that quantifies feature-specific information flow between brain regions using Wiener-Granger causality, revealing content and direction of information transfer beyond traditional methods."
"Team-PSRO and Team-PSRO Mix-and-Match extend PSRO to multi-team zero-sum games, with Team-PSRO guaranteed to converge to TMECor and outperforming self-play in large-scale games like Google Research Football."
"We propose an adversarial learning-based method using supervised classifiers and iterative heuristics to detect and correct feature shifts, outperforming existing statistical and neural network techniques."
"Perturbing only the affine normalization parameters during the adversarial step of Sharpness-aware Minimization (SAM) yields comparable or better generalization performance than perturbing all parameters, suggesting that the benefits of SAM may not be solely due to reduced sharpness."
"DrugCLIP is a contrastive learning framework for virtual screening that outperforms traditional docking and supervised methods by aligning protein and molecule representations without relying on binding-affinity labels, significantly reducing computation time, especially in zero-shot settings."
"UTSP is an unsupervised graph neural network framework for the TSP that uses a surrogate loss to produce edge probability heatmaps and local search to find near-optimal Hamiltonian cycles, achieving strong performance with far fewer parameters and training samples than supervised or reinforcement learning methods."
"We introduce a scalable method for solving the Gromov-Wasserstein problem with squared Euclidean discrepancy in low dimensions by reformulating it as a low-rank concave quadratic optimization problem, enabling efficient global solutions for large datasets."
"We propose a method that learns text-based image editing directions from single visual ""before"" and ""after"" example pairs using diffusion models."
"We propose a conditional generative modeling approach using score-based diffusion models to robustly estimate object poses from partially observed point clouds, achieving state-of-the-art results on REAL275 and generalizing to novel categories without fine-tuning."
We propose a computationally practical reduction from contextual bandits with feedback graphs to regression that achieves minimax rates.
"We propose FGVP, a fine-grained visual prompting method using segmentation-based blur reverse masks that significantly improves zero-shot referring expression comprehension compared to previous visual prompting techniques."
"We propose Relative Entropic Optimal Transport (RE-OT) for long-tailed classification, derive an epoch-varying RE-OT loss, and demonstrate its effectiveness across multiple classification and segmentation tasks."
FOCAL is a self-supervised contrastive learning framework for multimodal time-series that factorizes latent features into shared and modality-private components and enforces temporal locality to improve downstream performance.
"BIRD detects and removes backdoors from pretrained DRL policies without attack knowledge by optimizing trigger restoration and using a novel detection metric, effectively defending against multiple attacks in various environments."
"We rigorously analyze the SGD training dynamics of a 1-layer transformer for next token prediction, showing that self-attention implements a discriminative scanning algorithm that selectively focuses on key tokens based on co-occurrence, stopping at a phase transition controlled by the decoder learning rate."
"We introduce Feature Likelihood Divergence (FLD), a parametric sample-based metric that evaluates generative models by quantifying novelty, fidelity, and diversity, and show it detects overfitting where existing metrics fail."
"We propose a time-reversal symmetry-enforced dynamics model that improves offline reinforcement learning from small datasets by providing better representations and an OOD reliability measure, leading to a data-efficient offline RL algorithm that outperforms existing methods on benchmark tasks."
"We introduce a minimum-risk recalibration framework based on MSE decomposition to balance calibration and sharpness in probabilistic classifiers, showing that uniform-mass binning with $n^{1/3}$ bins achieves $O(n^{-2/3})$ risk and enabling efficient label shift adaptation with fewer target samples."
"We propose pFedBreD, a personalized federated learning framework that injects personalized prior knowledge into the global model using Bregman divergence regularization, improving performance by up to 3.5% on multiple benchmarks."
"We propose CRM, an algorithm that efficiently computes a Nash Equilibrium optimizing a given objective in multiplayer games by reducing the number of bilinear terms via correlation plans, significantly outperforming state-of-the-art methods."
"DDCoT prompting evokes multimodal chain-of-thought reasoning by separating reasoning and recognition tasks and integrating visual recognition into joint reasoning, significantly improving and generalizing reasoning performance in both large and small language models."
"We propose a multi-task learning framework using summary statistics and an adaptive parameter selection method, with theoretical and empirical analysis demonstrating its effectiveness under data-sharing constraints, particularly for genetic risk prediction."
Last-layer retraining with minimal group and class annotations improves group robustness nearly matching deep feature reweighting without requiring held-out group labels.
"SpatialRank improves urban event risk location ranking by modeling spatiotemporal dependencies and optimizing a hybrid NDCG loss, achieving up to 12.7% higher NDCG than state-of-the-art methods on crime and traffic accident datasets."
EASYLLP is a debiasing method for Learning from Label Proportions that estimates individual instance-level loss for arbitrary models and outperforms standard label proportion matching methods under certain conditions.
"We theoretically characterize the entire training process of a two-layer ReLU network on linearly separable data, revealing four distinct phases and capturing specific nonlinear behaviors from initialization to convergence."
"AOC is an accountable offline controller that uses a tailored subset of offline data to achieve high performance and accountability in healthcare control tasks, even with limited data."
We propose PAC-value-based offline RL algorithms with convergence guarantees for soft Q- and Q-functions under partial coverage using minimax loss functions derived from nonlinear convex optimization.
"We show that inverse dynamics modeling is effective for pretraining representations in imitation learning from multitask demonstration data, and provide a new theoretical analysis explaining its advantages."
We extend loss landscape analysis to Bayesian neural networks and find nearly zero marginalized loss barriers between linearly connected approximate solutions via permutation alignment.
We present the first end-to-end differentiable meta-BO framework using transformer-based neural processes with RL and an auxiliary task to improve sample efficiency on both synthetic and real-world optimization tasks.
"TextDiffuser improves text rendering in diffusion models using a keyword layout generator and a large OCR-annotated dataset, enabling high-quality, controllable text image generation and inpainting."
POMP is a memory- and computation-efficient prompt pre-training method for vision-language models that achieves state-of-the-art zero-shot performance across image classification and segmentation tasks.
"DESSERT is a new, efficient approximate algorithm for vector set search that improves retrieval speed in semantic search models with minimal recall loss."
"DaTaSeg is a multi-dataset, multi-task segmentation model that achieves improved performance on semantic, panoptic, and instance segmentation tasks via shared representations, weak supervision, and dataset knowledge sharing."
"We propose FCD, a simple knowledge distillation method using token- and sequence-level relations with a correlation-based loss that outperforms existing approaches on multiple NLP tasks for compressing BERT, RoBERTa, and GPT models."
"We define and solve the multi-fidelity active causal discovery problem using a mutual-information-based acquisition function and a cascading correlation model, extending to batch intervention and introducing an ε-submodular concept to justify a greedy method with theoretical guarantees."
We generalize and mitigate oversmoothing in directed graphs using fractional graph Laplacian neural ODEs that propagate information over long ranges while controlling long-distance jumps.
"HyFluid is a neural method that jointly infers fluid density and a physically plausible, hybrid-represented velocity field from sparse multiview videos, enabling accurate reconstruction of turbulent fluid flows without relying on optical flow."
NuTrea is a tree search-based GNN model with enhanced message passing and global context-aware node embeddings that improves multi-hop KG question answering by better handling ambiguous and uninformative nodes.
"We introduce NoiseCluster, a method that exploits long-trained representation clustering to correct subclass-dominant label noise, outperforming baselines on synthetic and real datasets."
"We present a machine-learned algorithm for Display Ads and generalized assignment that outperforms worst-case baselines using real-world and synthetic data, while remaining robust to poor predictions."
"The article rigorously defines conservation laws in gradient descent for neural networks, provides algorithms to compute them, and demonstrates their completeness for certain ReLU architectures."
"SDS-Complete uses a pre-trained text-to-image diffusion model to complete incomplete point clouds of out-of-distribution objects at test time using only text semantics, without requiring 3D training data."
"The choice of perception loss function in causal, low-latency sequential video compression significantly affects reconstruction quality, especially at low bit rates, but encoders trained for MSE can produce representations adaptable to either loss at the decoder."
"ConDaFormer improves 3D point cloud transformer efficiency and local geometric modeling by splitting cubic windows into 2D planes and adding depth-wise convolutions, capturing both long-range and local context."
"We propose an algorithm that identifies a minimal, causally relevant subgraph (necessary and sufficient causal graph) for a target outcome using probabilities of causation and demonstrate its effectiveness on simulated and real data."
"Graph Neural Networks outperform neural networks on node classification not only due to homophily but also because of both intra- and inter-class node distinguishability, and we introduce a new performance metric based on hypothesis testing that better captures this beyond homophily."
"We propose an identifiability theory and estimation methods for ICA with Gaussian sources using second-order statistics and structural variability assumptions, validated experimentally."
"We provide quantitative bounds on the generalization of GNNs to graphs of different sizes and sparsity using operator limits (graphops), valid for both dense and sparse graphs."
LfVoid uses text-to-image generative models and image editing to train reinforcement learning agents from natural language instructions without in-domain demonstrations.
"We present a hybrid imitation learning method that combines behavioral cloning and entropy-regularized reinforcement learning to refine cloned policies using a shaped reward and a critic hypothesis space, enabling stable and scalable imitation in high-dimensional tasks."
"SAME introduces a structure-aware, Shapley-based method that efficiently computes high-fidelity explanations for GNNs by exploring multi-grained substructures and achieves significant improvements in explanation fidelity across multiple benchmarks."
GROOVE improves the generalization of meta-learned RL optimizers by automatically generating curricula that maximize algorithmic regret during meta-training.
"Object Style Compensation improves semantic image segmentation by adapting object-level style discrepancies using a memory of category-specific style features, enabling more accurate pseudo-labels and state-of-the-art results."
"We propose Point Diffusion (PDF), a method using a point cloud super-resolution diffusion module and surface-based sampling to efficiently represent and render large-scale outdoor scenes with detailed textures."
We propose a teacher model with memory that dynamically adjusts loss functions using both student and loss states to improve student performance across multiple deep learning tasks.
"We present polylog(n)-space streaming algorithms that estimate the cost of correlation clustering with a constant multiplicative error and small additive error, and show that this tradeoff is necessary."
"$\ell$-C2ST enables local, interpretable, and theoretically grounded evaluation of posterior approximators in simulation-based inference without requiring samples from the true posterior."
"Lookaround is an SGD-based optimizer that alternates between training multiple networks on augmented data and averaging their weights during training, resulting in flatter minima and improved generalization compared to existing post-hoc weight averaging methods."
"We propose a Metropolis-based noising scheme for diffusion models on Riemannian manifolds with domain-informed constraints, providing efficient sampling and valid discretization of reflected Brownian motion, and demonstrate its effectiveness across geospatial, robotic, and protein design applications."
"We propose and analyze budgeted-at-k generalized metrics within the ETU framework for optimizing long-tail performance in extreme multi-label classification, with efficient, provably robust algorithms."
"We propose a semi-supervised imitation learning method that learns disentangled behavior representations from imbalanced, multi-modal demonstrations using limited labels by adapting semi-supervised GANs, aligning latent distributions, and applying regularized information maximization."
"We propose coop-CBM with concept orthogonal loss to learn meaningful, separable concept representations without fine-grained labels, improving robustness and accuracy under distributional shifts compared to standard and black-box models."
"We introduce an active forgetting mechanism during RoBERTa pretraining that enables faster and better low-data adaptation to new languages, especially distant ones."
"Plasticity injection is a parameter- and prediction-unbiased intervention that diagnoses and mitigates neural network plasticity loss in deep reinforcement learning, improving Atari performance and computational efficiency."
"MusicGen is a single transformer language model that generates high-quality mono and stereo music from compressed tokens, outperforming baselines on text-to-music tasks with efficient, single-stage conditioning on text or melody."
"The paper establishes that the continuous 1-WL test accurately characterizes the expressive power of MPNNs on graphons, linking their ability to distinguish graphs to topology via tree distance and substructure counts, and empirically shows that random MPNNs perform comparably to trained ones in preserving graph distances."
"We propose Domain Re-Modulation (DoRM), a generator structure with memory and domain association for few-shot generative domain adaptation that enables high-quality, diverse, and consistent multi-domain image synthesis via linearly combinable domain shifts and a similarity-based structure loss."
We propose a method that combines pretrained deep network features with explicit semantic knowledge and a differentiable implicit OOD detection layer to improve vision-language tasks with fewer samples and less training time.
"We propose a method that extracts interpretable, verifiable logical rules from a differentiable MaxSAT solver, achieving perfect accuracy and formal equivalence to ground truth rules on benchmark tasks."
"D-CIPHER is a robust method that efficiently discovers a general class of closed-form differential equations from noisy, infrequent data."
"We compare seven energy-based learning algorithms on deep convolutional Hopfield networks across five vision tasks and find that the centered equilibrium propagation variant with negative perturbations performs best, also achieving new state-of-the-art results and significant speedups."
"We derive PAC-Bayesian generalization bounds for VAEs, including upper bounds on Wasserstein distance between input and generated distributions."
"We propose a diffusion model-based replanning method that decides when to replan using estimated plan likelihoods and bootstraps new plans from previous trajectories, achieving 38% performance gains on Maze2D and enabling robust long-horizon robotic control in stochastic environments."
"We prove that the online single-timescale actor-critic algorithm with linear function approximation converges to an ε-approximate stationary point with sample complexity $\widetilde{\mathcal{O}}(\epsilon^{-2})$ in continuous state spaces, and $\mathcal{O}(\epsilon^{-2})$ under i.i.d. sampling, by controlling error propagation between actor and critic."
"We address view inconsistency in score-distilling text-to-3D generation by debiasing 2D diffusion model scores and prompts, improving 3D consistency with minimal overhead."
"A training-free, low-memory video object segmentation method using pre-trained ViT features and streaming clustering achieves state-of-the-art results on DAVIS-2017 and YouTube-VOS 2018."
"MKOR is a momentum-enabled Kronecker-factor-based optimizer with rank-1 updates that reduces second-order optimization complexity to quadratic, enabling more frequent updates and achieving up to 2.57x speedup over LAMB and 1.85x over KAISA/KFAC on BERT-Large-Uncased."
"We introduce a hierarchy of kernel-based, statistically significant d-order interaction measures for multivariate data, linked to lattice theory and simplicial complexes, and demonstrate their effectiveness on synthetic and neuroimaging data."
"We propose a method to reconstruct an image generator from a pre-trained classifier using only the network's parameters, leveraging the Maximum-Margin Bias of gradient descent."
"DROP decouples offline value estimation and policy extraction, transferring learned value models with conservative regularization to enable safe, adaptive policy optimization during testing."
The paper provides a formal definition of moral responsibility for AI systems using causal and epistemic conditions within causal models and generalizes it to a degree of responsibility.
"We systematically evaluate recent Bayesian deep learning methods on large-scale, distribution-shifted benchmarks, finding that ensembling improves generalization and calibration but that variational approaches outperform others when fine-tuning large transformer-based language models."
RAPHAEL is a text-conditional image diffusion model with stacked mixture-of-experts layers that achieves state-of-the-art image quality and style diversity in text-to-image generation.
SPRING uses LLM reasoning on game context to outperform RL baselines in Crafter without training.
"We propose sorting training images by spuriosity and show that finetuning on less spurious images reduces model bias more effectively than changing data or training, revealing that dataset composition drives spurious feature reliance more than training methods."
"Ensembling significantly improves classification performance when the model disagreement rate is high relative to the average error rate, especially for non-interpolating models like tree-based methods, but not for interpolating models."
"We introduce Rand PR and CycAug, data augmentation methods that improve sample efficiency in visual reinforcement learning by balancing spatial diversity and hardness through periodic operation cycling."
"We introduce generalized equilibrium with asymmetric regret constraints and show that, in repeated play, one agent using a no-swap strategy while the other is unconstrained can yield higher utility than traditional Stackelberg equilibria in many games, and that learning optimal strategies depends critically on the opponent's regret algorithm."
"We propose iterative elastic bins (IEBins), an iterative, uncertainty-aware classification-regression approach for monocular depth estimation that improves accuracy over prior state-of-the-art methods on KITTI, NYU-Depth-v2, and SUN RGB-D."
"We propose two methods, fast PLBF and fast PLBF++, that significantly reduce the construction time of partitioned learned Bloom filters while preserving nearly the same memory efficiency."
"We propose CluB, a LiDAR-based 3D object detection framework that integrates BEV and cluster-based representations via feature and query-level fusion, achieving state-of-the-art results on Waymo and nuScenes."
"Gradient flossing stabilizes RNN training by regularizing Lyapunov exponents, mitigating exploding and vanishing gradients and enabling longer time horizon learning."
"Denoising diffusion models, with self-supervised pre-training and innovations for noisy-incomplete data, achieve state-of-the-art results in optical flow and depth estimation without task-specific architectures, also enabling uncertainty-aware inference."
"We propose an ICC regularizer that, when added to contrastive losses, improves the repeatability and downstream performance of learned embeddings by reducing intra-class variance."
"HAVE introduces a hierarchical adaptive value estimation framework that dynamically weights and re-fuses multiple modalities for improved policy learning in vision-based RL, particularly demonstrating gains in autonomous driving with CARLA using event and depth data."
"We identify and demonstrate that safety failures in large language models stem from competing objectives and mismatched generalization, showing that current safety training is insufficient even after extensive red-teaming."
"Masked pre-training maximizes the model's marginal likelihood under suitable scoring, theoretically justifying its strong generalization and suggesting Bayesian training via self-supervision."
The paper introduces a two-stage framework that integrates structural and textual knowledge via soft triples and text-enhanced rules to improve inductive link prediction in knowledge graph completion.
"DynPoint accelerates novel view synthesis for unconstrained monocular videos by predicting 3D correspondences via depth and scene flow estimation, enabling rapid training and robust performance on long videos without canonical representations."
"We empirically show that in data-constrained language model training, up to 4 epochs of repeated data maintain low loss for fixed compute, but further repetition or compute provides diminishing returns, and propose a scaling law and mitigation strategies for data scarcity."
"We present SeeTRUE, a human-annotated dataset for text-image alignment evaluation, and two improved automatic methods—based on question answering and end-to-end classification—that outperform prior approaches and can localize misalignments and re-rank generation candidates."
"FedSep introduces a two-layer federated learning framework that separates learning and communication with decode/encode operations to address their conflicting requirements, offering improved performance for communication-efficient and heterogeneous federated learning."
SUGARL separately yet jointly learns motor and sensory policies in Active Vision Reinforcement Learning using an intrinsic sensorimotor reward to optimize active vision under partial observability.
"We present an energy-based probabilistic model for Large Hadron Collider events that captures higher-order particle interactions and supports physics simulation, anomaly detection, and particle identification."
"We establish optimal communication requirements for differentially private mean and frequency estimation in federated learning and analytics under central and shuffled models, showing significant bit savings when $n\min(\varepsilon, \varepsilon^2)\ll d$ via client-side randomization and compression."
"We present the first almost-linear time algorithms with optimal sample complexity for robust Gaussian mean estimation and linear regression under Huber contamination, achieving near-optimal error guarantees."
"We derive a scalable GP matching method for arbitrary ANNs and demonstrate that, in practice, only a subset of prior theoretical assumptions are necessary for close output approximation and interpretability across diverse datasets and architectures."
"We introduce a non-adversarial training method for Neural SDEs using signature kernel-based scoring rules that ensures theoretical guarantees, enables efficient backpropagation, and outperforms adversarial training on time series and spatiotemporal data generation tasks."
"OpenVik extracts format-free visual knowledge from images using an open relational region detector and a large multimodal model, improving performance in visual reasoning tasks."
"Discrete Adversarial Distillation (DAD) improves vision model robustness by distilling from a robust teacher using adversarial, discretized examples, offering strong gains with minimal overhead."
"Patch Diffusion is a patch-wise training framework that accelerates diffusion model training by at least 2×, improves data efficiency, and achieves strong generation quality even with as few as 5,000 images."
"Gradient flow on 2-layer diagonal linear networks with vanishing initialization jumps between saddles and converges to the minimum $\ell_1$-norm solution via incremental coordinate activation, as shown analytically and numerically."
"We improve deep networks’ compositional generalization by applying iterated learning to models with simplicial embeddings, as justified by Kolmogorov complexity analysis, and demonstrate this on both synthetic and real-world tasks."
"We propose and evaluate a novel punctuation-level adversarial attack in NLP that effectively fools state-of-the-art models with minimal impact on human perception, and introduce a search method (TPPEP) to efficiently determine optimal attack positions."
Adaptive recurrent neural networks with a learnable halting mechanism enable zero-shot generalization to unseen difficulty levels in visual reasoning tasks by dynamically allocating computational resources based on input difficulty.
"The authors present Generalized Belief Transport (GBT), a unified mathematical framework that generalizes Bayesian inference, cooperative communication, and classification as special cases within Unbalanced Optimal Transport, enabling interpolation, analysis of model boundaries, and study of convergence and drift adaptation in learning."
We analyze the computational tractability of subgame decomposition for depth-limited search in imperfect information games and introduce a scalable MCMC-based method for trick-taking card games like Oh Hell.
"We propose All-Inclusive Multi-Level Segmentation (AIMS), a unified model that segments images into part, entity, and relation levels via multi-dataset multi-task training to address annotation inconsistency and task correlation."
"We extend contrastive learning to semi-supervised regression by leveraging unlabeled data via spectral seriation to recover ordinal relationships, improving feature representation and surpassing existing methods."
"We propose FreTS, a frequency-domain MLP architecture that leverages global view and energy compaction for superior time series forecasting across multiple benchmarks."
"We propose a two-stage framework that constructs discrepancy graphs and trains an evaluator to estimate GNN performance on unseen, unlabeled graphs."
A GNN-based dynamic programming approach for the maximum independent set problem self-trains using graph comparisons and outperforms prior methods on synthetic and real-world datasets.
"EvoPrompting, combining evolutionary prompt engineering with soft prompt-tuning, enables language models to generate diverse and high-performing neural architectures across multiple tasks."
"We propose PGIB, an explainable GNN framework that uses prototype learning within an information bottleneck framework to extract key subgraphs crucial for predictions, improving both performance and interpretability over existing methods."
DELIFFAS achieves state-of-the-art photorealistic and controllable digital human avatar synthesis by representing appearance as a deformable surface light field on a controllable mesh and using perceptual supervision on full images.
"We introduce a setting where agents cooperate in the one-shot Prisoner’s Dilemma using only a single similarity score, enabling the same cooperative outcomes as full transparency with learnable solutions."
"PCF-GAN improves time series generation by incorporating path characteristic functions into the discriminator and combining them with auto-encoders, outperforming state-of-the-art methods on multiple datasets."
"We analyze the efficiency of Monte Carlo partition function estimators under different annealing paths and show that NCE outperforms importance sampling asymptotically, geometric paths reduce error faster than arithmetic ones except in a limit where arithmetic paths are optimal, and propose a two-step estimator to approximate the optimal path."
"Multiplicative mixtures of Matérn kernels in multi-output Gaussian processes yield identifiable covariance matrices up to a constant, unlike additive mixtures in single-output settings where smoothness and identifiability are limited by the least smooth component."
"We provide the first theoretical analysis of adversarial training in random deep neural networks, revealing that networks without shortcuts are generally not adversarially trainable, adversarial training reduces network capacity, wider networks alleviate these issues, and input/output dimensions affect adversarial loss bounds and weight variance dynamics."
"We establish a trichotomy of mistake bounds in transductive online learning—$n$, $\Theta(\log n)$, or $\Theta(1)$—depending on VC and Littlestone dimensions, improving the $\Theta(1)$ lower bound to $\Omega(\log d)$ and extending results to multiclass and agnostic settings."
"We propose Uncertainty-aware Alignment Network (UAN), which uses multimodal mutual information and an uncertainty-aware alignment mechanism to improve unsupervised domain adaptation in video-text retrieval under one-to-many cross-modal uncertainty."
"L2Dive, a learned diving heuristic based on graph neural networks, improves feasible solution quality and reduces solving time for mixed integer linear programs compared to standard heuristics in SCIP."
"We present two parallel algorithms for submodular function minimization achieving new depth–query trade-offs: one with depth 2 and query complexity $n^{O(M)}$, and another with depth $\widetilde{O}(n^{1/3} M^{2/3})$ and poly$(n, M)$ queries, including the first near-optimal depth algorithm for minimizing $\ell_\infty$-Lipschitz functions over the hypercube."
"We introduce Dissimilarity dimension and a general interactive estimation algorithm with polynomial regret and PAC bounds, unifying statistical-query learning and structured bandits under a new learnability measure."
"We propose faster algorithms with strong approximation guarantees for monotone and regularized submodular cover, and a feasible approximate algorithm for the general case, demonstrating their effectiveness in data summarization and graph cut applications."
"Deep Language Networks (DLNs) stack stochastic language layers with optimized prompts, and DLN-2 outperforms DLN-1 on reasoning and understanding tasks, potentially matching larger models with smaller components."
Tuning standard deep learning components can achieve state-of-the-art performance on class-imbalanced datasets without specialized loss functions or samplers.
"SGD biases expressive networks towards simpler subnetworks via stochastic attractivity to invariant sets, improving generalization and explaining the benefit of large learning rates early in training."
"MultiModN is an interpretable, modular, and MNAR-resistant multimodal network that achieves performance comparable to parallel fusion baselines while providing granular, real-time predictions and robustness to missing not at random."
"We propose a unified framework for deriving sharp bounds on causal effects under unobserved confounding in complex settings, generalizing the marginal sensitivity model to various causal effects and treatments, and provide a scalable estimation algorithm."
CAST is a two-stream RGB-based architecture with a cross-attention bottleneck that improves balanced spatio-temporal understanding for action recognition across multiple video benchmarks.
"We propose a fine-tuning method with geodesic multi-modal mixup to improve alignment and uniformity of CLIP embeddings, yielding more transferable and robust representations across diverse tasks."
"Mind-Video reconstructs high-quality, frame-rate arbitrary videos from continuous fMRI data using masked brain modeling, multimodal contrastive learning, and temporal-augmented diffusion, outperforming prior work in both semantic and pixel-level metrics while showing biological plausibility."
"ShiftAddViT reparameterizes Vision Transformer multiplications with bitwise shifts and additions to significantly reduce latency and energy consumption on GPUs with minimal accuracy loss, using a mixture-of-experts framework to balance speed and accuracy for MLPs."
"We propose and analyze a decentralized gradient algorithm that achieves approximate two-phase convergence for low-rank matrix estimation from near-isotropic measurements over a network, with sample and communication complexity depending on network topology and connectivity."
"Seal is a VFM-based framework that distills vision foundation models directly onto point clouds for automotive segmentation, achieving superior performance and generalizability across diverse datasets without requiring 2D or 3D annotations."
We generalize a gradient decomposition framework for fast norm computation in DP-SGD to arbitrary intermediate operations and show improved runtime and storage for linear layers using this framework.
"We propose a non-convex optimization algorithm with certificates based on the Fourier spectrum decay of smooth functions, outperforming Lasserre's hierarchy on high-dimensional polynomials."
"Stacking state-space models with layer-wise nonlinear activation enables approximation of any continuous sequence-to-sequence relationship, but does not fundamentally solve the exponential decay of memory."
Offline reinforcement learning enables an AI agent to learn effective influence strategies from human-human interaction data to improve suboptimal human behavior and adapt to changing human strategies without online experimentation.
"CLeAR, a novel continual learning methodology for abstract logical tasks, achieves near-zero forgetting and backward transfer, outperforming image-based CL methods on formal language tasks across Chomsky hierarchy levels."
"We propose a continuously weighted contrastive loss that improves 0-shot cross-modal transfer by better aligning embedding spaces, achieving 5–8% gains in image classification and 20–30% in speech intent/keyword tasks."
"A self-supervised polar architecture, motivated by Fourier theory and optimized for next-frame prediction, learns interpretable visual representations that rival deep networks in motion prediction and align with neural models of V1."
AMFormer is a query-centric few-shot segmentation model that uses adversarial training with object and detail mining transformers to achieve state-of-the-art results even with weak or incomplete support labels.
"We propose a method to project intermediate Vision Transformer representations onto class embeddings, revealing how image tokens and attention mechanisms build categorical representations and enabling interpretable analysis and identification of important image regions for classification."
We propose a continual learning method using neural processes with hierarchical latent variables and tailored regularizers that improves task transfer and provides reliable uncertainty estimates for novel data detection and confidence assessment.
"We introduce PMI₂, a type II unbiased, monotonic cluster comparison metric with efficient approximations that outperforms Standardized Mutual Information and aids in selecting better clustering algorithms."
AMDP is an adaptive procedure that optimally ranks and selects mediators while asymptotically controlling the false discovery rate in high-dimensional mediation analysis.
UniPC is a unified predictor-corrector framework that significantly improves diffusion probabilistic model sampling quality with very few function evaluations.
"We propose an inverse transform-then-train method with piece-wise linear decision neural networks (PLDNNs) to enable efficient and tightly verifiable deep reinforcement learning systems, achieving up to 438× speedup and significantly reduced overestimation compared to conventional DNN-based approaches."
"WireMask-BBO is a black-box optimization framework for VLSI macro placement that significantly reduces half-perimeter wirelength and runtime, and can improve existing placements by up to 50% HPWL."
"We provide efficient algorithms for approximating $\ell_p$ sensitivities and total sensitivity in regression, showing that real-world datasets often have much lower intrinsic dimensionality than theoretical bounds suggest."
"GTA is a transformer-based method that reconstructs clothed human avatars from single images using global image features and a 3D-decoupling decoder with a hybrid prior fusion strategy, outperforming state-of-the-art methods on CAPE and THuman2.0 datasets, especially for complex poses and loose clothing."
"We empirically find that magnitude-based pruning outperforms other criteria in low-density dynamic sparse training, with most methods yielding similar results otherwise."
"SIPO, a novel diversity-driven RL algorithm using state-space distance-based intrinsic rewards and iterative learning, discovers more diverse and interpretable policies than existing baselines across multiple domains."
"HiE is a post-hoc method that leverages label hierarchy to reduce mistake severity and improve fine-grained classification accuracy using coarse-grained predictions, achieving state-of-the-art results on iNaturalist-19 and tieredImageNet-H."
"We propose MMM, a pre-training framework with multi-dimensional position encoding, multi-level channel hierarchy, and multi-stage strategy on a unified EEG channel topology, improving emotional recognition on benchmark datasets."
We propose a grid selection scheme and adaptive stopping criterion for solving ℓ₂-regularized M-estimation that guarantees an accurate solution path with minimal computation.
"Taylor TD, a model-based reinforcement learning framework using first-order Taylor expansion of TD updates, reduces variance and maintains learning guarantees, and when combined with TD3 (TaTD3) achieves competitive performance on benchmark tasks."
"Wasserstein distributionally robust estimators provide generalization guarantees that hold for general model classes, are dimensionality-free, and extend to distribution shifts and regularized problems."
"We propose SE(3)-equivariant convolutions and transformers in ray space to learn geometric priors from multiple views without relying on transformation augmentations, enabling robust 3D reconstruction and neural rendering under roto-translation."
InsActor is a diffusion-based framework that generates physically simulated character animations from high-level human instructions by planning in latent skill space.
We propose local substructure encoding and frame transition encoding modules for expressive 3D equivariant GNNs and demonstrate their effectiveness in LEFTNet for molecular property prediction.
We propose initializing classification layers with GPT-3 and CLIP text embeddings to significantly increase image classification model resilience to hardware errors with minimal accuracy loss.
"We propose a factorizable pseudolikelihood method for efficiently enforcing symbolic constraints in auto-regressive neuro-symbolic models, improving logical consistency and toxicity reduction."
We propose a hierarchical training paradigm integrating geometric GNNs and large-scale language models to set new state-of-the-art performance in antibody sequence-structure co-design and fix-backbone design.
"Rewritten abstract in one short sentence:

We show that non-robust features, while useful in supervised learning, are not transferable to other self-supervised paradigms and are less useful than robust features, suggesting they are paradigm-specific shortcuts and that robust features alone do not guarantee model robustness."
We propose a scene-graph hallucination mechanism and an SG-based hallucination diffusion system that significantly improves abstract-to-intricate text-to-image generation on the COCO dataset.
"We propose Rubik's cube convolution, a zero-FLOP, zero-parameter, high-order channel-wise operator that enhances image restoration performance across multiple tasks by efficiently modeling higher-order channel interactions via spatial-shifting and element-wise operations."
"We present a kernel regression algorithm that recovers the degree-$q$ spherical harmonic expansion of a function on $\mathbb{S}^{d-1}$ from a near-optimal number of uniform samples, leveraging connections to Gegenbauer polynomials and achieving efficiency in any dimension."
Online fine-tuning during search enables efficient game solving with drastically reduced computation compared to AlphaZero-based baselines.
"We disentangle visual attributes in CLIP representations using part-of-speech associations to obtain subspace-based, disentangled visual encodings that improve controllability and invariance in text-to-image synthesis."
"We propose a self-supervised method that scales optical flow in videos to magnify subtle motions using a simple loss and test-time adaptation, without requiring synthetic datasets."
"We introduce Feature Multiplexing, a unified embedding framework that efficiently shares a single representation space across multiple categorical features, achieving better space-accuracy tradeoffs and significant real-world performance gains in large-scale systems."
AIA is a graph data augmentation method that generates new environmental features while preserving stable features to improve covariate shift robustness in graph classification.
We propose a GAN-based method to reconstruct face images from templates and demonstrate its high attack success rates and transferability against both whitebox and blackbox face recognition systems.
"We propose block variants of good and bad Broyden's methods for solving nonlinear equations that achieve explicit local superlinear convergence and, in the good variant, faster condition-number-free convergence via multiple Jacobian rank updates, while the bad variant reduces computational cost by directly estimating the inverse Jacobian, with theory and experiments explaining their relative performance."
"We introduce geometric flow matching, an equivariant diffusion model with stabilized probability dynamics that improves 3D molecule generation speed and performance."
"We present a non-interactive, differentially private sketching method that enables approximate joins on sensitive attributes across datasets without direct interaction, allowing data discovery while preserving privacy."
"We present a convergent, rational, and symmetric smoothed best-response learning algorithm for two-player zero-sum stochastic games with a finite-sample last-iterate convergence analysis using a coupled Lyapunov drift method."
"We propose USS loss, a sample-to-sample based loss with an explicit unified threshold for face recognition that outperforms state-of-the-art methods on multiple benchmarks."
"We prove that a linear-in-observations controller remains optimal in discrete-time finite-horizon LQG with unknown, Wasserstein-ambiguated noise distributions and propose a Frank-Wolfe-based method to compute it."
"GAPS is a gradient-based online adaptive policy selection algorithm that achieves optimal policy regret under convexity and provides the first local regret bound when convexity does not hold, enabling rapid adaptation in changing environments."
KDindex is a novel distillation-based learning framework for lightweight compressed search indexes that improves retrieval accuracy in high-dimensional ANNS and MIPS by preserving top-k ranking order and integrating reconstruction and balance losses.
We demonstrate that regularization-based algorithms achieve both optimal reproducibility and near-optimal convergence in convex and convex-concave optimization under various error-prone oracles.
"Score-based diffusion models achieve a generalization error of O(n^{-2/5}+m^{-4/5}) that is independent of data dimension when early-stopped, but data-dependent mode shifts degrade generalization."
"We introduce a modulated transformation module that enables style-based GANs to perform instance-wise geometric deformations via variable-location convolutions, improving performance across image, 3D, and video generation tasks, including a notable FID reduction on the TaiChi dataset."
CEED is a contrastive learning framework for high-density extracellular recordings that outperforms specialized methods for spike sorting and cell-type classification.
"VideoComposer enables flexible, compositional video synthesis using spatial and temporal controls, including explicit motion vector guidance and a Spatio-Temporal Condition encoder for improved inter-frame consistency."
"Autoregressive transformer models progressively straighten the neural trajectories of word sequences through their layers, which correlates with improved next-word prediction performance."
"We propose a method to estimate causal effects using front-door-like adjustment without knowing the causal graph, relying instead on testable conditional independence statements under limited structural side information."
The paper proposes using text embeddings and layout-guided cross-attention to efficiently compose multiple customized subjects in image synthesis with superior multi-subject control.
"Scattering Vision Transformer (SVT) introduces a spectrally scattering network and spectral gating with Einstein multiplication to capture fine image details without information loss from non-invertible down-sampling, achieving state-of-the-art ImageNet accuracy with fewer parameters and FLOPs than existing vision transformers."
"MQ-Det enhances open-vocabulary object detection by integrating visual exemplars with text as multi-modal category queries, improving state-of-the-art performance without extensive fine-tuning."
We present a differentially private second-order optimization method for convex problems that achieves quadratic convergence and significantly faster convergence than DP-GD/DP-SGD on logistic regression.
"We propose a new analysis using moment generating functions of supermartingales to achieve optimal high-probability convergence rates for clipped gradient methods under heavy-tailed noise, with improved T-dependency and without requiring prior problem constants for step size and clipping parameter choices."
"The paper introduces resilient constrained learning, a method that adaptively balances machine learning performance and constraint relaxation during training using a user-defined cost, and demonstrates its effectiveness in image classification and federated learning tasks."
"Voicebox is a large-scale, non-autoregressive flow-matching speech generative model that outperforms state-of-the-art zero-shot TTS models in intelligibility and speed while supporting diverse speech manipulation tasks via text and audio conditioning."
"We propose Query-based Temporal Fusion Network (QTNet), which enhances object queries using motion-guided temporal modeling to improve 3D detection performance on the nuScenes dataset with negligible computational overhead."
"We propose the Reasoning Game, a cognition-focused environment where deep-learning agents develop a stable, compositional communication system for high-level rule reasoning, enabling generalization and transfer across tasks."
"We show that the Greedy algorithm produces composable coresets for determinant maximization with an $O(k)^{3k}$ approximation guarantee, supported by a local optimality bound showing any single swap can increase the volume by at most a factor of $(1+\sqrt{k})$."
"We recast machine learning from explanations as a robustness problem using human explanations to define a manifold, improving performance over prior methods and achieving state-of-the-art results on benchmarks."
"We present a method that learns 3D geometry, appearance, and physical velocity from multi-view videos to enable future frame extrapolation and unsupervised 3D semantic scene decomposition."
FFNet is a flow-based feature fusion framework for vehicle-infrastructure cooperative 3D object detection that mitigates asynchrony and reduces transmission cost using predicted feature flows from raw infrastructure sequences.
"We propose a denoising diffusion model that learns to sample unobserved signals from partial observations via a differentiable forward model integrated into the generative process, enabling tasks such as sampling 3D scenes from single 2D images."
"We propose a scalable method using the relational graph structure in feature space to detect label errors and outliers in large-scale datasets, achieving state-of-the-art performance across image, speech, and language tasks."
"We analyze how training dynamics and spectral properties of weight and kernel matrices in wide feed-forward neural networks relate to feature learning, showing that small learning rates yield invariant bulk spectra while large rates or adaptive gradients induce heavy-tailed or spiked spectra correlated with better generalization."
"ConSpec is an offline contrastive learning-based reinforcement learning algorithm that identifies critical task steps via interpretable prototypes, improving credit assignment and out-of-distribution generalization."
"Coop improves tensor rematerialization by evicting contiguous tensors within a sliding window and optimizing allocation and in-place recomputation, achieving up to 2× memory savings and significantly reduced overhead and fragmentation compared to baselines."
"We analyze the expressiveness of fixed-size neural networks with transcendental activations, showing that they cannot generally approximate arbitrary functions on compact sets, while certain families including two-layer networks can approximate on finite sets but not everywhere."
"We automatically generate a hierarchical skill structure for autonomous agents by maximizing modularity in the interaction graph, yielding time-scale-dependent skills that improve learning performance across environments."
"SimMTM is a simple self-supervised masked time-series pre-training framework that reconstructs masked time points via weighted aggregation of neighboring series, achieving state-of-the-art results on time-series forecasting and classification tasks."
"We propose Deep Power Laws (DPL), a neural ensemble conditioned to predict power-law learning curves and using gray-box evaluations for incremental configuration selection, achieving state-of-the-art any-time performance on tabular, image, and NLP hyperparameter optimization benchmarks."
"HSIVI generalizes semi-implicit variational inference to multi-layer architectures and accelerates diffusion model sampling by progressively matching auxiliary distributions, improving expressiveness and sample quality with fewer function evaluations."
"We propose a photometrically guided, density-only rendering method that estimates the color field from the density field to better decouple shape and radiance in NeRF, yielding improved density field estimation."
"We propose a data-driven strategy to dynamically select cutting plane separators in MILP solvers, accelerating SCIP by up to 72% on synthetic and 37% on real-world benchmarks."
"Biased soft labels can remain effective for knowledge distillation under specific conditions, enabling students to achieve high accuracy even when teachers perform poorly, and this effectiveness can be measured and generalized to several weakly-supervised learning settings."
"We propose Geometric Harmonization, a simple self-supervised method that promotes category-level uniformity in representation learning to mitigate class imbalance and prevent tail class collapse under long-tailed distributions."
"We derive closed-form expressions for shallow ReLU NN denoisers under various geometric data assumptions and show their solutions align with training sample structures, outperforming empirical MMSE at low noise."
xTrimoGene is a scalable asymmetric transformer for single-cell RNA-seq that leverages data sparsity to dramatically reduce compute while achieving state-of-the-art performance on cell type annotation and related tasks.
We present a polynomial-sample bi-criteria approximation algorithm for finding SafeZones in MDPs with nearly 2× approximation in both escape probability and size.
We propose and analyze a joint caching and model selection framework that can reduce inference cost by up to 50× in simulation and by 1.8–4.3× in real-world experiments.
"We propose a method that leverages latent structure in units and interventions to consistently estimate all unit-specific potential outcomes with sample complexity poly(r) × (N + s²p), improving upon previous approaches that scale with min(N × s²p, r × (N + 2^p))."
"Latent Slot Diffusion (LSD) integrates diffusion models into object-centric learning as an unsupervised, slot-conditioned generator that outperforms transformer-based methods on complex scenes."
"sVORF is an unsupervised framework that decomposes 3D scenes into object radiance fields from a single image using slot-guided volumetric representation, achieving strong results on synthetic and real-world datasets."
"We propose a generative model that combines implicit and explicit distributions to enable fast, high-quality sampling with fewer steps than diffusion models."
"We propose and analyze private and robust algorithms for multi-armed bandits with heavy-tailed rewards, achieving near-optimal regret and optimal privacy-robustness-accuracy trade-offs via reward truncation and the Laplace mechanism."
"We establish a Large Deviation Principle connection between empirical arm draw proportions and rewards to derive improved error bounds for adaptive Multi-Armed Bandit algorithms, notably introducing and analyzing the CR algorithm which outperforms SR."
Diffsurv is a novel differentiable sorting-based method that handles censoring in survival analysis and outperforms existing baselines in both simulated and real-world risk prediction tasks.
"We provide the first global convergence analysis of vanilla local SGD for two-layer neural networks with Gaussian inputs, without overparameterization or added noise, by establishing a self-correction mechanism and a new recursive characterization of parameter updates that enable polynomial-time entry into a good region and linear speedup in convergence."
"Jittering improves worst-case robustness for denoising but is suboptimal for other inverse problems, and training on noisy real data has a similar robustness-enhancing effect."
"Diffusion-TTA adapts pre-trained discriminative models to each test example using a diffusion model’s generative feedback, improving accuracy across image classification, segmentation, and depth prediction tasks beyond existing test-time adaptation methods."
"We propose adaptive step-size methods (SLS and SPS) for bi-level optimization that simplify implementation, extend convergence guarantees, and outperform vanilla SGD and Adam-based BO methods with minimal tuning."
"Progressive Guidance mitigates the lack of diversity and adversarial effects in diffusion models by allowing shared class gradients in early sampling and progressively refining details later, improving both diversity and robustness."
"We derive optimistic statistical rates for the excess risk in multi-task representation learning that interpolate between standard and fast rates depending on task difficulty, and provide new results on excess risk for source tasks and local Rademacher complexity for multi-task learning."
"We propose a partial matrix completion framework that targets high-confidence entry reconstruction with guaranteed coverage, handling arbitrary sampling distributions and providing an efficient online algorithm."
"We propose Content-based Unrestricted Adversarial Attack (ACA), a framework that generates highly transferable, photorealistic adversarial examples by optimizing images on a natural image manifold, outperforming state-of-the-art attacks by 13.3–50.4% and 16.8–48.0% against standard and defended models."
We propose a Bayesian framework that adaptively infers and regularizes VAE network structures to prevent overfitting and improve generative performance across varying depths and architectures.
"We repurpose frozen pre-trained language or vision models for time series analysis tasks by fine-tuning only the output layers, achieving state-of-the-art results and showing that self-attention functions similarly to PCA."
Triangulation Residual loss enables data-efficient self-supervised multiview 3D pose estimation by enforcing global geometric consistency via minimization of the smallest singular value of the triangulation matrix.
"INVERT is a scalable, statistically grounded method that connects DNN representations to human-understandable concepts without requiring segmentation masks or high computational resources."
"Fed-GraB addresses federated long-tailed learning by dynamically re-weighting client gradients based on global distribution feedback, improving minority class performance under privacy constraints."
"Pengi is an audio language model that unifies audio and text inputs as prompts for a frozen language model, achieving state-of-the-art results on multiple audio tasks without task-specific fine-tuning."
"We reformulate NeRF's volume rendering as the exact integral under piecewise linear density, resolving quadrature instability and improving texture sharpness, geometric reconstruction, and depth supervision."
"Adaptive methods like Adam accelerate neural network optimization by biasing iterates towards regions of lower local condition number, contrary to the traditional global geometry explanation."
"We prove identifiability of nonlinear ICA under undercompleteness, partial sparsity, source dependence, and flexible grouping structures, and validate these results empirically."
"Influence functions are theoretically limited in predicting leave-one-out retraining effects due to parameter divergence in deep neural networks, but remain useful for debugging and correcting mispredictions via fine-tuning on influential examples."
"We propose SOL, an efficient method that computes tight linear bounds for general activation functions using optimal criteria for convex functions and adaptive sampling for Lipschitz-continuous functions, significantly improving robustness certification speed with minimal loss in certification rate."
"We propose Inverse Preference Learning (IPL), a parameter-efficient algorithm that learns from offline preference data without a learned reward function by leveraging the interchangeability of Q-functions and fixed policy, achieving competitive performance with fewer parameters and hyperparameters than transformer-based preference RL methods."
"ARCO is a semi-supervised contrastive learning framework with stratified group variance reduction that improves medical image segmentation, especially with limited labels, and outperforms state-of-the-art methods on multiple benchmarks."
"We propose a model-agnostic, alteration-free origin attribution method that distinguishes images generated by a specific model from others using reverse-engineering-based reconstruction loss analysis."
"We propose a sample prioritization method based on the ""learn-ability"" of samples—defined as the rate of training loss reduction—and show it outperforms random and loss-based prioritization in multiple domains."
BLIP-Diffusion is a multimodal subject-driven text-to-image generation model enabling zero-shot and efficiently fine-tuned subject rendition with up to 20x speedup over previous methods.
We propose DAG-tailored transformer adaptations that improve performance and efficiency over prior graph neural networks and transformers for directed acyclic graphs.
"We model competitive collaborative learning as a game and show that rational players manipulate updates to prevent learning, but propose mechanisms that restore honest communication and learning guarantees."
"LART is a 3D Transformer framework for motion transfer that learns correspondences without annotations and enforces motion metric preservation via latent space constraints, enabling high-fidelity motion generation on unseen 3D objects."
"We propose MIMEx, a general framework for intrinsic reward design via masked input modeling with tunable mask distributions, which outperforms prior baselines on sparse-reward visuomotor tasks."
"We propose Supported Value Regularization (SVR), a value regularization method that penalizes Q-values for out-of-distribution actions while ensuring standard Bellman updates for in-distribution actions, guaranteeing contraction mapping in tabular MDPs and improved policy convergence with empirical state-of-the-art performance."
"We propose a point-based Dynamic NeRF with Linear Blend Skinning that reconstructs high-fidelity, reposable 3D articulated objects from sparse multi-view video without object-specific skeletal templates."
"We show that assuming a block-structured Neural Tangent Kernel aligned with class labels leads to the emergence of Neural Collapse in deep neural networks, and validate this with extensive numerical experiments."
"We propose evaluating text-to-image diffusion models as zero-shot classifiers by using their denoising ability, showing they rival CLIP on classification and outperform on shape/texture bias and attribute binding tasks, supporting generative pre-training as a viable alternative for vision and vision-language models."
We propose a user-aware method for human-assisting dexterous grasping using a hand-object-conditional grasping primitive and a history-conditional residual policy to adaptively assist users in grasping objects.
"We analyze and mitigate sensitivity and ill-conditioning in 3D translation averaging under uncertainty, improving 3D reconstruction accuracy, point triangulation, and bundle adjustment convergence."
Demo2Code is a novel framework that generates robot task code from demonstrations by recursively summarizing demonstrations into concise specifications and then synthesizing code from those specifications.
"UE4-NeRF enables real-time, high-resolution rendering of large-scale scenes by partitioning scenes into sub-NeRFs with optimized meshes and integrating with Unreal Engine 4's rasterization pipeline."
We systematically analyze how the structure and statistics of training data govern the emergence and reliability of compositional generalization in conditional diffusion models.
"We introduce the Meek separator and use it to design randomized algorithms that achieve logarithmic approximation for subset search and causal matching with minimal interventions, providing the first average-case guarantees for these targeted causal discovery problems."
"We present the first agnostic learning algorithm for Single-Index Models with arbitrary monotone Lipschitz activations under only second moment assumptions, using calibrated multiaccuracy and Bregman divergences."
We propose a robustness-aware coreset selection method that efficiently enables adversarial contrastive learning on large datasets like ImageNet-1K without label information.
"We propose Heavy-OFUL for heavy-tailed linear bandits and Heavy-LSVI-UCB for heavy-tailed linear RL with function approximation, achieving minimax-optimal instance-dependent regret bounds using a novel robust self-normalized concentration inequality."
We propose a data-driven sparse gating mechanism that automatically selects and integrates multiple geometric spaces per input to optimize representation learning performance without manual intervention.
We introduce permutation-equivariant neural functional layers for processing neural network weights and demonstrate their effectiveness across multiple tasks.
"We introduce DiffusionITM to adapt diffusion models for image-text matching and GDBench for fine-grained evaluation, showing Stable Diffusion outperforms CLIP on compositional tasks and exhibits less bias in version 2.1."
"We propose a probabilistic VAE-based prototype learning method that improves few-shot 3D point cloud object detection by learning distinct, geometric-preserving latent prototypes for instance and class-level features."
We propose a provably guaranteed algorithm that uses an offline dataset to design a single non-reactive exploration policy and analyze its performance in terms of dataset coverage and additional data collected.
"BootGen optimizes biological sequences by training a score-conditioned generator with rank-based and bootstrapped proxy labels, outperforming baselines on sequence design tasks."
"We generalize complex-valued features and propose a new evaluation for distributed object representations, enabling scalable, continuous solutions to the binding problem beyond toy data."
Neural and behavioral data show that primate-like future prediction in dynamic environments is best explained by models trained to predict future states in the latent space of video foundation models optimized for diverse egocentric tasks.
We adapt randomized smoothing to provide certified robustness against edit distance-bounded adversaries for discrete sequence classifiers using a random deletion mechanism that achieves 91% certified accuracy for MalConv at an edit distance of 128 bytes.
"We propose a limited-information method that constructs and compares multiple directed cyclic graphs (DCGs) efficiently using scalable parallel computation and bootstrap inference, with theoretical guarantees and visualization via correspondence analysis."
"CARP is a self-supervised visual representation learning method that uses a random partition-based consistency regularization to learn stable prototypes online via gradient descent, outperforming existing methods on multiple transfer learning tasks."
"LEPARD is a learning-based framework that reconstructs 3D animal shapes from single in-the-wild images by discovering semantically meaningful 3D parts represented as parameterized primitive surfaces, using kinematics-inspired optimization and off-the-shelf deep features without annotation."
"FastSA is a simulated annealing-based recomputation algorithm that achieves substantial GPU memory savings with modest computational overhead, scaling efficiently to large neural network graphs."
"We propose the Multi Time Scale State Space (MTS3) model, a probabilistic framework that efficiently learns multi-time scale world models for accurate long-horizon predictions and uncertainty estimation in complex dynamical systems."
"Generalized representers are the unique class of sample-based model explanations satisfying a set of axiomatic properties, defined by a global and a local sample importance, and encompass many existing methods while being applicable to modern nonlinear models."
"Masked graph modeling for molecular graphs benefits significantly from subgraph-level tokenization and expressive decoders with remask decoding, as demonstrated by the proposed SimSGT method that outperforms existing self-supervised learning approaches."
"We propose DA-Pro, a domain adaptive object detection framework that uses domain-adaptive prompts to dynamically generate detection heads for each domain, improving generalization via shared and specific domain knowledge and prompt ensembling."
"Untuned SGD with constant stepsize achieves a suboptimal rate with exponential dependence on smoothness, while adaptive methods like NSGD, AMSGrad, and AdaGrad remove this dependence without requiring knowledge of problem parameters."
"We show that in linear Fisher markets with adversarially sequenced proportional response updates under liveness constraints, the process converges to the unique competitive equilibrium in the generic case, also revealing properties of no swap regret and best response dynamics."
"We propose a curriculum learning strategy for Graph Neural Networks that incrementally incorporates edges from easy to hard based on model prediction difficulty, improving representation generalization and robustness."
"Three-layer neural networks can learn hierarchical nonlinear features more efficiently than two-layer networks, leading to provably better sample complexity in certain settings."
"We propose a statistically consistent, asymmetric softmax-based surrogate loss for learning to defer that yields calibrated and bounded deferral probabilities."
"We characterize finite width fluctuations in wide but finite neural networks, showing that in the rich feature learning regime, kernel and prediction variances are dynamically coupled and can be reduced, while in deeper networks kernel variance accumulates but the signal-to-noise ratio improves, with empirical evidence of finite width effects in CNNs on CIFAR-10."
"RPO integrates the generalized reduced gradient method into reinforcement learning to efficiently handle both equality and inequality hard constraints, outperforming prior approaches on new, complex benchmark tasks."
"We propose a diffusion-based amortized long-run MCMC sampler for latent space energy-based models, improving sampling quality and model performance on image benchmarks."
"We present an end-to-end neural algorithm for sample-based entropic optimal transport via the Schrödinger Bridge saddle point formulation, enabling fast inference and small entropy regularization."
"We generalize Stackelberg Games to Calibrated Stackelberg Games, where agents best respond to calibrated forecasts of the principal's actions, and show the principal can achieve the optimal Stackelberg utility, with applications to Stackelberg Security Games and strategic classification."
SALT models combine the parameter efficiency of SLDSs with the interpretability and inference benefits of ARHMMs by using low-rank tensor factorization to model time-varying dynamics in time-series data.
MTDiff is a diffusion-based multi-task offline planning and data synthesis method that outperforms state-of-the-art approaches on Meta-World and Maze2D using Transformer backbones and prompt learning.
We propose a biologically plausible temporal predictive coding model for sequential memory that functions as a stabilized Asymmetric Hopfield Network and aligns with neuroscience observations.
"We propose Bi-Sampling Parameter Attribution (BSPA), which combines uniform and inverse sampling with parameter attribution to address data imbalance and improve single image super-resolution."
We provide convergence guarantees for black-box variational inference with dense Gaussian families by addressing the quadratic noise of reparameterization gradient estimators in proximal/projected SGD for non-smooth objectives.
"We adapt reinforcement learning with heuristic rewards to unsupervised object discovery in LiDAR data, achieving more accurate and faster training than previous label-free methods."
"SA-Solver is an improved stochastic Adams method for efficiently sampling from diffusion SDEs, achieving state-of-the-art FID on benchmark datasets with a suitable number of function evaluations."
We propose a norm-aware non-Euclidean interpolation and centroid method in diffusion model latent seed space that improves few-shot and long-tail image generation.
"We propose a PAC-based framework for in-context learnability and show that, under mixture-of-tasks pretraining, downstream tasks can be efficiently learned via in-context learning without weight updates, highlighting that in-context learning primarily identifies the task rather than learning it."
"CoAlign is a multi-user framework that operationalizes concepts for NLP model alignment by learning local models for each concept and a global model for integration, effectively avoiding interference and enabling collaborative adjustment of model behavior."
"We propose an auditing method for differentially private machine learning that uses a single training run to estimate privacy with minimal assumptions, efficiently applying in both black-box and white-box settings and yielding meaningful empirical privacy bounds for DP-SGD without the need for multiple trained models."
"RESPO is a reinforcement learning framework that optimizes for both safety and reward in stochastic environments, guaranteeing persistent safety where possible and otherwise driving the agent toward feasible safety regions."
"HubRouter improves global routing in VLSI by introducing hubs and a two-phase deep learning approach that ensures connectivity and outperforms state-of-the-art methods in wirelength, overflow, and runtime."
"CROMA is a self-supervised learning framework that combines contrastive and reconstruction objectives with spatial attention biasing to learn effective unimodal and multimodal representations for remote sensing, outperforming state-of-the-art methods on classification and segmentation tasks."
"TACO, a temporal action-driven contrastive learning method, improves sample efficiency in visual continuous control by jointly learning state and action representations that contain sufficient control-relevant information."
"We propose a contextual bandit-based method that uses real-time user binary feedback during instruction following to continuously improve an instruction-following agent, achieving a 15.4% absolute increase in instruction execution accuracy and showing robustness and equivalence of feedback to supervised learning signals."
"We present a data-driven algorithm that learns under unknown distribution drift without prior drift knowledge, achieving error close to an oracle with known drift, and outperforming methods relying on loose drift bounds, demonstrated for binary classification and linear regression."
"Model Spider efficiently selects the most suitable pre-trained model for a given task by representing both models and tasks as vectors and learning to measure their fitness, with improved selection via task representation enriched by top candidate models."
We propose a dictionary learning-based attention module that decomposes and reconstructs input features to capture global context and improve performance in deep vision models.
"We propose replacing the standard argmax prediction with the Fréchet mean to improve zero-shot and few-shot class prediction using label space metrics, achieving up to 29.7% relative improvement on ImageNet and 10.5% on pretrained zero-shot models."
VPP is a progressive 3D generation method using voxel and point representations that efficiently produces high-quality 8K point clouds in 0.2 seconds and supports multiple downstream 3D tasks.
We propose a nonparametric constraint-based algorithm that identifies the entire causal structure from subsampled time series by using future observed proxies of hidden variables to remove subsampling bias.
"Doubly-robust self-training adaptively combines labeled and pseudo-labeled data in semi-supervised learning, outperforming standard self-training on ImageNet and nuScenes datasets."
"Specialization in multi-dimensional producer content choices under personalized recommender systems can arise under certain conditions and may enable producers to achieve positive profits, potentially reducing marketplace competitiveness."
"Feature Augmented Training (FeAT) improves out-of-distribution generalization by iteratively augmenting and retaining diverse features during training, addressing the bottleneck of ERM's preference for spurious features."
"We generalize an interactive joint differential privacy definition for online processes and show that, unlike traditional differential privacy, any learning rule can be made private with only polynomial overhead in the mistake bound for online classification."
$\pi$-GNN is a pre-trained interpretable GNN that distills universal structural patterns from synthetic graphs to improve both interpretability and performance across diverse downstream graph tasks.
"We propose shape-optimized vision transformers that, using compute-optimal scaling of width and depth, achieve strong performance with significantly fewer parameters and lower inference cost than larger models."
"We demonstrate label-free zero-shot visual classification improvement by using an LLM to generate category descriptions and an unlabeled image set, achieving up to 11.7% absolute gains over base models without any labels or paired data."
Adjusting synaptic weights to minimize the roughness of the energy landscape along embedded attractor manifolds can stabilize continuous working memory representations in RNNs without reducing capacity.
Resilient Multiple Choice Learning (rMCL) extends MCL for regression with multiple targets using a Voronoi-based scoring scheme that preserves prediction diversity and is validated on synthetic and sound source localization tasks.
"A recurrent neural circuit can learn tree-structured attractor states to disentangle and robustly represent the ordinal structure of temporal sequences, facilitating flexible processing and transfer learning."
DiffVL enables non-experts to specify soft-body manipulation tasks via vision and language for efficient gradient-based trajectory optimization with differentiable physics simulation.
"We propose a memory mechanism that stores past events as human-readable language using CLIP and a pretrained language model, enabling faster learning and improved interpretability in partially observable reinforcement learning."
GraphGP is a graph-structured Gaussian process framework that adaptively transfers knowledge across graphs with homophily or heterophily assumptions and shows improved performance on transfer learning benchmarks.
"UPIDet enhances cross-modal 3D object detection by leveraging image-based local coordinate estimation and point-to-pixel gradient backpropagation to improve point cloud representation, achieving top performance on KITTI's cyclist class."
"Penguin is a novel ciphertext packing technique that accelerates homomorphic encryption-based Graph Convolutional Network inference by exploiting GCN's unique computation pattern, achieving up to 10× speedup and 79% memory reduction while preserving data privacy."
We propose a sharpness-aware Bayesian posterior for neural networks that promotes flatter models and improves generalization over standard Bayesian methods.
"The Twisted Diffusion Sampler (TDS) is an SMC-based method that enables exact conditional sampling in diffusion models without task-specific training, improving accuracy in image generation and outperforming state-of-the-art models in protein motif-scaffolding."
"We propose a pipeline that predicts response lengths to group and batch similar queries, achieving an 86% throughput improvement in LLM inference without loss of accuracy."
We propose a Monte Carlo algorithm inspired by computer graphics path tracing that models human inference of dynamic events from static scenes with high correlation to human intuition.
"XAGen is the first 3D generative model for human avatars that enables expressive control over body, face, and hands via a multi-scale, multi-part 3D representation and rendering, outperforming prior methods in realism, diversity, and expressive control."
"Gradient descent on the lifted tensor optimization problem for matrix sensing, with small initialization, induces implicit regularization and promotes global optimality."
"ASPEN removes operator synchronization barriers in DNNs by dynamically scheduling fine-grained dataflow tiles, enabling opportunistic parallelism and achieving up to 4.3× faster inference than TorchScript and TVM on CPU."
"We propose a dimension-independent fast underdamped Langevin algorithm with $\tilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{\epsilon^{2/3}}\right)$ iteration complexity for sampling from composite log-concave distributions, achieving a $d^{1/3}$ faster convergence rate for posterior sampling in high-dimensional linear models than previous methods."
"We rigorously unify Bayesian, variational Bayesian, and ensemble methods for uncertainty quantification in deep learning by reformulating non-convex optimization as convex optimization in the space of probability measures via Wasserstein gradient flows, explaining deep ensemble success and enabling provably convergent ensembling schemes."
We introduce a zeroth-order local training technique for spiking neural networks that improves accuracy and training speed compared to surrogate gradient methods.
"BLEEP is a contrastive learning-based bi-modal embedding framework that predicts spatially resolved gene expression from H&E histology images with higher accuracy than existing methods, enabling cost-effective molecular analysis of tissue architecture."
"Plurality is the only positional scoring rule that is strategyproof for voters with correlated beliefs under standard social choice models, unlike other scoring rules and many non-scoring rules."
We analyze the generalization performance and separability transition of empirical risk minimization with convex loss and regularization for estimating centroids in high-dimensional data from double-stochastic Gaussian mixtures with power-law-tailed variances.
"We propose RGIB, an information-theory-guided method with two instantiations (RGIB-SSL and RGIB-REP), to improve GNN robustness to edge noise by extracting reliable supervision and preventing representation collapse."
"TRIAGE is a model-agnostic data characterization framework for regression tasks that uses conformal predictive distributions to identify under-, over-, and well-estimated samples and enables improved model performance and novel approaches to dataset and feature selection."
"We propose NN-AGP, a model combining neural networks for reward approximation with Gaussian processes for explicit uncertainty quantification, demonstrating improved accuracy and theoretical guarantees in contextual decision-making tasks."
D2C is a curriculum reinforcement learning method that uses goal-conditional classifiers and bipartite matching to efficiently explore unknown environments with minimal prior knowledge and outperforms previous methods with arbitrarily distributed goal examples.
"We introduce equivariant flow matching, an efficient, symmetry-exploiting training objective for equivariant continuous normalizing flows that improves sampling efficiency and scalability for rotational and permutation-invariant systems."
We propose a frequency-enhanced data augmentation method that improves Vision-and-Language Navigation by training agents to selectively focus on high-frequency visual features relevant to textual instructions.
"We analyze circuit complexity of relational neural network policies for planning, classifying problems by growth of width and depth and demonstrating utility for network design."
"We propose a semi-supervised pre-training method on a large, diverse point-cloud dataset that yields generalizable representations improving downstream perception performance on Waymo, nuScenes, and KITTI benchmarks."
"RGMIL improves instance-level representation in multi-class multiple instance learning by leveraging a regressor-guided pooling aggregator that accurately encodes MIL problem structure, outperforming existing MIL methods and approaching supervised performance on challenging datasets."
"We define high-frequency components from a long-tailed frequency spectrum perspective, show that models under-fit them due to limited information, and propose a Balance Spectrum Sampling method that improves robustness without sacrificing accuracy by enhancing HFC learning."
We propose a derivative-based method that dynamically selects the optimal active learning strategy for a given budget and task.
"We introduce OAR and SimOAR, adversarially robust evaluation metrics for GNN explanations that mitigate out-of-distribution issues and improve computational efficiency."
"The sparse modern Hopfield model extends the dense version by deriving a closed-form sparse energy and retrieval dynamics that approximate sparse attention, with tighter error bounds and proven advantages of sparsity under certain conditions, and empirically outperforms the dense model."
"We propose a tri-modal self-supervised model that predicts open-vocabulary 3D semantic voxel embeddings from 2D images and LiDAR, enabling zero-shot 3D segmentation and language-guided retrieval without 3D annotations."
"Automatic clipping eliminates the need for manual tuning of the gradient clipping threshold in differentially private training while maintaining privacy and efficiency, and it achieves competitive performance without additional hyperparameters."
"The authors propose a doubly-robust estimator that enables valid statistical inference from imperfect surrogate document labels generated by LLMs in computational social science, even when surrogates are highly biased."
We develop a GFlowNet-based method that uses RNA velocity to learn the posterior distribution over cyclic gene regulatory networks in the presence of measurement noise.
"We demonstrate that unsupervised word translation can achieve high accuracy using high-dimensional signals, outperforming low-dimensional methods and challenging their assumed necessity."
"We define and empirically approximate a notion of common information between random variables via optimization, enabling quantification and disentanglement of shared and unique information using a modified variational auto-encoder, and demonstrate its effectiveness on high-dimensional data and datasets with known latent factors."
"We introduce a new NLP task, CLadder, to evaluate large language models' causal inference according to formal rules, using a dataset of causal queries with ground-truth answers and a specialized prompting strategy, revealing that current LLMs struggle with this task."
"We provide tight, computable upper bounds for equalized odds violation in settings without sensitive attributes and a post-processing method to control worst-case equalized odds, clarifying when directly targeting predicted sensitive attributes is optimal."
"We introduce algorithmic and scaling improvements that enable a diffusion language model, Plaid 1B, to outperform GPT-2 124M in likelihood on standard benchmarks."
"This paper establishes asymptotic normality and chi-square results for the multinomial logistic MLE in high dimensions with null covariates and proposes a feature significance test, validated by simulations."
"MathNAS is a divide-and-conquer neural architecture search framework that predicts network performance from pre-trained block performances using mathematical programming, drastically reducing evaluation cost while achieving state-of-the-art accuracy and real-time on-device performance."
"We introduce the buggy-code completion task and show that code large language models’ performance degrades significantly when code context contains bugs, with limited effectiveness of mitigation methods."
We present a method to learn the Signed Distance Function directly from unorganized point clouds without normals by enforcing alignment between the SDF gradient and Hessian to improve shape reconstruction and suppress ghost geometry.
"We propose a differential privacy framework using two gradient oracles to improve second-order stationary point identification in non-convex optimization, and show the regularized exponential mechanism achieves strong risk bounds without smoothness assumptions."
We propose a lightweight low-rank PINN with hypernetwork-based meta-learning to efficiently solve PDEs for varying parameters and mitigate PINN failure modes.
"We propose Confidence Optimal Transport (COT) and its thresholded variant (COTT) to more accurately estimate out-of-distribution error by addressing pseudo-label shift, outperforming existing methods by up to 3x on standard benchmarks."
We propose a single-scale global matching method with a hybrid local-global-cross transformer for point cloud scene flow estimation that achieves state-of-the-art results on multiple benchmarks.
"HiP is a hierarchical foundation model that integrates language, vision, and action models with iterative refinement to solve long-horizon table-top manipulation tasks by generating and grounding symbolic, video-based plans into visual-motor control."
We present an SO(2)-equivariant neural architecture for 3D pose prediction from 2D images that generalizes prior methods and achieves state-of-the-art results on PASCAL3D+ and SYMSOL.
We propose a variational structured memory module with a bionic memory updating strategy that enhances few-shot generation by mimicking human memory mechanisms and integrating with generative models under a Bayesian framework.
"DPVO is a recurrent patch-based deep monocular visual odometry method that outperforms previous state-of-the-art systems while being three times faster and using one third the memory, demonstrating that sparse patch matching with recurrent updates and bundle adjustment surpasses dense flow approaches."
"SmoothHess estimates second-order interactions in ReLU networks via Stein's Lemma and Gaussian smoothing, requiring only gradient calls and providing a sample complexity bound, outperforming Hessian-based methods on benchmark and medical datasets."
"We propose an Active Testing while Learning (ATL) framework that jointly optimizes annotation for both model training and evaluation, using periodic testing and active feedback to reduce total annotation cost while providing unbiased, sample-efficient risk estimation."
We propose a semi-offline reinforcement learning evaluation method that uses human annotations of counterfactual trajectories with importance sampling and a novel weighting scheme to reduce bias and variance compared to standard offline evaluation.
"The Memory-Perturbation Equation (MPE), derived from Bayesian principles, provides a general framework to estimate a model's sensitivity to training data perturbations and predicts generalization performance."
"We present the first private, personalized, and memory-efficient on-device LSH framework that enables local hash table generation without centralized access to data or full weights."
"We propose a language-based prompt similarity method with dynamic re-weighting to address segment-level label noise in weakly-supervised audio-visual video parsing, achieving state-of-the-art performance."
"We give an $\tilde O(k)$ amortized update, $\tilde O(k^2)$ query, $O(1)$-approximate fully dynamic algorithm for $k$-median (and $k$-means) in general metric spaces, and show a matching lower bound of $\tilde \Omega(k)$ amortized update time for any $\tilde O(\text{poly}(k))$-time $O(1)$-approximate algorithm."
We propose a deterministic low-rank Gaussian filter that reduces Kalman filtering complexity from cubic to quadratic (and linear under certain conditions) while outperforming ensemble methods in accuracy.
"GIT is a gradient-based method that efficiently targets interventions to minimize experiments, matching or outperforming baselines especially with limited data."
"For binary events with conditionally independent expert signals, the sample complexity to learn an approximately optimal forecast aggregator is $\tilde{O}(1/\varepsilon^2)$, but in general it is $\tilde{\Omega}(m^{n-2}/\varepsilon)$, growing exponentially with the number of experts."
"We propose Factorized Transformer (FactFormer), a computationally efficient surrogate model for PDEs using axial factorized kernel integration, achieving accurate and efficient simulation on 2D and 3D problems."
"We present ILPD, a single-stage intermediate-level adversarial attack that simultaneously maintains effective adversarial direction and large perturbation magnitude, outperforming state-of-the-art methods by over 10% on ImageNet and 3.88% on CIFAR-10."
"We introduce a logarithmic-time row sampler for the Khatri-Rao product that enables efficient, high-accuracy low-rank approximation of large sparse tensors."
"We characterize unit redundancies and reducible functional equivalence classes in single-hidden-layer hyperbolic tangent networks as piecewise-linear path-connected sets, typically spanning at most 7 linear segments in diameter."
"We propose Graph Differential Privacy (GDP), a framework for differentially private graph learning that enables multigranular privacy guarantees and outperforms existing DP-GNNs in privacy-utility trade-offs via Differentially Private Decoupled Graph Convolutions."
"We introduce Deep GMRFs, a computationally efficient method for state estimation and learning in graph-structured state-space models using variational inference and closed-form posterior sampling."
"We propose a time-series data augmentation method based on mixup that improves feature representations and outperforms state-of-the-art methods on heart rate estimation, human activity recognition, and cardiovascular disease detection by better capturing the non-stationary nature of time-series data."
"The epinet is a supplementary architecture that enables conventional neural networks to estimate uncertainty and outperform large ensembles with far less computation, introducing the ENN interface for joint prediction models beyond traditional Bayesian neural networks."
"PCMCI$_{\Omega}$ is a constraint-based, non-parametric algorithm that identifies causal relations in semi-stationary time series with periodic causal mechanisms without assuming stationarity."
"We introduce SAL and SCoreBO, acquisition functions that prioritize hyperparameter learning for Gaussian processes, outperforming state-of-the-art methods in active learning and Bayesian optimization."
"MAT introduces a selective, module-wise update strategy based on modular neural tangent kernel principal eigenvalues to halve training computation and improve accuracy in over-parameterized models."
We present a Bayesian model of inductive learning that uses a language model to generate natural language hypotheses and predicts human judgments on diverse concept learning tasks by estimating the prior from human data.
"RECKONING improves robust reasoning in language models by integrating contextual knowledge into model parameters via bi-level learning, outperforming in-context reasoning on multi-hop datasets and showing better generalization and efficiency."
"We show that sparse ReLU neural networks with certain sparsity patterns may lack optimal parameters, and provide a method based on topological properties of implementable function spaces to determine when a global optimum exists."
"A neural circuit can simultaneously support attractor dynamics and excitation-inhibition balance if synapses consist of strong fast and weak slow components, enabling faster attractor convergence and improved performance in tracking tasks."
"We present a deep unsupervised method for estimating oriented normals from 3D point clouds without using ground truth normals, by learning neural gradient functions that yield unit-norm gradients consistent with the underlying surface."
MASE is a meta-algorithm that safely explores by combining RL with uncertainty quantification to guarantee no safety violations while achieving better performance on benchmark tasks.
"We propose an IRM-inspired, model-agnostic local explanation method that eliminates unstable feature attributions and yields high-fidelity, stable, and unidirectional explanations using random perturbations, outperforming LIME without requiring data manifold learning or side information."
We propose a high-fidelity multi-band diffusion-based framework that generates diverse audio from low-bitrate discrete representations and outperforms state-of-the-art methods at equal bit rate.
"We propose CPED, which uses a flow-GAN to estimate behavior policy density for less conservative offline RL, theoretically guaranteeing optimal Q-function estimation and outperforming existing methods empirically."
"High-dimensional causal models, such as synthetic control with many controls, can outperform simpler models by acting as model-averagers, improving estimation even after perfect pre-treatment fit."
"SwiFT, a Swin Transformer with 4D window attention and positional embeddings, outperforms state-of-the-art models on fMRI-based prediction of sex, age, and cognitive intelligence while enabling end-to-end learning and interpretability of brain region contributions."
"We propose a keypoint-augmented fusion layer with global and local self-supervised pretraining to enhance UNet-based medical image segmentation by integrating long-range self-attention with CNN features, outperforming existing SSL and standard architectures."
"We propose using counterfactual data augmentation guided by causal knowledge to learn more robust text classifiers by intervening on spurious features, and show improved out-of-distribution accuracy compared to baseline methods in medical and semi-synthetic settings."
"We prove the first convergence guarantee for black-box variational inference with reparameterization gradients under log-smooth posteriors and show that proximal SGD achieves optimal rates, unlike standard implementations with nonlinear scale parameterizations."
We propose a training-free framework that collaborates off-the-shelf large models to reduce reliance on spurious features in open-set object recognition by extracting their implicit knowledge.
"When user consent to cookie sharing varies by demographic, a user's decision to withhold consent can paradoxically lead to greater inference of their attributes by a recommender system, amplifying demographic disparities in privacy-fairness."
"We present a collaborative implicit neural SLAM system with a neural point-based 3D scene representation and distributed-to-centralized learning that achieves superior camera tracking and mapping via unified odometry, loop detection, sub-map fusion, and global refinement."
"SPINN, a separable PINN architecture with forward-mode automatic differentiation, greatly reduces computational cost and memory usage for solving high-dimensional PDEs while improving accuracy, enabling the solution of complex equations such as 3D Navier-Stokes on a single GPU."
"Fed-CO₂ is a federated learning framework that simultaneously addresses label distribution skew and feature skew via cooperation between online and offline models, along with intra- and inter-client knowledge transfer, outperforming existing personalized FL methods in both scenarios."
"We propose MaxCOSD, an online algorithm with provable guarantees for multi-product inventory control under partial information and non-i.i.d. demands, requiring non-degeneracy assumptions for learning."
"We introduce Decision-Pretrained Transformers (DPT), which learn in-context reinforcement learning by predicting optimal actions from state and task history, and show they generalize to new tasks, exhibit exploration and conservatism, and can be viewed as efficient Bayesian posterior sampling with improved sample efficiency."
"ProtoConcepts enhances prototype-based image classification by learning and visualizing prototypical concepts with multiple image patches, improving interpretability without sacrificing accuracy."
"SmooSeg improves unsupervised semantic segmentation by enforcing piecewise smoothness via a novel smoothness loss and self-supervised pseudo labels, outperforming STEGO on COCOStuff, Cityscapes, and Potsdam-3."
"We show that combining contrastive loss and score matching reduces the KL divergence gap in diffusion models by minimizing discretization error during fine-tuning, improving sample quality and sampling efficiency."
"A*Net is a scalable path-based knowledge graph reasoning method that uses a priority function to efficiently select nodes and edges, achieving state-of-the-art results on million-scale datasets while visiting only a fraction of the graph per iteration."
ZipLM is a novel structured compression method for large language models that achieves superior accuracy-speed trade-offs and efficiency across encoder and decoder architectures with minimal computational cost.
"We propose a method for physically plausible insertion of virtual objects into real indoor scenes to augment training data for monocular 3D object detection, significantly improving model performance."
Meta-AdaM is a meta-learned adaptive optimizer with momentum for few-shot learning that leverages weight-update history and a double look-ahead mechanism to improve convergence.
USB-PO is a model-based reinforcement learning algorithm that adaptively controls model updates to guarantee performance improvement by unifying model shift and bias considerations.
We propose a score-based denoising method that estimates the score function of noisy images and solves a resulting system to effectively handle complex noise models.
"Diff-Pruning efficiently compresses diffusion models by pruning non-contributory timesteps using Taylor expansion and gradient ensembling, halving FLOPs with minimal retraining while preserving generative quality."
"ProtoRe improves concept negation in generative models by identifying and purifying negative concepts via prototype-guided feature refinement, outperforming prior methods on image benchmarks."
"We propose a Gini-impurity-preserving encryption scheme for random forests that encrypts features using label and order information and combines it with CKKS to encrypt labels, preserving decision utility while ensuring privacy."
"We propose scalable, versatile unbalanced low-rank optimal transport solvers and demonstrate their effectiveness on spatial transcriptomics matching."
"Adaptive Centered Representations (ACR) improves zero-shot anomaly detection by combining batch normalization and meta-training, achieving state-of-the-art results on tabular and domain-specific image data."
"We introduce an asymptotically locally Bayes optimal message-passing graph neural network for node classification on sparse, fixed-dimensional graphs and show its performance interpolates between MLPs and convolutions depending on the graph signal, with theoretical non-asymptotic guarantees."
"We propose Variational Rectified Activation (VRA), which optimally suppresses low and high and amplifies intermediate activations for out-of-distribution detection, outperforming ReAct and existing methods."
"We analyze when randomized ensembles improve adversarial robustness over deterministic classifiers and show that for any probabilistic binary classifier, a deterministic classifier exists that outperforms it, explicitly describing such a set for common probabilistic classifiers."
We investigate configuration strategies for in-context image-text pairs in vision-language image captioning and find that optimized combinations improve CIDEr scores by 20.9 compared to random sampling.
"MeLoDy is an LM-guided diffusion model that generates high-quality music with up to 99.6% fewer forward passes than MusicLM, using a novel dual-path diffusion approach for efficient semantic-to-audio decoding."
"We introduce SemSim, a learning-based metric trained on human annotations, which more accurately reflects human judgment of privacy leakage in reconstructed images compared to hand-crafted metrics like PSNR and SSIM."
"We present GPT4Tools, a LoRA-based method that enables open-source LLMs to use multi-modal tools via a self-instruct-generated dataset, improving both seen and zero-shot tool use performance."
"We characterize the learnability of multilabel ranking with relevance-score feedback for a broad class of losses in batch and online settings, identifying two equivalence classes of losses based on learnability."
"We resolve fundamental identifiability and interaction challenges in softmax gating Gaussian mixture of experts by introducing Voronoi loss functions and deriving convergence rates for MLE, linking over-specified cases to polynomial system solvability."
"We propose MATTE, a theoretically grounded model that identifies domain-varying dependencies between content and style for unsupervised counterfactual generation without paired data or labels."
"This paper provides near-optimal regret bounds for reinforcement learning with delayed or missing state observations, showing that efficient learning is still achievable with regret depending on the system's state-action size."
"RayDF is a ray-based 3D shape representation framework that introduces a ray-surface distance field, a dual-ray visibility classifier, and multi-view consistency optimization, outperforming coordinate- and ray-based baselines in surface reconstruction and rendering speed."
We propose an efficient online policy achieving sublinear approximate regret for fair resource allocation with a phase transition at α = 1/2.
"Back-Modality is a data augmentation framework that transforms data between modalities and back, enhancing performance in data-scarce tasks across image and text domains."
We introduce and compute a group-fair peer review assignment for large AI conferences to prevent community withdrawal while preserving interdisciplinary interaction.
We propose a differentiable Gaussian process regression method that infers implicit low-dimensional manifold structure from data to improve scalability and predictive performance in high dimensions.
"Context-PIPs improves persistent point trajectory estimation in videos by integrating spatial context features through SOFE and TAFA modules, significantly reducing trajectory error and increasing tracking accuracy."
"We propose a Thompson sampling algorithm with dynamic episode sizes for optimal control of countably infinite state-space MDPs with unknown parameters, achieving a Bayesian regret bound of $\tilde O(dh^d\sqrt{|\mathcal A|T})$ under ergodicity assumptions and demonstrate its application to queueing models."
"We propose a self-supervised method for acoustic matching that learns from target audio and image alone, outperforming state-of-the-art approaches on real-world data."
"We propose a hierarchical vector quantized prototype-oriented Transformer for unsupervised image anomaly detection that uses discrete iconic prototypes to avoid the ""identical shortcut"" issue and achieves state-of-the-art results on MVTec-AD and VisA datasets."
"We generalize Plug-and-Play methods for Poisson inverse problems by replacing the Gaussian denoiser with a Bregman Score Denoiser that serves as the proximal operator in a Bregman Proximal Gradient framework, ensuring convergence to stationary points and demonstrating effective restoration performance."
"A-NeSI is an approximate neurosymbolic inference framework that enables scalable, end-to-end probabilistic reasoning with symbolic explanations and constraint satisfaction, outperforming exact methods on exponential tasks without sacrificing performance."
"Recurrent neural circuits with separate reservoir and sampler units can sample from arbitrary distributions via Langevin dynamics, enabling more expressive sampling-based Bayesian brain models."
"We propose a geometrically grounded, differentiable approach using spherical transform and a robust correlation-aware homography estimator for fine-grained cross-view geo-localization, achieving sub-pixel alignment and meter-level GPS accuracy with superior performance on VIGOR and KITTI benchmarks."
"Separate normalization layers for [CLS] and tokens in transformers improve downstream performance by 2.7% across image, language, and graph domains."
"We propose Irreversible Backdoor Attack (IBA), a stealthy and durable backdoor attack framework for federated learning that efficiently poisons model parameters and evades detection while maintaining high attack success and persistence."
"mSMI is a scalable, information-theoretic generalization of CCA that captures nonlinear dependencies and outperforms existing methods in high-dimensional settings."
"We propose CWAEE, a semi-supervised domain generalization method that detects known and unknown classes in mixed-domain data and leverages consistency regularization for improved generalization to unseen domains."
"We propose a model-based imitation learning method using deep generative state modeling and energy-based policy priors for non-Markovian decision-making, demonstrating effective decision-making as inference in both prototypical and MuJoCo planning tasks."
DeepACO is a deep reinforcement learning-enhanced Ant Colony Optimization framework that automates heuristic design and outperforms standard ACO and problem-specific methods on multiple combinatorial optimization problems using a single model and hyperparameter set.
"BaCon addresses distribution-agnostic generalized category discovery in long-tailed open-world settings by combining a contrastive-learning and a pseudo-labeling branch with self-balanced knowledge transfer and a novel contrastive loss, outperforming existing baselines on multiple datasets."
"We propose MetaSin, a trainable ensemble activation that outperforms ReLU and standard sin activations in Monte-Carlo denoising and image resampling without requiring special initialization."
"Af-DCD is a contrastive knowledge distillation method for semantic segmentation that uses a masked feature mimicking strategy and a novel loss to efficiently transfer dense local knowledge without data augmentation or memory buffers, achieving state-of-the-art performance on multiple benchmarks."
"We derive optimal rotation-invariant estimators for noisy matrix factorization with high-rank symmetric factors using random matrix theory and statistical mechanics, and numerically confirm their optimality."
"In constant-sum polymatrix games with subgame stability, self-play with no-external-regret algorithms yields strategies with bounded vulnerability against post-training opponents."
"We show that for accurate predictions in personalized high-stakes domains, users need to disclose only a small subset (e.g., 10%) of their features without loss in accuracy."
We propose a method that adaptively generates task-specific word embeddings using contextual semantic graphs and a Gaussian mixture prior to improve topic modeling for low-resource settings.
PPO with achievement distillation efficiently discovers hierarchical achievements in procedurally generated environments using fewer parameters and less data than previous methods.
We generalize residual neural networks to arbitrary Riemannian manifolds and show they outperform existing manifold-specific networks on hyperbolic and SPD matrix manifolds.
"We resolve the asymptotic generalization of overparameterized linear multiclass classification under Gaussian covariates, showing tight misclassification thresholds and that the min-norm interpolating classifier can be suboptimal, using a new Hanson-Wright inequality variant applicable to sparse multiclass labels."
"GraphMP is a neural motion planner that integrates customized GNN architectures and training mechanisms to enhance both graph pattern extraction and search, improving path quality and planning speed across low- and high-dimensional environments."
"We derive tight sample complexity bounds for distributionally robust RL with generative models under TV and χ² divergence uncertainty, showing that robustness can be less sample-intensive than standard RL under TV, but scales linearly with uncertainty under χ²."
"AMenuNet is a scalable, permutation-equivariant neural network that generates dominant strategy incentive compatible and individually rational multi-item auctions with high revenue and minimal dependence on auction size."
"PanoGen generates diverse, text-conditioned panoramic environments to improve Vision-and-Language Navigation agents’ generalization to unseen settings, achieving state-of-the-art results on multiple datasets."
"We propose variational imbalanced regression (VIR), a probabilistic deep model that improves accuracy and uncertainty estimation in imbalanced regression by leveraging label-similar data for latent representations and predicting full predictive distributions with probabilistic reweighting."
"We propose a hierarchical goal-conditioned reinforcement learning algorithm that leverages subgoal decomposition to robustly learn from offline data, enabling long-horizon goal-reaching in high-dimensional settings."
"We establish tight relationships and separations between quantum learning models using entangled, separable, and statistical measurements in the quantum statistical query model, providing exponential separations for certain concept classes and introducing new lower bound techniques."
"Neural Geodesic Field (NeuroGF) encodes all-pairs geodesics on 3D meshes via implicit learning, enabling efficient and accurate geodesic queries and generalizing to unstructured point clouds."
"Transformers' representation layers exhibit a consistent pattern of intrinsic dimensionality—rising in early layers, peaking, contracting in intermediate layers, and stabilizing later—with semantic content maximized at the first intrinsic dimensionality minimum."
"We propose a text-driven image-to-image translation method that edits regions of interest in a source image using a pretrained diffusion model and a novel conditional score function incorporating both source image and text, enhanced by a Gaussian-based gradient adjustment and a mixup technique for improved fidelity."
We propose an online learning algorithm that achieves $\tilde{\mathcal{O}}(\sqrt{T})$ strategic regret under stochastic agent sequences and $\tilde{\mathcal{O}}(T^{(d+1)/(d+2)})$ under adversarial sequences in a high-stakes setting with apple-tasting feedback and strategic agents.
"We introduce Monarch Mixer (M2), a sub-quadratic architecture using Monarch matrices that matches or outperforms Transformers in language modeling and vision tasks while reducing parameters and improving throughput."
"We propose a spike camera–guided deblurring model that integrates high-speed spike streams with blurry RGB images to effectively restore sharp images from motion blur, outperforming state-of-the-art deblurring methods."
"We propose a covariance neural network framework that provides anatomical interpretability for brain age prediction using cortical thickness, revealing region-specific contributions to brain age gaps in Alzheimer’s disease."
We propose a Wasserstein gradient descent method for estimating the rate-distortion function that learns the support of the optimal reproduction distribution and achieves tight bounds with less tuning than neural network methods.
"We present the first polylogarithmic-approximate fair hierarchical clustering algorithm that achieves low cost, bridging the gap between state-of-the-art fair and vanilla methods."
"We introduce the k-PC algorithm, a sound constraint-based method for robust causal discovery under sample constraints by limiting conditioning set size to k and defining k-Markov equivalence classes."
"SeqMatch, a sequential subset matching strategy for dataset distillation, addresses coupling issues in synthetic data generation and outperforms state-of-the-art methods on SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet."
"We propose MARS, a data-dependent UCB algorithm for symmetric multi-armed bandits that avoids reliance on known scale parameters and achieves a regret bound comparable to ψ-UCB without correction factors."
"Emma-X, an EM-like multilingual pre-training algorithm leveraging non-parallel multilingual data and joint semantic prediction, achieves state-of-the-art performance and superior representation universality on the xrete benchmark."
PLASTIC improves sample efficiency in RL by preserving both input and label plasticity through smoother loss landscapes and refined gradient propagation.
"We provide tight bounds and a simple local search algorithm for constructing small volumetric spanners under all ℓ_p norms, and apply these to improve coresets for the Minimum Volume Enclosing Ellipsoid problem."
"We derive tight, general risk bounds for unregularized gradient methods in separable linear classification, showing dependence on loss tail decay and improving previous results with a simpler proof that extends to SGD."
"BIOT is a flexible biosignal encoder that tokenizes diverse biosignal formats into a unified structure, enabling cross-dataset pre-training and outperforming baselines on EEG, ECG, and activity sensor tasks."
"Chain-of-thought explanations generated by large language models can be misleading and fail to reflect the true basis of model predictions, even under input bias, thus undermining their transparency and safety benefits."
FedSupLinUCB achieves near-optimal regret $\tilde{O}(\sqrt{dT})$ and controlled communication cost in federated linear bandits with adversarial finite action sets.
"TALAR improves natural language-conditioned RL by introducing a simplified, task-related task language translated from natural language, resulting in better policy comprehension and higher success rates."
We propose a diffusion-based low-light image enhancement framework with global structure-aware and uncertainty-guided curvature regularizations that improves image quality and noise suppression compared to state-of-the-art methods.
"ChatIR is a chat-based image retrieval system that uses dialogue with users to improve image search accuracy, achieving over 78% retrieval success after five rounds compared to 64% for single-query retrieval."
DAFT-RL introduces a framework that uses object-centric representations and attribute-based graph models to improve compositional generalization and sample efficiency in reinforcement learning with novel objects and task compositions.
"SeViLA is a parameter-efficient BLIP-2-based framework that uses a language-aware localizer and answerer chained for query-aware keyframe selection and question answering in videos, achieving state-of-the-art results on multiple benchmarks without expensive annotations."
"ParaDiGMS accelerates diffusion model sampling by parallelizing denoising steps via Picard iterations, achieving 2–4× speedups without quality loss."
"SyncTREE is a tree-based GNN that improves RC-tree interconnect timing analysis in ICs by using two-pass message passing, a tree contrastive loss, and a closed-form timing model, outperforming conventional GNNs on real IC data."
"EDGI is an SE(3) × ℤ × Sₙ-equivariant diffusion-based MBRL algorithm that improves sample efficiency and generalization in embodied agents by leveraging spatial, temporal, and permutation symmetries."
We propose treating policy planning as text-conditioned video generation to create generalizable agents that can combinatorially generalize to novel goals and transfer knowledge from internet videos to real-world robot tasks.
"Conan is an interactive open-world environment designed to evaluate active reasoning and reveal the limitations of current vision-language models in multi-round, incomplete-information question answering."
"HIPIE unifies hierarchical, open-vocabulary segmentation of images at semantic, instance, and part levels using decoupled text-image fusion and differentiated representation learning for ""things"" and ""stuff""."
We present an efficient Douglas-Rachford-based algorithm for regularized optimal transport with strong convergence guarantees and superior speed in domain adaptation and generative model learning.
"The authors introduce ActionBench to diagnose and address video-language models’ poor action knowledge, and propose Paxion with a Discriminative Video Dynamics Modeling objective to significantly improve action understanding without degrading other capabilities."
We propose robust variants of SPS and SLS with optimal convergence rates across most settings and a variance-reduced acceleration method that achieves optimal rates when combined with these variants.
"Vision-and-language models like CLIP and Stable Diffusion exhibit sound symbolism, as demonstrated by zero-shot probing reflecting the kiki-bouba effect."
"We present a scalable 3D instance segmentation method that lifts 2D instance segments using a slow-fast clustering objective and neural fields, outperforming state-of-the-art approaches on multiple datasets including a new Messy Rooms dataset with up to 500 objects per scene."
We introduce and algorithmically solve a fractional liquid democracy rule with ranked delegations that simultaneously generalizes copy-robustness and anonymity using the Markov chain tree theorem and Fulkerson's algorithm.
"We integrate Lie point symmetries into PINNs via a symmetry loss that enforces equivariant solution behavior, significantly improving sample efficiency."
We propose a complete subgoal search method that combines high-level and low-level actions to guarantee completeness in discrete planning while maintaining efficiency.
"We propose a low-rank factorization method, informed by kissing number theory, to efficiently approximate large permutation matrices, drastically reducing memory use and enabling the solution of previously infeasible large assignment and matching problems."
"We propose spectral normalization to stabilize gradient variance in model-based reparameterization policy gradient methods for long-term reinforcement learning, improving convergence and performance."
We discover that directly removing smallest-magnitude weights from large pre-trained transformers reveals a sharp essential sparsity threshold and that self-supervised learning induces stronger sparsification than supervised learning.
We propose a deep ensemble-based reset method for deep reinforcement learning that mitigates primacy bias and improves sample efficiency and safety.
"We propose Robust State-Confounded MDPs and an algorithm that learn policies robust to spurious correlations caused by unobserved confounders, outperforming baselines in self-driving and manipulation tasks."
"We introduce Primal-Attention, a novel self-attention mechanism using asymmetric Kernel Singular Value Decomposition that optimizes attention via variance maximization and regularization, improving efficiency and promoting low-rankness without explicit kernel decomposition."
"We present a computationally tractable algorithm for robust Bayesian optimization against general φ-divergence data shifts, achieving sublinear regret and outperforming existing methods."
"QuinNet is an end-to-end graph neural network that efficiently and accurately models up to five-body interactions in molecular dynamics simulations, outperforming state-of-the-art models on complex systems without increasing computational cost."
We formulate meta-optimization as an optimal control problem with convex relaxation to obtain regret guarantees against the best offline optimizer.
"The Structured Neural Network (StrNN) injects conditional independence structure into neural networks via masked pathways designed using binary matrix factorization, enabling more data-efficient generative modeling and causal inference."
AdvInfoNCE is a contrastive loss tailored for collaborative filtering that adaptively mines hard negatives and improves generalization in out-of-distribution recommendation tasks.
"We propose a method to learn flexible, layer-wise equivariance from data using differentiable marginal likelihood optimization, achieving performance comparable to or better than models with hard-coded symmetries."
"We introduce prediction-based sequential nonparametric two-sample and independence tests that use sequentially updated predictors to detect distributional differences without requiring kernel selection, outperforming kernel-based methods on structured data and maintaining validity under data distribution drift."
"We propose a Bayesian pseudocoreset construction method that matches function-space variational approximations of posteriors to enable scalable, robust Bayesian inference with improved uncertainty quantification for deep neural networks."
"We propose USC-PFN, a parser-free virtual try-on network that uses unified self-cycle consistency and a Markov Random Field to achieve realistic garment deformation and state-of-the-art performance without relying on auxiliary tasks or prior knowledge."
"We systematically evaluate six video self-supervised learning methods on six types of natural distribution shifts and find that v-MAE is robust to context shifts and excels at temporal learning, while contrastive methods are more robust to viewpoint shifts, with a trade-off observed between closed- and open-set recognition without fine-tuning."
"We propose a few-shot image restoration method that, given a pre-trained model and a few restored images, can trade off perceptual quality and MSE at test time via closed-form latent-space linear transformations."
Neural Lad is a neural ODE-based latent dynamics model that incorporates attention-based signal change and residual periodicity modeling to improve forecasting for univariate and multivariate time series.
We propose a Riemannian Laplace approximation for Bayesian neural networks that better matches non-Gaussian posteriors and improves robustness to prior choice by using a learned Riemannian metric based on the log-posterior gradient.
"We introduce feature-convex neural networks that provide efficient, closed-form certified robustness against asymmetric adversarial attacks for a sensitive class across various norms."
"We propose FourierGNN, a graph neural network using a novel hypervariate graph representation and Fourier space operations for efficient and accurate multivariate time series forecasting."
We propose a dual-level framework that detects domain and semantic shifts using global low-level and dense high-level features to improve out-of-distribution segmentation under domain shift.
"DiffLogic is a differentiable neuro-symbolic framework that adaptively selects essential triples using a dynamic rule-based filter and integrates probabilistic soft logic to jointly optimize embeddings and weighted rules, improving both effectiveness and efficiency in knowledge graph reasoning compared to existing baselines."
We present a Monte Carlo method for Bayesian structure learning in Gaussian Process Networks that outperforms existing algorithms in graph recovery and posterior approximation.
"We propose Marich, a model-oblivious active sampling algorithm that efficiently extracts high-fidelity replicas of black-box models using minimal queries from public datasets."
"We compare segmentation and classification for radiology image tasks, showing segmentation-for-classification improves sample efficiency, robustness, recall for rare classes, and interpretability with fewer labels."
"We introduce GET, a DEQ-based model that directly distills diffusion models from noise to image in one step, achieving superior performance with less computational cost than larger ViT models."
"Alternating gradient descent with fixed step size and a specific random initialization achieves near-optimal convergence for asymmetric matrix factorization in a number of iterations proportional to the squared condition number times the logarithm of the inverse error, with mild overparameterization ensuring a constant factor."
"We theoretically and empirically analyze the robustness of removal-based feature attribution methods under input and model perturbations, revealing how model Lipschitz regularity affects attribution stability."
"We propose a three-parameter algebra based on the generalized Gamma distribution to accurately track and compute heavy-tailed distributions during static analysis in probabilistic programming, improving inference performance in density modeling and variational inference."
"We propose using the light slab representation for neural light fields, achieving faster training and better real-time novel view synthesis on mobile devices than previous methods."
"Linear interpolation stabilizes neural network training by leveraging nonexpansive operator theory, and combining it with appropriate optimization methods yields convergent algorithms with improved performance on GANs."
"We analyze matricization strategies for low-rank tensor recovery under noise, proving sharp recovery thresholds for unfolding and partial tracing in general tensor shapes, and showing exact recovery for power iteration and recursive unfolding under certain conditions using random matrix theory."
"We analyze how graph neural networks model interactions across graph partitions using separation rank and walk index, and introduce a sparsification method that preserves these modeling capabilities more effectively than existing approaches."
"We propose and analyze two function approximation-based experiment planning strategies for contextual bandits with pre-collected contexts and few or general reward functions, including an eluder-dimension-dependent method and a competitive uniform sampling approach when actions are few, and we identify a statistical gap between planning and adaptive learning."
"We present greedy Poisson rejection sampling (GPRS), an optimal-time algorithm for one-shot channel simulation in one dimension with unimodal density ratios, outperforming state-of-the-art A* coding."
"We provide tight upper and lower bounds on the robust overfitting bias in multiclass classification as a function of the number of classes, robust accuracy queries, and test examples."
"LEACE is a closed-form method for minimal concept erasure in representations, enabling concept scrubbing in large language models to measure concept reliance and reduce bias."
"We propose RAS, a risk-averse active sensing method that decides when and what to measure for cost-effective, timely, and accurate healthcare outcome prediction, especially for high-risk patients."
An interpretable model using spatial-temporal logic rules and an EM algorithm learns and predicts human movement patterns from trajectory data with demonstrated performance on pedestrian and basketball datasets.
"The authors extend concept-based interpretability methods to train models that are more interpretable, less biased, and capable of incorporating prior knowledge by integrating concept losses and distillation into model training."
"LICO improves the explainability of saliency-based image classification by aligning image and language features and using optimal transport for fine-grained attention maps, also enhancing classification performance without inference overhead."
We present a self-supervised method that jointly estimates camera poses and reconstructs 3D neural scene representations from real-world video in a single forward pass.
"FactorCL is a self-supervised multimodal representation learning method that captures both shared and modality-unique task-relevant information without labels, achieving state-of-the-art results on six benchmarks."
"DT2GS is a novel framework that decomposes multi-agent reinforcement learning tasks into generalizable subtasks using scalable encoding and adaptive semantics, demonstrating superior zero-shot transfer and outperforming existing methods."
"Adaptive weight decay, which adjusts the weight decay hyperparameter per iteration based on classification and regularization losses, improves adversarial robustness without extra data or tuning."
"We prove that gradient descent on the DDPM objective can efficiently recover parameters of Gaussian mixture models under certain separation conditions, leveraging connections to EM and spectral methods."
Cross-Episodic Curriculum (CEC) improves Transformer agent learning by structuring cross-episodic experiences into a curriculum that enhances learning efficiency and generalization in both multi-task reinforcement and imitation learning settings.
We propose a training-free continual learning method for pre-trained models using random projection and class-prototype accumulation that outperforms prior approaches on multiple benchmarks without rehearsal.
"TreeVAE is a generative hierarchical clustering model that learns tree-structured latent representations, enabling flexible clustering, improved log-likelihood, and conditional sample generation."
"We propose an online density ratio estimation method with theoretical guarantees to address continuous covariate shift in sequential settings, enabling adaptive training with minimal cumulative prediction risk."
MixVal is an unsupervised domain adaptation model selection method that uses unlabeled target data and mixed sample probing to stably and effectively select the best UDA model without labeled target data.
"MarioGPT is a GPT2 model fine-tuned to generate Super Mario Bros levels from text prompts and, combined with novelty search, enables open-ended, diverse, and controllable procedural content generation."
"We propose Anchor Data Augmentation (ADA), a causality-inspired data augmentation method for over-parameterized nonlinear regression that outperforms C-Mixup by generating multiple modified sample replicas to improve robustness."
"We show that five prominent deep multiple instance learning models violate core MIL assumptions by failing simple synthetic tests, risking incorrect learning and operational failure in real applications."
"We propose LAMP, a framework that integrates large language model abductive reasoning to improve event sequence prediction by retrieving and evaluating potential causal events, significantly outperforming state-of-the-art models on real-world datasets."
"D$^3$R is a novel end-to-end anomaly detection network for non-stationary multivariate time series that uses data-time mix-attention for dynamic decomposition and noise diffusion for external control of the information bottleneck, achieving an 11% average relative improvement over prior SOTA methods on real-world datasets."
We show that last-iterate convergence in Wasserstein distance for advanced sampling schemes can be established by analyzing their continuous-time counterparts using dynamical systems theory.
"Scale-Space HyperNetworks (SSHN) learn a range of CNNs with varying rescaling factors to efficiently explore the accuracy–efficiency trade-off in medical image analysis, outperforming fixed and dynamic rescaling at lower training cost."
"We mathematically show that the limiting behavior of Transformer representations, determined by the spectrum of the value matrix, leads to low-rank Boolean self-attention matrices and the emergence of token leaders."
"FourierHandFlow introduces a 4D continuous hand representation using Fourier series-based query flows to model articulation-aware, smooth temporal shape dynamics from RGB videos, outperforming prior methods in reconstruction and efficiency."
Inference-Time Intervention (ITI) improves LLM truthfulness on TruthfulQA with minimal data and computational cost by shifting activations along learned directions during inference.
"We introduce Learning-to-Modulate (L2M), a method that enables continual reinforcement learning by modulating frozen pre-trained models to retain performance on prior tasks while excelling on new ones, and we release a combined dataset of 50 Meta-World and 16 DMControl tasks."
We propose a polynomial-time feature-adapted Lasso that achieves near-optimal sample complexity for sparse linear regression under correlated Gaussian covariates with a few outlier eigenvalues.
"We propose a generalized active learning framework for regression that supports diverse data types and approximation spaces, introducing generalized Christoffel functions to optimize sampling and demonstrating improved efficiency in scientific computing applications."
"The authors propose a recurrent spiking neural network with a spiking convolutional block attention module to jointly extract spatial and temporal features from DVS time-series data, achieving higher accuracy with efficient memory use."
We introduce label randomizers for regression that use a privately estimated label prior to improve privacy-utility trade-offs and demonstrate their superiority with theoretical insights into unbiased randomizers.
"We analyze spectral methods for estimating low-rank matrices in reinforcement learning, showing they achieve near-optimal entry-wise error and enable state-of-the-art algorithms for low-rank bandits and MDPs."
"The paper shows that in any sequential game, it is possible to achieve sublinear no-linear-swap regret—regret against all linear transformations of mixed strategies—in polynomial time, establishing the existence of efficiently approachable linear-deviation correlated equilibria in extensive-form games."
We prove an improved prediction error bound for CART under a sufficient impurity decrease condition and provide sufficient conditions for its satisfaction in additive models with locally reverse Poincaré inequalities.
"Knowledge distillation provides limited privacy against membership inference attacks, especially when training sets are similar or the teacher set is poisoned."
We propose an MCMC-EM fine-tuning method that improves large language models' accuracy on reasoning tasks using chain-of-thought prompting without requiring manually written rationales.
"VoxMol is a score-based method that generates 3D molecules as atomic densities on grids via denoising, outperforming diffusion-based approaches in both sample quality and speed."
"We propose a rehearsal learning framework that identifies actionable decisions to prevent undesired outcomes predicted by ML models, using structural rehearsal graphs and providing a PAC bound for risk quantification."
"Thought Cloning improves AI agents by imitating both human behaviors and their internal thoughts, leading to faster learning, better out-of-distribution generalization, and enhanced safety and interpretability."
"BiDock is a rigid protein docking model that integrates sequence and structure information via bi-level optimization, achieving up to a 234% relative improvement over baselines in antibody-antigen docking."
"We present an efficient, dimension-independent, differentially private range counting algorithm with tunable small additive error and small multiplicative error using a novel variant of Locality-Sensitive Hashing."
"We present a simple, data-efficient framework that integrates language models and random walks to generate unsupervised node embeddings capturing both attribute semantics and graph structure, outperforming state-of-the-art methods on real-world attributed graphs."
"We present a preprocessing method using a tree data structure that achieves o(nmd) per-iteration time for neural network inference with only O(nmd) preprocessing, and show a conditional lower bound on further speedup."
"We propose an SE(3) diffusion model that frames 6D object pose estimation as a denoising process on the SE(3) manifold, achieving robust point cloud registration in real-world settings."
"We propose converting irregularly sampled time series into line graph images and using pre-trained vision transformers for classification, achieving state-of-the-art results and strong robustness to missing data."
"We show that while meta-learned update rules with momentum improve convergence rates in single-task meta-learning, achieving true acceleration requires an optimistic approach such as Bootstrapped Meta-Gradient."
"We introduce a collapsed inference method for Bayesian neural networks that leverages volume computation to achieve scalable, accurate uncertainty quantification and predictive performance."
"We propose a unified conditional diffusion model framework for image restoration that integrates spatially-adaptive guidance at every diffusion step and enables arbitrary-resolution generation, improving performance on low-light denoising, deblurring, and JPEG restoration."
"We prove convergence to a global optimum for shallow neural networks in a two-timescale training regime with small inner-layer stepsizes, without requiring large neuron count, and show SGD converges in this regime but not necessarily otherwise."
"TrojLLM is a black-box framework that generates universal, stealthy triggers and poisoned prompts to manipulate LLM outputs via adversarial and Trojan attacks, demonstrating effectiveness against real-world LLM APIs."
"Gradient methods for two-player zero-sum differentiable games converge locally to Nash equilibria if the symmetric Jacobian part is nonzero and its kernel is in general position relative to the antisymmetric part, and the convergence rate depends on the average of the symmetric eigenvalues when the symmetric part is small compared to the antisymmetric part, implying that adding over-parameterized degrees with curvature can accelerate equilibrium computation."
"SimFBO and its variant ShroFBO provide a simple, sub-loop-free federated bilevel optimization framework with efficient aggregation and robustness to system-level heterogeneity, achieving provable improvements in convergence speed, sample complexity, and communication efficiency."
"We derive minimax bounds on the power loss of standard meta-analysis test combinations in high-dimensional many normal means models under mild restrictions, revealing an elbow effect and connections to distributed inference."
Rememberer is an evolvable LLM-based agent with long-term memory and RLEM learning that outperforms prior SOTA by 4% and 2% in success rate on two RL tasks without fine-tuning the LLM.
"We introduce a continuous parametric optical flow model using B-splines and neural ODEs to estimate dense, continuous motion from sparse frames, outperforming discrete and point-tracking methods on long-term sequences."
"We propose a lightweight initialization-based proxy that measures feature, parameter, and gradient consistency between clean and perturbed images to enable fast, efficient neural architecture search for robustness to diverse perturbations with reduced computational cost."
Continual reinforcement learning is defined as the setting where the optimal agents must continuously search and adapt indefinitely.
"TART improves LLMs' reasoning and task performance generically across models and modalities by adding a synthetically trained reasoning module, revealing that the in-context learning gap is due to deficient probabilistic reasoning rather than inadequate representations."
"We present efficient algorithms for diagonal preconditioning and solving structured linear systems, achieving improved runtimes over existing methods by solving matrix-dictionary approximation and recovery problems via novel SDP algorithms."
"We show that atypicality, not just confidence, strongly correlates with miscalibration and accuracy, and that incorporating atypicality estimates improves uncertainty quantification and model performance in neural networks and large language models."
"We propose a self-supervised learning method that learns general-purpose representations of PDEs from heterogeneous data, improving PDE coefficient regression and neural solver time-stepping."
"We present a first-order algorithm that solves box-simplex games in $d \times n$ matrices with bounded rows in $O(\log d \cdot \epsilon^{-1})$ matrix-vector queries, improving complexities for $\ell_\infty$ regression and related combinatorial optimization problems, and extend it to a near-linear time algorithm for a matrix generalization relevant to semidefinite programs."
"XTR improves multi-vector retrieval by refining token retrieval to enable efficient, high-recall candidate scoring, achieving superior performance over ColBERT with much lower computational cost."
"We propose JKO-iFlow, a neural ODE-based normalizing flow using residual blocks and adaptive time reparameterization that achieves competitive performance with lower computational and memory cost than existing flow and diffusion models."
"We propose POLO, an oracle-efficient algorithm for low-rank MDPs with adversarial losses that achieves $\widetilde{O}(K^{5/6}A^{1/2}d\ln(1+M)/(1-\gamma)^2)$ regret and prove a matching $\Omega(\frac{\gamma^2}{1-\gamma}\sqrt{dAK})$ lower bound, showing that low-rank MDPs are harder to learn than linear MDPs in the adversarial setting."
"We modify grid-based NeRFs by training model heads at multiple spatial resolutions and selecting the appropriate one at render time, reducing aliasing artifacts and error rates by 20–90% with minimal performance overhead compared to Mip-NeRF."
"HAIDNet is a neural-network framework for adaptive information design that approximates optimal policies even when receiver behavior deviates from Bayesian rationality, and it generalizes to complex or previously unsolved settings, including real-world human subjects."
"We propose a metric learning loss that organizes feature space by depth without increasing model size or inference time, improving monocular 3D object detection by 23.51% on KITTI and 5.78% on Waymo."
"PanoGRF is a spherical radiance field method that improves novel view synthesis from wide-baseline panoramas by directly leveraging panoramic geometry and appearance without panorama-to-perspective conversion, and by incorporating 360° depth priors."
We propose a diffusion-based image super-resolution method that achieves high performance with only 20 sampling steps by efficiently transferring between low- and high-resolution images via a residual-based Markov chain and a flexible noise schedule.
SPEED is a dataset distillation framework using sparse parameterization and dictionary learning to efficiently represent and distill high-resolution datasets with state-of-the-art performance.
"SORL formulates open-world semi-supervised learning as a graph spectral decomposition problem, providing theoretical error bounds and outperforming baselines on standard benchmarks."
"We rigorously analyze the mean field limit of kernel methods and Support Vector Machines as the number of input variables tends to infinity, establishing convergence of empirical and infinite-sample solutions and their risks in this novel setting."
"We propose Self-Notes, a method where models interleave explicit reasoning with input text to enable multi-step reasoning and improved memory retention, outperforming chain-of-thought and scratchpad approaches."
"We propose a method combining deep learning and Bayesian optimization to personalize neuroprosthetic stimulation, improving perceptual outcomes in visual prostheses despite model inaccuracies and noisy feedback."
"We provide the first theoretical analysis of multi-instance Partial Label Learning with possibly unknown transitions, establishing a necessary and sufficient learnability condition and deriving error bounds, while highlighting scalability challenges in weak supervision."
"AdANNS improves accuracy–compute trade-offs in web-scale ANNS by using adaptive, matryoshka representations that vary in capacity across retrieval stages."
"We introduce Multiplicative Smoothing (MuS), a model smoothing technique that, when combined with feature attribution methods like LIME and SHAP, provides formal stability guarantees for machine learning models by enforcing Lipschitz continuity with respect to feature masking."
"We propose a regularity condition in the interpolation regime that enables stochastic gradient descent to achieve the same worst-case iteration complexity as deterministic gradient descent using a single sampled gradient per step, and show this condition holds for sufficiently wide feedforward networks with a linear output layer."
"ACE enables efficient, simulation-amortized generalized Bayesian inference by training a neural network to approximate the cost function for misspecified simulators, improving parameter inference with fewer simulations."
"We systematically study the integration of semi-supervised learning into programmatic weak supervision and find that simple methods often match complex state-of-the-art approaches, with SSL being most beneficial for small end models and when weak supervision labels only a small fraction of data."
"NEO-KD improves robustness of multi-exit neural networks against adversarial attacks via neighbor and orthogonal knowledge distillation, achieving higher adversarial accuracy with less computation."
"PuzzleFusion, a diffusion model-based system, solves jigsaw and room arrangement puzzles end-to-end and outperforms prior methods on new synthetic and real datasets."
"We propose a method using discrete Morse theory to estimate structure-wise uncertainty in curvilinear segmentation, outperforming pixel-wise approaches on 2D and 3D datasets."
"Tree-Ring Watermarking embeds a robust, invisible Fourier-based fingerprint into diffusion model outputs during sampling, enabling reliable detection after generation with minimal quality loss."
"We show that deep equilibrium models with infinitely wide layers converge to a non-degenerate Gaussian process, even when depth and width limits are interchanged."
"We propose BASS, a contextual bandit-based task scheduling framework for meta-learning that adaptively balances exploration and exploitation to optimize task selection without pre-defined protocols."
We propose a stochastic accelerated gradient-extragradient algorithm for strongly monotone variational inequalities that achieves optimal convergence rates and optimal statistical error in bilinear saddle-point problems.
We propose a temperature-controlled logistic-softmax likelihood for Bayesian Gaussian process meta-learning that improves uncertainty calibration and achieves strong few-shot classification performance.
"Pathwise regularization of deep ReLU networks transforms the non-convex training problem into a convex, group sparsity-regularized problem, enabling efficient, globally optimal training algorithms."
"We propose a proximal alternating direction method of multipliers for optimizing rank-based loss, showing convergence and demonstrating its effectiveness on synthetic and real datasets."
"We propose a method to learn compositional neural network policies in stochastic environments with formal probabilistic guarantees using reach-avoid supermartingales and compositional specifications, and demonstrate it on a Stochastic Nine Rooms environment."
"We propose a text-promptable surgical instrument segmentation model with a mixture-of-prompts mechanism and hard area reinforcement, achieving superior performance and generalization on surgical datasets."
"We introduce Deep Kernel Mixture Point Processes (DKMPP), a deep spatio-temporal point process model with multimodal covariates and a flexible deep kernel, trained via scalable denoising score matching, which outperforms baselines."
"We propose MapVR, a differentiable rasterization-based framework for high-definition map vectorization that improves accuracy and enables sensitive evaluation with no extra inference cost."
STDN improves video domain generalization by enhancing spatial and temporal diversity in learned features to reduce reliance on domain-specific cues.
"We introduce the Multi-order Fractional Fourier Convolution (MFRFC) operator, leveraging the Fractional Fourier Transform to unify and enhance spatial-frequency analysis in deep learning-based computer vision, achieving superior performance across multiple tasks."
"EPISODE++ is the first algorithm to achieve linear client and communication efficiency under client subsampling and unbounded smoothness in federated learning, and it outperforms baselines on federated RNN text classification, though gradient clipping does not eliminate exploding gradients."
We show that increasing width and using sparse initialization in MLPs improves sample efficiency in sparse feature learning tasks and can outperform random forests on tabular data by mitigating computational-statistical gaps.
"Diffusion-SS3D improves semi-supervised 3D object detection by using a diffusion model to denoise corrupted object size and class labels, enhancing pseudo-label quality and outperforming existing methods on ScanNet and SUN RGB-D."
"GraphAdapter improves adapter-based tuning of vision-language models by modeling dual-modality class relationships via a dual knowledge graph, outperforming previous methods on 11 benchmarks."
"We introduce STEVE-1, an instruction-tuned Minecraft agent trained via self-supervised behavioral cloning and hindsight relabeling using pretrained VPT and MineCLIP models, achieving strong open-ended instruction following with minimal human annotation and low compute cost."
"We propose training methods for neural min-sum LDPC decoders that eliminate the error-floor effect using ensemble networks, a block-wise training schedule, and weighted check node penalties, achieving superior error-floor performance without extra hardware."
"We propose a voting-theory-based evaluation framework for neuron interpretation methods and find that rankings of salient neurons are highly consistent across methods, with probeless ranking being most reliable and sensitivity concentrated in the last layer."
"We establish a nearly tight information-computation gap for learning general halfspaces with random classification noise under the Gaussian distribution, showing a sample complexity of $\tilde{\Theta}(d/\epsilon)$ but an $\Omega(d^{1/2}/(\max(p, \epsilon))^2)$ SQ lower bound, revealing an inherent quadratic inefficiency for efficient algorithms."
ALEXP improves model selection regret in linear bandits to logarithmic dependence on the number of models by emulating full-information feedback with a favorable bias-variance trade-off and a novel time-uniform Lasso analysis.
"We propose a 3D diffusion model with a 3D autoencoder that generates high-quality 3D assets from 2D images or monocular videos, outperforming state-of-the-art methods on diverse benchmarks."
"We show that myopic, confidence-based algorithms in multi-armed bandits fail to explore and thus underperform greedy, and provide positive results and theoretical justification for the necessity of exploration."
"InCA is a parameter-efficient transfer learning framework that attaches lightweight cross-attention adapters to intermediate model activations, enabling efficient parallel adaptation of large vision transformers with minimal memory use and strong performance."
"We propose a generalization bound based on the complexity of the learning trajectory and the bias-diversity ratio of the training set, showing that trajectory information effectively tracks generalization error during training."
"HPTR is a hierarchical Transformer framework with K-nearest neighbor attention and relative pose encoding that achieves state-of-the-art motion prediction performance with real-time, on-board scalability."
"We propose EOCP, an algorithm for the multi-armed bandit problem that achieves optimal regret and identifies the best arm in logarithmic time for both fixed and adaptive stopping, and show it is sample optimal or near-optimal while outperforming UCB by avoiding over-exploration."
"BiMatting is a binarized video matting model that addresses representation degradation and redundant computation, achieving substantial gains in efficiency while maintaining visual quality comparable to full-precision models."
"Toolformer is a language model trained in a self-supervised manner to use external tools via APIs, improving zero-shot performance on diverse tasks without losing core language modeling abilities."
"We propose a two-phase fMRI framework that pre-trains with a Double-contrastive Mask Auto-encoder and fine-tunes via image auto-encoder guidance to enable a latent diffusion model to reconstruct visual stimuli from brain activity, outperforming prior methods by 39.34% in semantic classification accuracy."
"We propose GLASS, a GAN-based Data Reconstruction Attack against Split Inference that outperforms existing methods by leveraging public data via StyleGAN and remains effective against seven defense mechanisms."
We introduce new surrogate structured prediction losses with proven H-consistency bounds and efficient minimization algorithms.
"The authors show that the Neural Tangent Kernel (NTK) of a gated ReLU network with fixed mask weights equals its Multiple Kernel Learning (MKL) kernel on training data, and that optimizing mask weights via iterative reweighting yields the optimal MKL kernel equivalent to the exact convex reformulation, with prediction error analyzed via group lasso consistency."
"Mutate Everything is a parallel deep learning algorithm that predicts the thermodynamic stability effects of all single and higher-order mutations in a protein in a single forward pass, achieving state-of-the-art performance without being specifically trained for stability prediction."
We propose differentially private offline RL algorithms with strong learning guarantees in tabular and linear MDP settings that achieve utility close to non-private methods on medium-sized datasets.
"We propose MoVie, a simple test-time adaptation method that significantly improves view generalization of model-based visual RL policies across 18 tasks without training modifications or reward signals."
"CELL-E 2 is a bidirectional transformer that generates protein subcellular localization images from amino acid sequences and vice versa, enabling de novo protein design and NLS creation."
"We propose a regularization-free, over-parameterized algorithm for high-dimensional SVMs using Nesterov's smoothing, showing near-oracle convergence and demonstrating its advantages over explicit regularization."
"We introduce 3D-LLMs that process 3D point cloud inputs and outperform 2D VLMs on diverse 3D tasks, including ScanQA, 3D captioning, and 3D-assisted dialogue."
"Decoupled mixup (DM) is an efficient mixup objective that leverages hard mixed samples to enhance feature discrimination without extra computation, enabling static mixup to match or surpass dynamic methods."
We propose a parametric continuous convolution network for diffusion MRI angular super-resolution that achieves competitive performance with fewer parameters and generalizes to clinical downstream analyses.
"CLEAM is a new framework that significantly reduces measurement errors in fairness evaluation of generative models, revealing substantial biases in text-to-image generators and GANs."
"Off-manifold robustness explains why robust computer vision models have perceptually-aligned gradients and rudimentary generative abilities, and reveals three robustness regimes affecting both alignment and accuracy."
"We show that pre-training loss alone does not guarantee good downstream performance, but the existence of an ""anchor vector"" in the representation space, combined with task properties, ensures performance transfer."
ContiFormer integrates continuous-time dynamics and attention to improve modeling and prediction of irregular time series.
"We propose a conformal prediction framework that generates contiguous, minimal-size prediction sets with coverage guarantees for ordinal classification, outperforming baselines by 4% in Accuracy@K and reducing set size by 8%."
"NICE introduces adaptive noise modulation and consistency regularization in the discriminator to prevent overfitting and stabilize GAN training with limited data, achieving state-of-the-art results on multiple image datasets."
"We propose efficient stochastic first-order methods for Inverse Reinforcement Learning under the average-reward criterion, achieving new theoretical complexity bounds and validating them on benchmark tasks."
"We propose a model-free, ensemble-based exploration strategy for reinforcement learning that approximates instance-specific sample lower bounds and outperforms existing methods in policy discovery speed."
"We propose A²CiD², an asynchronous randomized gossip-based optimization algorithm with continuous local momentum that reduces communication costs and idle time in decentralized distributed deep learning, outperforming standard asynchronous methods especially in poorly connected networks."
"We present a guaranteed A*-based method for finding counterfactually optimal action sequences in continuous-state, discrete-action sequential decision problems, and demonstrate its efficiency on clinical data."
"We present ODPP, an unsupervised reinforcement learning algorithm that unifies diversity and coverage objectives using Determinantal Point Processes to discover options with superior performance on Mujoco and Atari tasks."
"We propose model Shapley, a novel, equitable valuation method for black-box ML models in large-scale AI marketplaces, validated empirically on real-world datasets."
"We present Q-GP-UCB, the first quantum Bayesian optimization algorithm achieving polylogarithmic regret, surpassing classical $\Omega(\sqrt{T})$ lower bounds."
"We propose a post-training quantization method for diffusion models that disentangles and corrects quantization noise and adapts the variance schedule and bitwidth per step, achieving high sample quality with minimal FID increase and significant bit operation savings."
"We propose a cross-category end-to-end method for reconstructing multiple articulated objects from a single RGBD image using part-level representations, incorporating kinematics-aware fusion, anisotropic scale normalization, and a balancing strategy to improve detection and reconstruction accuracy."
"Projected gradient flow on two-layer neural networks with $n=O(d^{3.1})$ samples achieves a non-kernel-achievable error in polynomial time, separating unmodified gradient descent from kernel methods."
"We propose a new classification loss that jointly maximizes model accuracy and minimizes expert review volume by optimizing the confidence operating characteristic curve, improving accuracy and reducing human oversight in critical applications."
"We propose a unified Frank-Wolfe-based method for maximizing continuous DR-submodular functions under various oracle and feedback settings, achieving improved results or first regret bounds in several cases with reduced computational cost."
"We prove optimal contraction rates for intrinsic and extrinsic Gaussian processes on Riemannian manifolds and show empirically that intrinsic processes can perform better, indicating a need for finer analysis to distinguish data-efficiency in low-data geometric settings."
"We propose Rank-based PruninG (RPG), a structured pruning method that maintains high-rank sparse weight topology via adversarial rank optimization, achieving 1.13% higher top-1 accuracy than state-of-the-art on ImageNet ResNet-50 at 98% sparsity."
"We analyze how functional local differential privacy, particularly Gaussian LDP, balances statistical utility and privacy via minimax risk and show it outperforms ε-LDP in univariate and nonparametric estimation."
"We propose a diffusion-based video inpainting method that separates the human hand from the environment in egocentric videos, improving occlusion handling and enabling better transfer to robotics tasks."
"CPTPP is a prompt-enhanced framework that aligns graph contrastive pre-training with downstream recommendation tasks via personalized user prompts and prompt tuning, improving user embedding diversity and outperforming state-of-the-art baselines."
The Correlation Aware Pruner (CAP) enables high sparsity unstructured pruning of modern vision models with minimal accuracy loss by addressing complex weight correlations and incorporating efficient fine-tuning.
ReSync is a Riemannian subgradient algorithm that achieves local linear convergence to ground-truth rotations for the robust rotation synchronization problem under random corruption.
"We propose a learning-based non-rigid shape registration framework that uses deep functional maps to guide mesh-to-pointcloud alignment with dynamically filtered correspondences and an orientation regressor, achieving state-of-the-art results on challenging benchmarks."
"LRD-GNNs improve graph representation learning by using low-rank decomposition on node attributes and tensors, avoiding propagation across classes and enabling robust capture of local and long-range relationships without labels."
"LayoutGPT uses LLMs to generate layouts from text, outperforming text-to-image models and approaching human performance in spatial and numerical layout tasks across 2D and 3D domains."
We propose an early stopping criterion that accelerates text-to-image personalization up to 8× without quality loss by leveraging rapid concept learning.
"We propose estimate-verify-release (EVR), a differential privacy paradigm that converts estimated privacy parameters into strict guarantees via Monte Carlo-based verification and accounting, improving utility-privacy tradeoffs in machine learning."
"We propose a transformer-encoder and locality-aware INR-decoder framework that enables fine-grained, locality-aware modulation for generalizable implicit neural representations, outperforming prior methods and improving image generation."
"We propose Multi-Prompt Alignment (MPA), a computationally efficient multi-source unsupervised domain adaptation method that uses domain-specific prompts and prompt alignment via auto-encoding to achieve state-of-the-art results on DomainNet."
"We propose Past Action Leakage Regularization (PALR), a kernel-based regularization method that mitigates past action leakage in observation histories for imitation learning, significantly improving imitation performance when past actions contaminate observations."
"We prove SQ lower bounds for Non-Gaussian Component Analysis under moment-matching conditions alone, showing that the chi-squared condition is unnecessary."
"Res-Tuning is a parameter-efficient tuning framework that unbinds lightweight tuners from the backbone, enabling flexible design, memory efficiency, and superior performance across discriminative and generative tasks."
"We present fault-tolerant, load-balanced algorithms for decentralized, geodistributed inference and fine-tuning of large language models over consumer-grade networks, enabling efficient collaborative use of idle compute resources."
We present a graph factorization model using two nonnegative node vectors that exactly represents graphs with low arboricity and enables interpretable link prediction and community detection.
"SFGC is a structure-free graph condensation method that encodes graph topology into node attributes to effectively distill large graphs into small graph-free node sets, outperforming existing methods across condensation ratios."
"We show that a shared image-text representation space can be created without training by using pre-trained single-domain encoders and a small set of image-text pairs, enabling rapid updates and interpretable dimensions, while achieving strong zero-shot performance and raising questions about data efficiency and retrieval in multimodal models."
"BTS-RED adaptively allocates replications to experimental conditions based on noise variance to achieve asymptotic no-regret in risk-averse, heteroscedastic experimental design."
We analyze the error and convergence effects of pseudo labeling in graph learning and propose a confidence- and consistency-based cautious pseudo labeling method that outperforms existing strategies on link prediction and node classification.
"We derive a tighter regret bound of $\mathcal{O}\left(\sqrt{\alpha T (1 + \ln(K/\alpha))}\right)$ for online learning with strongly observable undirected feedback graphs, matching known bounds for experts and bandits and improving intermediate cases, using a q-Tsallis FTRL algorithm and a new variance bound, and complement it with an improved $\Omega\left(\sqrt{\alpha T \ln K / \ln \alpha}\right)$ lower bound via a multitask reduction."
"We propose a framework with localization, contrastive feature extraction, and pixel alignment modules to improve whole-body pose and shape estimation robustness by addressing bounding box quality issues."
"We introduce OSPG, an optimal stopping policy gradient algorithm that uses RNNs to efficiently solve non-Markovian optimal stopping problems without recursion or Monte Carlo rollouts by optimizing value functions via inference on a novel Bayesian network representation."
"We propose HI-Diff, a hierarchical latent diffusion model with regression-based deblurring that achieves efficient and accurate image deblurring by generating priors in a compact latent space and fusing them at multiple scales."
"We introduce the Q-exponential process, a probabilistic model corresponding to L_q regularization of functions, providing an explicit prior for Bayesian function estimation with sharper sparsity control than Gaussian processes."
"We derive martingale-based calibration for pretrained diffusion probabilistic models to reduce score matching loss and increase likelihood, and validate it empirically."
"We address the problem of adapting a pre-trained model to a target domain with partial label coverage, proposing methods that preserve classification accuracy for missing classes while improving overall performance."
DiViNet reconstructs 3D surfaces from as few as three sparse RGB images using learned neural surface priors as 3D Gaussian templates.
"SAMA improves scalable meta learning by combining implicit differentiation with efficient distributed training, achieving higher throughput, lower memory use, and state-of-the-art results in language and vision tasks."
"POT improves Graph Contrastive Learning by regularizing node embeddings via a theoretically derived ""node compactness"" metric that addresses training imbalance."
"We introduce binary radiance fields, a compact radiance field representation using ±1 features and a 2D-3D hybrid grid that achieves high reconstruction quality with only 0.5 MB storage."
L2RCLIP is a language-driven ordinal classification method that leverages prompt tuning and a cross-modal ordinal pairwise loss to better capture ordering relations using pre-trained vision-language models.
"We propose Individualized DP-SGD, a variant of DP-SGD that assigns different privacy budgets to users to better balance privacy and utility."
"We provide the first theoretical analysis of kernel-based dataset distillation for KRR, showing that a small distilled set—linear in the RFF dimension or near-linear in the effective degrees of freedom—exists and achieves bounded excess risk dependent on the regularization parameter."
"We introduce a stop-gradient-based method that improves stability and performance of positive-only self-supervised learning methods by implicitly incorporating contrastive ideas, enabling robust training with small batches and without predictors."
"We introduce a Bayesian perspective on training data attribution for deep models, showing that TDA estimates are often unreliable due to noise from model initialization and SGD, and recommend using TDA only when influence is robust to such noise."
"We quantify two types of discrimination in ML—aleatoric (inherent in data) and epistemic (from model development)—and show that while current fairness methods reduce epistemic discrimination on standard datasets, they struggle with aleatoric discrimination in data with missing values."
"We propose a hybrid Block-State Transformer combining state space models and block transformers that improves language modeling perplexity, generalizes to longer sequences, and achieves over tenfold speedup with model parallelization."
"Mirror Diffusion Models enable tractable diffusion-based generation on constrained sets by mapping to a Euclidean space via a mirror map, outperforming existing methods and enabling new applications like constrained-set watermarking."
"FreeMask leverages generative models to create well-aligned synthetic image-mask pairs for semantic segmentation, improving model performance and reducing annotation effort, especially when combined with real data and enhanced by robust filtering and adaptive sampling."
Temporal-difference learning converges when the interplay between changing optimization objectives and algorithmic forces in the linear TD setting with quadratic loss is favorable.
"CosNet generalizes spectral kernel mapping to the complex domain and effectively captures long-range and periodic relations in time-sequential data, outperforming existing kernel and complex-valued neural network methods."
DiffUTE is a self-supervised diffusion model that enables realistic multilingual text replacement and editing in images using glyph and position information.
We introduce a differential privacy-based incentive mechanism that values and rewards participants in collaborative machine learning while preserving privacy and maintaining incentive compatibility.
"We propose the first framework to solve linear inverse problems using latent diffusion models, theoretically analyze its sample recovery, and demonstrate superior performance over prior pixel-space methods on multiple imaging tasks."
"Diffusion models can be generalized to use arbitrary deterministic image degradations, challenging the necessity of stochastic noise for generation and enabling broader classes of generative models."
"DreamWaltz is a framework that generates and animates complex 3D avatars from text and body priors using 3D-consistent occlusion-aware SDS and pose-conditioned supervision, enabling artifact-free avatar creation and pose-based animation without retraining."
"We show that proving convergence rates for first-order ODE optimization models reduces to verifying positive semidefiniteness of certain Hilbert-Schmidt integral operators, enabling new convergence rate results and revealing a correspondence between function value and gradient norm minimization."
We prove existence of Nash Equilibria for λ-regularized Graphon Mean-Field Games under weak conditions and propose a provably efficient discrete-time algorithm for weakly monotone GMFGs with analyzed action-value estimation.
"We propose a neural network layer that enforces low-frequency feature representations to better capture the irregular target functions in tabular data, improving performance and convergence over standard fully connected layers."
"GAIA detects out-of-distribution data by analyzing gradient-based attribution abnormalities, significantly reducing false positive rates on CIFAR10 and CIFAR100."
"We propose a one-bit quantization method for Transformer machine translation models that maintains float-level quality with 16× size reduction using weight binarization and architectural improvements, and show robust scaling on large datasets."
"We propose SOBOW, a computationally efficient single-loop online bilevel optimizer with window averaging that achieves sublinear local regret under function variations and streaming data by estimating hypergradients with a moving average and novel error control."
SEM introduces a self-weighted multi-view contrastive learning framework with reconstruction regularization to adaptively strengthen reliable view pairs and mitigate representation degeneration in multi-view data.
"We present a unified framework connecting discrete-time interacting particle systems to their mean-field limits, bridging the gap between theoretical analysis and practical implementation."
LLMs show limited autonomous planning ability but can improve AI planner performance when used as heuristic sources with external verification.
"The paper introduces Explain Any Concept (EAC), a concept-based XAI method that leverages the Segment Anything Model (SAM) to provide flexible, efficient explanations of deep neural network decisions using automatically extracted concepts from images."
"We propose and efficiently learn a variational inference method for latent SDEs evolving on the unit sphere using a geometric Euler-Maruyama scheme, achieving competitive performance on time series tasks."
"We present TA-Bench, a standardized benchmark evaluating 30+ transfer-based adversarial attack methods on 10 ImageNet models to provide fair, comprehensive, and practical comparisons and future evaluation guidelines."
"As deep neural networks have become more accurate at object recognition, they have paradoxically become worse at predicting inferotemporal cortex responses, but harmonizing their representations with human data restores predictive accuracy."
"We address causal imitation learning with known context-specific independence, showing NP-hardness of feasibility, providing a necessary graphical criterion (and sufficiency under a structural assumption), and proposing a sound algorithm that leverages CSI and data."
We prove an O(T²/m) uniform forgetting bound for linear continual learning with cyclic task orderings by characterizing the union of numerical ranges of projection products as a sinusoidal spiral.
We propose a risk-averse deep reinforcement learning framework that achieves robust and safe performance across perturbed environments using a model-free approach without minimax optimization.
"STORM, a stochastic Transformer-based world model combining variational autoencoder properties, achieves state-of-the-art Atari 100k performance with efficient training using limited real-world interaction."
We propose randomized linear classifiers that achieve probabilistic invariance and universal approximation with fewer resources than deterministic invariant neural networks.
"We propose a bilevel framework with a weighted AUC loss that integrates variable cost distributions to achieve both cost-sensitive and class-distribution robust learning, outperforming existing methods."
"We propose a Distributionally Robust Optimization framework that trains an ensemble of diverse, complementary sparse sub-networks to improve model calibration without sacrificing accuracy or increasing inference cost."
MotionGPT is a unified pre-trained model that treats human motion as a language and achieves state-of-the-art results on multiple motion tasks by jointly modeling text and motion using discrete tokenization and prompt-based fine-tuning.
"Meta generative regularization (MGR) improves generative data augmentation by using meta-learned synthetic samples to regularize feature extractors, boosting test accuracy by up to 7 percentage points on small datasets."
"Joint optimization of ensemble loss leads to degenerate, non-generalizing pseudo-diversity among base learners."
"Anc-VI accelerates Value Iteration, achieving an O(1/k) error rate via an anchoring mechanism and matching lower bounds, outperforming standard VI for near-unity discount factors."
"We propose an algorithm for contextual bandits and imitation learning that achieves regret $O(\min\{\sqrt{T}, d/\Delta\})$ and query complexity $O(\min\{T, d^2/\Delta^2\})$ using noisy preference feedback, matching standard contextual bandit performance without reward observations and enabling policy improvement beyond suboptimal experts."
"LLM-MCTS outperforms LLM-only policies and model-free MCTS on complex tasks by using the LLM as a compact world model within a search framework, with effectiveness predicted by the minimum description length difference between the world model and policy."
AIME enables zero-shot imitation by learning a world model from past experience and inferring expert actions from observation-only demonstrations using probabilistic inference.
Penalizing repetitions in training data fundamentally reduces neural text degeneration across model sizes and tuning methods.
"We propose a differentiable, end-to-end cross-modality registration framework that learns a shared latent space between 2D images and 3D point clouds using a triplet network with voxel, point, and pixel branches, and trains with a probabilistic PnP solver to improve 2D-3D correspondence and pose estimation."
"We prove that the superlevel sets of policy optimization objectives in reinforcement learning are connected and equiconnected, and apply this to establish a novel minimax theorem for robust reinforcement learning under adversarial reward attacks."
"Training models on anonymous cluster centers via look-alike clustering can act as a regularization that improves generalization in high-dimensional regimes, as shown both theoretically and empirically."
We propose a section-based contrastive pre-training and masked section training approach that improves ICD code prediction from clinical notes with limited and variable data.
Low-tensor-rank dynamics naturally govern the evolution of recurrent synaptic connectivity during learning in both biological and artificial neural networks.
"We introduce DIPPER, a paraphrase generation model that effectively evades AI-generated text detectors, and propose a defense that retrieves similar API-generated sequences to detect paraphrased content with high accuracy and low false positives."
"We present DeWave, a framework that enables EEG-to-text translation without word-level markers by using a discrete encoding sequence aligned with pre-trained language models, outperforming prior baselines and enabling full EEG period translation."
We present a scalable online learning algorithm for recurrent neural networks that matches offline backpropagation's performance on long-range dependency tasks by doubling inference resources and using independent recurrent modules.
"We present a dynamic programming framework that efficiently optimizes separable objectives and constraints in decision trees, outperforming general-purpose solvers in scalability across five application domains."
"Gradient descent on the softmax-attention parameters converges to a max-margin token selector that separates locally-optimal from non-optimal tokens, and under certain conditions, jointly optimizing parameters yields hard-margin SVM solutions with interdependent regularization paths."
We propose efficient algorithms with guarantees for estimating optimal data source mixtures for a target distribution and for learning target-specific models without solving empirical risk minimization for each domain individually.
"Rank-DETR improves DETR-based object detectors by aligning ranking with localization accuracy via rank-oriented architecture and loss designs, yielding higher AP on COCO with multiple backbones."
We propose a multi-view clustering method using orthogonal non-negative tensor factorization with tensor Schatten p-norm regularization that leverages both within-view and between-view information for improved clustering performance.
"We propose a diffusion model for dynamic forecasting that encodes temporal dynamics in the diffusion process, enabling efficient, flexible, and competitive multi-step forecasting across diverse dynamical systems."
"FAIR is a full-atom iterative refinement framework that co-designs protein pocket sequences and 3D structures, improving pocket-ligand binding design by over 10% in accuracy and structure metrics compared to existing methods."
"We present a geometric divide-and-conquer adaptation of the Hungarian method that computes minimum-cost bipartite matchings in expected time $\tilde{O}(n^{7/4}\log\Delta)$ for stochastic 2D point sets, achieving the first sub-quadratic time for such inputs."
We propose projection-free primal-dual methods with first-order oracles for nonconvex-concave saddle point problems and establish convergence rates under standard and strong concavity assumptions.
We propose a method that uses ExCeeD's stability metric with a constant rejection threshold to provide strong theoretical guarantees and improved anomaly detection performance without labeled data.
"CMTA improves multi-task reinforcement learning by constraining and combining modules at a finer granularity via contrastive learning and temporal attention, outperforming baselines on Meta-World."
"RNC is a regression framework that learns continuous, rank-ordered representations by contrasting samples based on target rankings, achieving state-of-the-art performance and improved robustness and generalization across diverse regression tasks."
"We prove that under certain assumptions, ensemble methods reduce selective classification risk compared to individual models for a range of coverages, explaining their improvement primarily through handling top-ambiguity samples, and validate these results experimentally on vision and NLP tasks."
"We propose the lattice tensor, a data structure that enables standard machine learning operations on non-Cartesian data and demonstrate its implementation in a software library."
We propose a neural operator that takes triangular mesh geometries as input and rapidly approximates PDE solutions without retraining or fixed parameterization.
"We propose a poison-only backdoor attack for visual object tracking by injecting a trigger into negative training examples, significantly degrading tracker performance on poisoned data without affecting benign data."
We propose a zero-shot image purification framework that uses a linear transformation and a diffusion model to defend black-box models against backdoor attacks without model or prior knowledge.
"Generative data augmentation improves learning guarantees for small datasets in non-i.i.d. settings, especially when the divergence between the learned and true distributions decays faster than max(log(m)β_m, 1/√m)."
The paper proposes and validates a method to reduce correlation disparity related to protected attributes in CCA by minimizing correlation disparity error while preserving CCA accuracy.
PolyDiffuse introduces a Guided Set Diffusion Model that enables structured reconstruction of polygonal shapes from visual sensor data by resolving permutation ambiguity and conditioning the generation process on sensor inputs.
"ViTs trained on first-person visual data from newborn chicks learn view-invariant object recognition with no more data than the chicks, showing they are not more data-hungry than biological learners."
"Transformers exhibit sporadic reasoning errors on long-range tasks due to attention glitches, which may explain closed-domain hallucinations in large language models."
"We propose and analyze a doubly pessimistic model-based policy optimization algorithm for distributionally robust offline reinforcement learning that achieves provably efficient learning under partial coverage assumptions using various model classes, including novel function approximation settings."
"VisionLLM is an LLM-based framework that treats images as a language and enables open-ended, instruction-driven vision tasks, achieving strong results including over 60% mAP on COCO with a generalist model."
"RevColV2 retains the decoder in masked image modeling, preserving disentangled low-level and semantic features to improve performance across image classification, segmentation, and detection tasks."
"We replace multiplications in neural networks with bit-level integer addition and fully piecewise affine operations, enabling accurate training of transformers without any multiplications in forward, backward, or update steps."
"We systematically evaluate LLMs' planning abilities using a cognitive science-inspired protocol and find that, despite apparent competence in simple tasks, LLMs fail systematically in complex planning due to lack of underlying relational understanding."
"AdaptSSR introduces an augmentation-adaptive self-supervised ranking pretext task that better pre-trains discriminative user models from noisy, diverse behavioral data without requiring strict semantic consistency between augmented views."
We propose an active-learning method to efficiently learn an interpretable probabilistic model of black-box sequential decision-making agents with theoretical guarantees and empirical sample efficiency.
"We propose SGDD, a graph dataset distillation method that preserves original graph structure information, reducing Laplacian Energy Distribution shifts and achieving state-of-the-art performance with significant scale reduction across multiple datasets."
"NaViT enables Vision Transformers to process images at native resolutions with variable aspect ratios, improving training efficiency and model robustness while allowing flexible inference-time performance trade-offs."
"Chanakya is a learned execution framework for real-time perception that adaptively balances accuracy and latency using intrinsic and extrinsic context, outperforming prior methods on both servers and edge devices."
"MeZO enables memory-efficient fine-tuning of very large language models using only two forward passes per update, achieving comparable performance to backpropagation with drastically reduced memory and compute."
"Humans show structured errors in estimating the number of clusters in probability distributions during uncertainty learning, best explained by resource-limited model expansion."
Diffusion Hyperfeatures consolidate multi-scale and multi-timestep diffusion model features into per-pixel descriptors that improve semantic keypoint correspondence on real and synthetic images.
We propose a model-based RL algorithm using ODEs with Gaussian Processes and an adaptive measurement selection strategy to achieve sublinear regret in continuous-time reinforcement learning.
"We introduce Multi-learner Nonparametric Teaching (MINT), a framework that accelerates teaching multiple learners of scalar-valued target models in a vector-valued RKHS and demonstrates faster teaching than repeated single-learner instruction, especially with learner communication."
We introduce a surrogate-based method leveraging the Fisher information spectrum and activation output distribution to discover improved sigmoidal activation functions that outperform rectifiers in deep learning tasks.
"DropPos is a simple self-supervised pretext task that improves Vision Transformers' location awareness by reconstructing dropped positional embeddings from visual appearance, outperforming supervised pretraining and competing with state-of-the-art methods on downstream benchmarks."
"NRES, a low-variance online evolution strategy, converges faster than existing AD and ES methods in learning dynamical systems, meta-training optimizers, and reinforcement learning."
"We propose a local regression method to estimate the Riemannian metric tensor from noisy similarity measures, with theoretical convergence analysis and applications to taxi trip time and MNIST data."
"Uni-ControlNet is a unified framework that enables simultaneous, composable fine-tuning of frozen text-to-image diffusion models with multiple local and global control signals using only two additional adapters, improving controllability and generation quality without extensive retraining."
"We propose a bridge-based decomposition method that reduces large-scale MTP₂ Gaussian graphical model learning to smaller sub-problems and explicit bridge solutions, significantly improving computational efficiency and performance."
"We introduce the Continually Changing Corruptions benchmark and show that most state-of-the-art test-time adaptation methods eventually collapse, while a simple model-resetting baseline performs at least as well."
"We propose PlanCP, a diffusion-based trajectory prediction model for robotics that incorporates a quantile loss during training and uses conformal prediction at test time to provide guaranteed-coverage uncertainty sets for model-based planning and offline reinforcement learning."
We propose two efficient algorithms for generalized linear bandits with heavy-tailed rewards that achieve an almost optimal regret of $\widetilde{O}(dT^{1/(1+\epsilon)})$.
"We propose Adapter Re-Composing (ARC), a parameter-sharing adapter method that reuses and compresses adaptation parameters for efficient pre-trained model fine-tuning in vision transformers."
"We propose Rand-Proj-Spatial, a sparsification method using random subspace projection that leverages client correlation to outperform Rand-k-Spatial in distributed vector mean estimation for optimization and federated learning."
"We introduce score-based data assimilation that decomposes long trajectory scores into short segment scores and uses a generative score model for non-autoregressive trajectory inference, decoupling the observation model for zero-shot scenarios."
"We analyze the geometry of diffusion model latent spaces and demonstrate image editing by moving in the local latent basis at specific timesteps, revealing coarse-to-fine generation and effects of dataset complexity and text prompts."
"We show that batch-normalized deep networks can be attacked using intermediate latents and angular loss without labels, indicating batch norm creates a security vulnerability relevant to transformers."
"Functional Diffusion Processes generalize score-based diffusion to infinite-dimensional function spaces, enabling high-quality image generation with simple MLPs and far fewer parameters via a novel mathematical framework."
"We introduce LogSpecT and its practical variant rLogSpecT, which are always feasible and recoverable, and outperform existing methods for graph learning from stationary signals."
"We present an efficient gradient-based method to automatically optimize interpretable hard text prompts for generative models, enabling easy concept discovery and bypassing of token-level content filters."
"Meta-variational dropout (MetaVD) improves federated learning on non-IID data by predicting client-dependent dropout rates via a shared hypernetwork, enhancing model personalization, OOD robustness, and communication efficiency."
GCX is a graph neural network-based framework that enables efficient exploration and optimization of analog circuit designs across technology nodes using semi-supervised learning and specialized algorithms.
"We propose a hyperbolic space embedding method with hierarchical cosine margins for effective coarse-to-fine few-shot fine-grained recognition, achieving state-of-the-art results on five benchmarks."
"We propose UP-DP, an unsupervised prompt learning method that leverages joint vision-text features from BLIP-2 to improve data pre-selection, achieving up to 20% performance gains and demonstrating generalizable prompts across datasets."
We derive an anytime exploration-exploitation algorithm (Anytime-E2D) by re-parametrizing and solving the min-max DEC program for online regret minimization in structured bandits and linear reinforcement learning.
"We propose Distribution Normalization (DN), which approximates negative sample information at test time by using the mean representation of a batch, improving performance over dot product and existing augmentations for CLIP-based visual-language tasks."
IMFL is a cost-effective multi-fidelity learning framework that outperforms baselines with up to five times more human annotations on domain-specific language tasks by optimally combining human and LLM annotations.
"We propose a principal-agent game framework where budget-optimal, threshold contracts, equivalent to Neyman-Pearson testing, incentivize accurate outsourced machine learning, and can be constructed from small-scale data."
"We propose a learnable neural polarizer layer inserted into backdoored models to filter out trigger information while preserving benign data, efficiently defending against backdoor attacks with minimal clean data."
"The authors propose a statistical test to determine whether human expert predictions provide information beyond that available to algorithms, and demonstrate using hospital admissions data that physicians’ decisions for acute gastrointestinal bleeding include information not captured by standard screening tools, even when the algorithm is more accurate."
"MixTURE is the first multi-agent learning-from-demonstration framework that automatically learns effective inter-agent communication from human expert data, reducing human workload by ~44% and increasing usability by ~46% in complex multi-robot tasks."
"We show that training two-layer neural networks with ReLU or linear threshold activations is NP-hard in two dimensions, W[1]-hard with zero training error for four ReLUs, and FPT for convex maps when parameterized by input dimension and number of hidden neurons."
"We propose a method that automatically discovers a task graph from how-to videos to better recognize procedural keysteps in novel instructional videos, improving zero-shot keystep localization and representation learning beyond the state of the art."
"We show that independent latent variables with time-delayed causal relations can be identified in nonstationary settings without auxiliary variables, and introduce NCTRL, a framework that outperforms baselines by leveraging nonstationarity."
"We prove identifiability of latent causal variables and graphs in a general nonparametric setting using multiple environments from unknown interventions, up to unavoidable ambiguities, and show that causal influence strengths are preserved."
"SEEDS is a derivative-free, training-free SDE solver that achieves optimal quality diffusion sampling with 3–5× speedup by analytically computing linear and stochastic parts of solutions."
"We train a vision-language model to accept image regions as input by distilling commonsense knowledge from language models using automatically generated descriptions, resulting in more precise zero-shot reference-grounded reasoning."
"Lockdown mitigates backdoor attacks in federated learning by isolating client training subspaces, using randomness with pruning and recovery, and applying quorum consensus to purge malicious parameters, achieving superior defense performance with improved communication efficiency and reduced model complexity."
"AutoACER automatically identifies and mitigates reliance on spurious attributes in classification by estimating their causal effect on the label and applying regularization, improving classifier robustness to spurious correlations across datasets even with noisy causal effect estimates."
"Returning agents can exploit prediction mechanisms to strategically manipulate future matches, increasing their own benefit and inequality in school choice markets."
"We show that imposing a perfect perceptual-quality constraint in causal Gaussian Markov filtering forces the filter to sometimes ignore new observations, increasing mean squared error compared to the Kalman filter."
TeCoS-LVM is a computationally efficient spiking neural model that better captures realistic spike train dynamics and generalizes over time by adaptively modeling temporal dependencies without temporal filters.
"We propose a method that learns data-driven, natural language rules to guide effective human-AI collaboration, improving accuracy in object detection and question-answering tasks."
Label noise increases the Rashomon ratio by enabling diverse yet equally accurate classification patterns among models.
"We present constant-approximation algorithms for $k$-means and $k$-median clustering under distance-based privacy, achieving error depending only on the attacker's precision bound rather than the data space radius, and show empirical improvements over prior differentially private and baseline methods."
"Directional Stimulus Prompting improves black-box LLM outputs by using a small tunable model to generate instance-specific prompts that guide the LLM toward desired responses, significantly boosting performance on multiple tasks with minimal supervision."
"UP-NeRF enables pose-prior-free NeRF optimization on unconstrained image collections with varying illumination and transient occluders by introducing color-insensitive feature fields, an occluder-aware module, and robust pose estimation with transient-aware supervision."
We propose a Bayesian optimization framework with learned graph kernels that efficiently optimizes functions on large-scale graphs.
"We prove the existence of structured subnetworks in random convolutional networks that approximate any sufficiently smaller network, providing the first sub-exponential bound for structured pruning under the Strong Lottery Ticket Hypothesis."
"KOSMOS-1 is a multimodal large language model trained on web-scale interleaved text and image data, demonstrating strong zero- and few-shot performance across language, perception, and reasoning tasks, and enabling cross-modal transfer without fine-tuning."
"We propose N²-GNN, a practically efficient and highly expressive graph neural network based on (k, t)-FWL+ that outperforms state-of-the-art methods on ZINC-Subset and BREC."
"TWIST improves SpeechLMs by warm-starting from pretrained textual language models and demonstrates that both model and dataset scale are key to performance, releasing larger models and new spoken evaluation benchmarks."
"We show that the stationary distribution of offline SGD exhibits approximate power-law tails, with the approximation improving as the training dataset size increases and the empirical data distribution converges to the true distribution in Wasserstein distance."
"We propose IOAR, a monocular 3D reconstruction model that classifies outer-surface, inner-surface, and surface voxels separately and uses separate occupancy and TSDF branches to improve mesh accuracy on ScanNet, ICL-NUIM, and TUM-RGBD."
"LoCoOp improves few-shot out-of-distribution detection by using CLIP local features to regularize prompt learning, outperforming existing methods even with one label per class."
MAFOCOPS is a first-order constrained optimization method for multi-agent reinforcement learning that simultaneously improves performance and ensures safety constraints.
"DAMEX improves universal object detection by training dataset-specific experts with dynamic token routing, outperforming previous methods on diverse datasets."
"A machine learning-based control variate can achieve the minimax optimal convergence rate for Monte Carlo estimation of Sobolev functions under sufficient smoothness, but fails to improve convergence when rare and extreme events are present."
"Double Gumbel Q-Learning addresses heteroscedastic Gumbel noise in DNN-based Q-Learning, outperforming several baselines across 33 control tasks with a well-chosen pessimism hyperparameter."
"We evaluate whether large language models' causal and moral judgments in text-based scenarios align with human intuitions using a curated dataset annotated with key influencing factors, finding improved overall alignment with recent models but differing weighting of factors."
"We present a method to integrate frozen text-only LLMs with image encoders and decoders via learned embedding-space mappings, enabling multimodal dialogue, image retrieval, and image generation from interleaved text and image inputs, with improved performance on complex language conditioning and a learned decision module to choose between retrieval and generation."
We establish asymptotic identifiability of causal structures and latent variables in nonlinear latent hierarchical models with general nonlinearities and relaxed structural assumptions.
"We introduce Normalized Negative Loss Functions (NNLFs) as robust passive loss alternatives to MAE in Active Passive Loss, forming the Active Negative Loss framework, which outperforms state-of-the-art methods."
MEVI integrates deep generative models into vector indexing to improve retrieval performance with serving efficiency comparable to dense retrieval.
"We propose DNIK, a method that represents novel 3D shapes as compositions of known part concepts with constraints and part relation encoding to improve 3D novel class discovery."
Intensity Profile Projection is a framework for learning coherent node trajectories from continuous-time dynamic networks with theoretical error guarantees and adaptive smoothing.
We develop sign equivariant neural network architectures with proven expressiveness for graph tasks where sign invariance is insufficient.
"We derive PAC-Bayes-based generalization bounds for inductive conformal prediction that enable simultaneous optimization of prediction set coverage and efficiency using the full calibration dataset, outperforming Hoeffding-based baselines, especially with limited data."
We propose a hyperbolic large margin classifier using horospherical boundaries that yields a geodesically convex optimization problem solvable by Riemannian gradient descent and achieves state-of-the-art performance.
We propose computationally efficient approximations to optimal non-parametric stochastic bandit algorithms that maintain asymptotic regret guarantees.
"Multimodal transformer representations enable encoding models to transfer and align brain responses between language and vision, revealing shared semantic concept dimensions and improved cross-modal prediction."
"Class-Aware Pseudo-Labeling (CAP) improves semi-supervised multi-label learning by using class-aware thresholds and class distribution estimates to more accurately assign pseudo-labels, reducing false positives and negatives and improving performance even with few labeled examples."
We introduce a Kullback-Leibler-based instability measure for statistical parameters that quantifies sensitivity to distributional shifts and improves transfer learning under changing distributions.
GenRet is a discrete auto-encoding method for learning semantic document identifiers that enables generative retrieval models to achieve state-of-the-art performance on NQ320K and improved generalization on other benchmarks.
"GraphACL is a simple, augmentation-free graph contrastive learning method that outperforms state-of-the-art approaches on both homophilic and heterophilic graphs by leveraging asymmetric neighbor views to capture local and monophily information."
"We propose SUCBVI and SRF-UCRL for safe reinforcement learning with step-wise violation constraints and without guaranteed safe actions, achieving near-optimal regret and violation bounds and demonstrating superior safety in experiments."
"We extend FlashAttention to efficiently support dynamic sparse attention patterns, achieving up to 3.3× faster training on 16k-token sequences without loss in perplexity."
"We analyze adversarial label perturbations in online learning, showing that a critical attack strength causes a discontinuous drop in learner accuracy, and that greedy attacks are highly effective, especially with small batch sizes."
"CoBEVFlow is an asynchrony-robust collaborative perception system that uses BEV flow to realign asynchronous perceptual features and mitigate information mismatch without generating new features, outperforming baselines on both synthetic and real datasets."
"SEEM is a versatile, interactive segmentation model that unifies diverse visual and textual prompts for robust, open-vocabulary segmentation across multiple tasks with minimal supervision."
"MEMTO is a memory-guided Transformer that improves anomaly detection in multivariate time series by using a novel updatable memory module and a two-phase training method, achieving higher F1-scores than state-of-the-art approaches."
"We introduce NLGraph, a natural language benchmark for graph reasoning tasks, and show that while LLMs exhibit some graph reasoning ability, their performance degrades on complex tasks and is sensitive to spurious correlations, with specialized prompting improving results but leaving advanced graph reasoning unresolved."
"We propose a non-recurrent time series kernel based on the equivalence between reservoir dynamics and NVAR processes, offering interpretable hyperparameters and demonstrating strong accuracy and speed on real-world classification tasks."
"We propose CRCL, a robust framework with an Active Complementary Loss and Self-refining Correspondence Correction to improve image-text matching under noisy correspondences."
"We propose a data-dependent contraction technique and a fine-grained generalization bound to explain and unify the effects of loss re-weighting and logit adjustment for imbalanced classification, and demonstrate its effectiveness empirically."
"We propose DSGAS, a self-supervised, disentangled graph neural architecture search method that discovers optimal architectures from unlabeled graph data by learning latent graph factors without supervision."
"We propose a hierarchical transformer for massive point clouds in continuous space that addresses irregularity, scale, and computational challenges, and includes uncertainty estimation, scaling to one million points on a single A100 GPU."
"We theoretically unify and improve neural network binarization methods by deriving principled forward-backward quantizer pairs and propose BNN++, which empirically matches or exceeds standard binarization techniques on image classification tasks."
We propose two single-time-scale policy-based primal-dual algorithms for constrained infinite-horizon MDPs with non-asymptotic convergence guarantees for the policy iterates to an optimal constrained policy.
"We propose a masked two-channel decoupling deep framework for incomplete multi-view weak multi-label learning that improves representation learning via cross-channel contrast and label-guided regularization, and adapts to missing views and labels."
"PAW optimizes Winograd convolution quantization across all stages and introduces FSQ, achieving higher accuracy with 8-bit quantization and larger tile sizes than previous methods."
"We replace the spatial model in SPatial verification with a topological model using bio-inspired saccade and fovea functions, achieving state-of-the-art explainable image retrieval without fine-tuning."
"We enhance sequence capacity in recurrent Hopfield-like networks using a nonlinear interaction term and a generalized pseudoinverse rule, deriving improved scaling laws and enabling storage of variable-timed sequences with biological plausibility."
"We propose a new, computationally efficient, kernel-based, doubly-robust test for distributional treatment effects with valid type-I error control."
"We propose and demonstrate efficient protocols to verify the training data of large neural models, enabling detection of data inclusion and resisting known attacks."
"OPNP is a training-free OOD detection method that removes overfitting-related parameters and neurons by evaluating their sensitivity using averaged gradients over training data, consistently outperforming existing methods."
"GIMLET improves zero-shot molecule property prediction from natural language instructions by unifying graph and text encoding with generalized position embeddings and instruction decoupling, outperforming baselines and approaching supervised GNNs on tasks like ToxCast and MUV."
"Meta-Adapter is a lightweight plug-and-play adapter that enables efficient online few-shot learning with CLIP, outperforming state-of-the-art methods by 3.6% on average across eight datasets without additional fine-tuning."
"SatLM improves LLM reasoning by generating declarative task specifications and using an automated theorem prover to ensure correct solutions, outperforming program-aided LLMs on multiple datasets."
"We propose a predict-then-calibrate framework for risk-sensitive contextual optimization that separates prediction from uncertainty quantification, enabling flexible model choices and generalization guarantees for both robust and distributionally robust contextual linear programming."
Regularized Stein thinning addresses empirical pathologies of the original algorithm and is theoretically and empirically shown to efficiently post-process MCMC outputs by minimizing kernelized Stein discrepancy with improved guarantees.
"We derive an information-theoretic recovery criterion based on the Chernoff-Hellinger divergence and propose an exponential-family-based iterative clustering algorithm that jointly leverages network and node attribute information, outperforming single-source and existing joint methods."
"We use statistical physics to analyze learning dynamics and plateaus in temporal difference reinforcement learning with linear function approximation under a Gaussian equivalence hypothesis, revealing how features, learning rates, and rewards affect performance and how interventions like annealing and reward shaping modify these dynamics."
Bayesian target optimisation reduces off-target stimulation in two-photon optogenetics by optimising laser power and target location using a nonparametric Bayesian model of neural responses.
"SeqBoat introduces Sparse Modular Activation to sparsely and dynamically activate attention modules, achieving linear inference complexity and improved quality-efficiency trade-offs in sequence modeling tasks."
"4M is a multimodal masked modeling approach that trains a single Transformer to process and generate across text, images, geometric, semantic, and feature map modalities, yielding versatile vision models capable of diverse tasks, effective fine-tuning, and multimodal generation."
"We propose a variable importance framework that quantifies variable importance across all good models and is stable across data distributions, consistently recovering true variable importance in both simulations and a real HIV genetics study."
"We present L-CAD, a unified model for language-based colorization that handles any-level descriptions and preserves spatial structures, outperforming previous methods on diverse scenarios."
"WAFFLE attacks show that multi-exit language models are vulnerable to adversarial slowdown, with more complex mechanisms being more susceptible, and input sanitization outperforming adversarial training in mitigating these attacks."
"We introduce Fast Trainable Projection (FTP), a scalable and efficient projection-based fine-tuning method that improves out-of-distribution robustness and generalizes to multiple learning scenarios."
"We propose MIM4DD, a dataset distillation method that maximizes mutual information between synthetic and real datasets using contrastive learning to improve information preservation."
We propose a variational perspective using the forced Euler-Lagrange equation to achieve faster convergence for gradient norm minimization with Nesterov's accelerated gradient and introduce a stochastic variant for noisy gradients.
"We propose tighter confidence sequences for the stochastic linear bandit problem using adaptive martingale mixtures, leading to improved worst-case regret bounds and better empirical performance in hyperparameter tuning."
"We propose an offline RL method that constrains the policy to high-quality data subsets using importance sampling, outperforming state-of-the-art algorithms by up to five times on imbalanced datasets."
We introduce a memory-efficient algorithm for fair PCA in streaming settings with theoretical guarantees of probably approximately fair and optimal learnability.
"Bounce is a reliable Bayesian optimization method for high-dimensional mixed and combinatorial spaces, outperforming state-of-the-art methods in diverse benchmarks."
We propose correlative information maximization as a biologically plausible alternative to backpropagation that resolves the weight symmetry problem and better models dendritic and lateral inhibition in neural networks.
NAP is the first diffusion-based 3D generative model to synthesize articulated objects using an articulation graph representation and a specialized graph denoising network.
We analyze multi-task and multi-domain training dynamics and propose population-based search for optimal scalarization weights when handling many tasks or domains.
"CSRO reduces context shift in offline meta-reinforcement learning by minimizing policy influence on context during both meta-training and meta-test using mutual information learning and a non-prior context collection strategy, thereby improving generalization."
"We propose a Bayesian method using sparse Laplace priors and Langevin MCMC to principledly identify which parameters to update during fine-tuning, achieving state-of-the-art results on NLP and vision benchmarks."
"GLOBER is a non-autoregressive video generation method that uses a video auto-encoder and diffusion model to produce globally coherent and locally realistic videos from global features, achieving new state-of-the-art results."
"We prove that neural network moments with non-polynomial analytic activations yield injective multiset functions (with near-optimal moment count), and provide separations for graph neural networks and negative results for piecewise-linear networks and bi-Lipschitzness."
"DAS enables efficient parameter and computation reduction in vision-language pre-trained models via reinforcement learning-guided dynamic module skipping with adapters, outperforming existing PETL methods in FLOPs and performance."
MTGC3 is a multi-task graph neural architecture search method that discovers task-specific architectures and leverages task-aware collaboration and curriculum learning to improve performance across multiple graph tasks.
"We propose a stochastic encoding model of salamander retinal ganglion cells to compute stimulus discriminability under naturalistic conditions using information geometry, revealing that retinal noise correlations limit information transmission and that population coding equalizes information across firing rates."
"We propose learning rate–free particle-based sampling algorithms for constrained domains, unifying them with existing methods and demonstrating competitive performance without hyperparameter tuning."
"CoDet reformulates region-word alignment as a co-occurring object discovery problem using image caption groups and visual similarity, achieving new state-of-the-art results in open-vocabulary detection."
"Transformers optimize a sparse rate reduction objective by compressing data into low-dimensional Gaussian mixtures via self-attention and MLP layers, achieving performance close to ViT on ImageNet."
"We provide uniform convergence guarantees for Gaussian data using Rademacher complexity and the square root Lipschitz constant of the loss, generalizing previous smoothness-based results and applying to non-smooth losses such as those in phase retrieval and ReLU regression."
"REx is a flexible quantization method using residual error expansion and group sparsity that achieves better accuracy-speed trade-offs across bit-widths and models, including large language models, and is theoretically grounded and compatible with existing quantization operators."
"We replace the linear kernel in optimization-induced deep equilibrium models with a Gaussian kernel to create GEQ, a more stable and higher-performing model capable of capturing nonlinear feature dependencies."
"We propose a regularizer that reduces concurvity in Generalized Additive Models, improving interpretability without significantly affecting prediction quality."
MultiMon automatically discovers systematic failures in multimodal models by finding inputs that produce erroneous agreement and generating natural-language descriptions of these failure categories.
"UNSSOR is an unsupervised neural speech separation algorithm that leverages over-determined microphone arrays and a constraint that speaker estimates at each microphone must sum to the observed mixture, using sub-band forward convolutive prediction and a magnitude scattering loss to address frequency permutation."
"PUCA is a novel J-invariant U-Net architecture for self-supervised image denoising that uses patch-unshuffle/shuffle and dilated attention blocks to expand receptive fields and incorporate global context, outperforming existing methods."
TAPS improves certified robustness and standard accuracy by combining IBP and PGD training to optimize tighter worst-case loss approximations.
"We introduce a training method for generative models that explicitly optimizes a user-defined trade-off between precision and recall by minimizing a novel PR-divergence, improving performance of models like BigGAN on ImageNet."
"We propose a neural sampler using implicit distributions with local linearization to accurately approximate complex posteriors in large Bayesian neural networks, outperforming existing methods on uncertainty quantification."
We propose RL algorithms for dialogue management that exploit Mixture-of-Expert Language Models to reduce action space size and improve intent diversity and performance in open-domain dialogue.
DeepSimHO improves 3D hand-object pose estimation stability by integrating physics simulation and learned gradient approximation into a data-driven pipeline.
"We introduce binomial voting, a rule that achieves strong distribution-independent guarantees on expected distortion and welfare."
"We adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to efficiently handle sparse inputs, reducing training time from $\mathcal{O}(T D S + T N S)$ to $\mathcal{O}(N S + T \sqrt{D} \log D + T S^2)$ and achieving up to $2,200\times$ speedup for differentially private sparse regression."
"Topological parallax measures geometric similarity between a dataset and a trained model via topological data analysis, offering a tool to assess trustworthy generalization and robustness."
"We propose a method to generate pixel-level semantic segmentation pseudo-labels from Stable Diffusion text prompts, enabling effective training of segmenters without manual pixel annotation."
Cappy is a small pretrained scorer that improves multi-task LLM performance and enables efficient downstream supervision without LLM finetuning or parameter access.
We propose sub-sampled activation estimators that reduce transformer fine-tuning memory usage by up to 2.7× with negligible accuracy loss and enable up to 6.4× larger batch sizes.
"We show that in sequential Bayesian persuasion with farsighted receivers modeled as MDPs, history-dependent ""promise-form"" signaling schemes are both optimal and computable in polynomial time, unlike myopic or Markovian policies."
"We propose federated learning algorithms (FedSGDA+ and FedSGDA-M) for distributed nonconvex minimax problems, achieving improved communication and sample complexity bounds and matching centralized performance in strongly convex settings."
"We derive near-optimal convergence rates for clustering with point and subspace centers, showing $\tilde{O}(\sqrt{k/n})$ for point-based objectives and $\tilde{O}(\sqrt{kj^2/n})$ for j-dimensional subspaces, with a matching lower bound $\Omega(\sqrt{kj/n})$ for projective clustering."
We propose a peer-to-peer machine learning framework that secures against malicious servers and is robust to malicious clients by transforming robust aggregation algorithms for distributed model training.
ARTree is a deep autoregressive graph neural network model that learns flexible distributions over tree topologies for efficient phylogenetic inference without heuristic features.
"We propose a multi-stage Bayesian risk-averse Q-learning algorithm for robust reinforcement learning with streaming real-world observations, providing convergence guarantees and balancing robustness with reduced conservativeness in model-mispecified infinite-horizon MDPs."
We propose a Fr\'echet regression method exploiting covariate low-rank structure to improve estimation accuracy in high-dimensional and noisy settings.
O-ZD is the first structured finite-difference algorithm for non-smooth black-box optimization that approximates the gradient of a smooth approximation using random orthogonal directions and achieves optimal complexity for non-smooth convex functions and strong performance in practice.
"Stochastic gradient descent enables scalable and accurate Gaussian process inference with reliable uncertainty estimates, even without full convergence."
"VaSSO stabilizes the adversary in sharpness-aware minimization, improving generalization and robustness to label noise compared to SAM."
"We present a method to learn optimistic symbolic world models for reinforcement learning that enables faster exploration and generalization with minimal human input, and evaluate it on benchmark domains."
"DiT-3D is a scalable Diffusion Transformer for 3D shape generation using voxelized point clouds and 3D window attention, achieving state-of-the-art results on ShapeNet."
We propose a speculative decoding algorithm using optimal transport with membership cost that achieves up to 2.91× wall-clock speedup over autoregressive sampling with no quality loss.
"ReMaX improves panoptic segmentation training for efficient mask transformers by relaxing mask and class predictions, achieving new state-of-the-art results on COCO, ADE20K, and Cityscapes with minimal computational overhead."
VAPNet learns visual attributes from known categories without annotations and integrates them into retrieval models to enable open-set fine-grained retrieval of unknown categories.
"The Hierarchical Exponential-family Energy-based (HEE) model enables simultaneous inference and localized, rapid learning in neural networks by decomposing the partition function and using neural adaptation as momentum, producing brain-like visual representations and competitive marginal generation performance."
"LaCLIP improves CLIP pre-training by augmenting text inputs via large language model rewrites, significantly boosting transfer performance without additional computation or memory overhead."
Replacing geometric similarity metrics with Kendall’s rank correlation for channel importance improves few-shot learning performance across methods and datasets.
"We enhance large language models by finetuning them on diverse embodied experiences from a physical world simulator, using elastic weight consolidation and low-rank adapters to retain general language ability and significantly improve performance on reasoning and planning tasks."
"We propose a regularized loss that penalizes mismatch-inducing statistics, enabling robust simulation-based inference under model misspecification for methods like NPE and ABC."
"We propose and demonstrate a self-recalibrating intracortical brain-computer interface method that maintains stable, high decoding accuracy (93.84%) for over a year in a human user without interruptions."
"MomentDiff is a generative diffusion-based framework for video moment retrieval that enables robust, bias-resistant temporal localization by denoising random noise guided by text-video similarity, outperforming state-of-the-art methods on standard and anti-bias datasets."
We show that a simple GP nearest-neighbour regression method achieves high predictive accuracy and well-calibrated uncertainty at very low computational cost for large datasets by requiring minimal parameter estimation and correcting only the noise variance.
We present a method to learn interpretable cognitive models directly from behavioral data using a recurrent neural network with a sparsity penalty.
An autoregressive method with subgraph tokenization and annealed entropy regularization better solves combinatorial optimization problems by modeling dependencies among solution variables.
"PLANNER combines latent semantic diffusion with autoregressive generation to efficiently produce fluent, globally controlled long-form text."
CoLA is a compositional linear algebra framework that efficiently handles structured matrices and accelerates large-scale linear algebra operations with automatic differentiation and GPU support in JAX and PyTorch.
"We show that proper value equivalence is insufficient for optimal planning in risk-sensitive reinforcement learning, and introduce new model equivalence notions enabling optimal planning for selected risk measures, with theoretical and empirical validation."
"We propose Constrained Trainable Random Weight (CTRW), a method that optimizes random weight parameters within theoretically derived bounds to improve the trade-off between natural accuracy and adversarial robustness in neural networks."
"We propose a new residual block and loss function that enable state-of-the-art, scalable deterministic robustness guarantees for deep networks on ImageNet under ℓ₂ perturbations."
"H-GRAM enables transferable, scalable few-shot learning in hyperbolic neural networks by learning inductive biases from local subgraphs and meta-gradients, outperforming baselines on large graph datasets."
"We introduce contextual stochastic bilevel optimization (CSBO) and propose an efficient double-loop gradient method with proven sample and computational complexity guarantees, particularly effective for meta-learning without dependency on the number of tasks."
"The paper formalizes and identifies concept subspaces in the representations of score-based text-guided generative models, enabling concept manipulation via algebraic operations, and demonstrates this on Stable Diffusion."
"We establish an adaptive algorithm for nonparametric contextual bandits that achieves minimax dynamic regret using a localized notion of significant reward changes, rather than global measures."
We propose a distillation-based collaborative learning method using shared unlabeled auxiliary data with adaptive trust weights to improve individual model performance while mitigating the impact of poor models and reducing communication overhead.
"We propose graph convolutional kernel machines (GCKMs), which use kernel functions with graph convolutions to achieve strong generalization, global optimality, and high interpretability for graph-based tasks with less design and tuning effort than GCNs."
"We provide nearly optimal VC-dimension and pseudo-dimension estimates for the derivative functions of deep neural networks, enabling tight approximation results in Sobolev spaces and characterizing generalization errors for learning methods involving derivatives."
"PARNI-DAG is a novel MCMC sampler for efficient Bayesian structure learning of DAGs from observational data, using adaptive proposals and pre-tuning for improved mixing and scalability."
SIGNET is a self-interpretable graph-level anomaly detection model that simultaneously identifies anomalous graphs and provides informative subgraph explanations using a multi-view subgraph information bottleneck framework.
"The paper introduces hierarchical perceptual and spurious contexts to refine category descriptions for out-of-distribution detection in vision-language models and demonstrates a scalable, category-extensible OOD detection method called CATEX that outperforms existing approaches on ImageNet-1K and supports efficient expansion to thousands of categories."
"We propose a method that reconstructs smooth surfaces from unorganized point clouds using implicit neural representations and enforcing the p-Poisson and curl-free constraints on the gradient of the signed distance function for robust, high-quality surface reconstruction without additional supervision."
"We provide nonparametric conditions and a constructive method for identifying latent causal structures in measurement models from unknown interventions, introducing the concepts of imaginary subsets and isolated edges, without assuming linearity, Gaussianity, known hidden variables, or faithfulness."
"We propose model-based posterior sampling algorithms for competitive reinforcement learning in zero-sum Markov games with general function approximation, providing sublinear regret bounds based on new exploration complexity measures and handling both self-play and adversarial learning under full or partial observability."
A3FL is a federated learning backdoor attack that adversarially adapts the trigger to survive global model updates and existing defenses by optimizing it against worst-case unlearning scenarios.
Neural networks trained on modular addition can discover multiple qualitatively different algorithms depending on hyperparameters and initialization.
"We propose λ-equitune, a weighted group averaging method that outperforms previous equivariant transfer learning techniques across multiple tasks and models, and prove its equivariance and universal approximation properties."
"Distributional reinforcement learning outperforms vanilla RL in low optimal cost regimes and enjoys improved theoretical bounds and empirical performance across contextual bandits, online, and offline settings."
Self-Refine improves LLM outputs at test-time by iteratively generating feedback and refining responses without additional training.
We propose a bandit algorithm that leverages batch data and causal model invariances across environments to achieve sub-linear regret with explicit dependence on the informativeness of related environments.
"ClusterFormer is a universal vision model with recurrent cross-attention clustering and feature dispatching that achieves state-of-the-art results across image classification, object detection, segmentation, and panoptic segmentation."
We introduce parameterized positive non-trigonometric random features that provide lower-variance Gaussian and softmax kernel approximations and demonstrate superior performance in kernel regression and Transformer self-attention approximation.
"SwiftSage, a dual-process agent framework combining a fine-tuned small LM and a large LLM for action planning, outperforms existing methods on the ScienceWorld benchmark."
"We present a novel distributed method for variational inequalities that combines function similarity, information compression, and local updates, achieving superior theoretical communication complexity and empirical performance in adversarial learning."
"DivOE enhances out-of-distribution detection by synthesizing more informative outliers through a multi-step optimization method during training, improving extrapolation beyond the original auxiliary outliers."
"FedDPA improves personalized federated learning with differential privacy by dynamically personalizing model layers using Fisher information and adaptively constraining parameter updates, enhancing performance and convergence under clipping."
We propose a meta-learned hybrid visual active search method that integrates prediction and search modules to better leverage supervised information and outperform state-of-the-art approaches on multiple geospatial tasks.
"RKD is theoretically analyzed as spectral clustering on a teacher-induced graph, showing provable low clustering error and sample complexity bounds in semi-supervised learning, and contrasting its global perspective with the local focus of consistency regularization."
"We propose spectral distillation and a co-distillation framework with a wait-free local training protocol to better capture generic and personalized model representations in personalized federated learning, demonstrating improved performance under data heterogeneity."
"We propose TranSVAE, a disentanglement-based unsupervised video domain adaptation method that separates and transfers static and dynamic latent factors to reduce spatial and temporal domain divergence."
"We propose an active learning algorithm for multi-group PAC learning that achieves label complexity $\tilde{O}\left( (\nu^2/\epsilon^2) G d \theta_{\mathcal{G}}^2 \log^2(1/\epsilon) + G\log(1/\epsilon)/\epsilon^2 \right)$ for the agnostic case and $\tilde{O}\left( G d \theta_{\mathcal{G}} \log(1/\epsilon) \right)$ for the realizable case, improving upon standard multi-group learning when disagreement is low and the number of groups is moderate."
"We propose Heavy Hitter Oracle (H₂O), a KV cache eviction policy that retains key tokens and significantly improves throughput and reduces latency in large language model inference by up to 29× and 1.9×, respectively."
"We propose and analyze posterior sampling-based algorithms for linear function approximation in RL with delayed feedback, achieving near-optimal regret bounds while reducing computational cost via Langevin dynamics."
"We propose an active representation learning framework that adaptively selects source tasks to reduce sample complexity for target tasks, theoretically and empirically demonstrating up to a 70% improvement over baselines."
"SimMMDG is a multi-modal domain generalization framework that splits modalities into specific and shared features, applies contrastive and distance learning, and uses cross-modal translation to improve generalization, achieving strong results on EPIC-Kitchens and a new HAC dataset."
"We introduce contrastive feature selection (CFS), a feature selection method for the contrastive analysis setting that outperforms existing methods on biomedical datasets."
"We propose a linear multi-weight correlation method that reduces computational cost by 76% when growing DeiT-base from DeiT-small, outperforming previous weight reuse methods."
"DeepPCR parallelizes sequential operations in neural networks using Parallel Cyclic Reduction, reducing their complexity from O(L) to O(log₂L) and achieving up to 200× speedup for backward passes and up to 11× for diffusion model generation."
We propose a contrastive learning framework for environment-aware affordance that generalizes to complex occlusions without requiring explicit training on all possible occlusion combinations.
"GenS is an end-to-end, generalizable neural surface reconstruction model that uses a multi-scale volume encoding and multi-scale feature-metric consistency to achieve robust, high-quality surface reconstruction from multi-view images without 3D supervision."
MetaMAE enhances modality-agnostic self-supervised learning by interpreting masked auto-encoder reconstruction as a meta-learning task and integrating gradient-based meta-learning and task contrastive learning.
"BanditPAM++ accelerates the k-medoids clustering algorithm BanditPAM by reusing clustering information within and across iterations, achieving up to tenfold speedups while preserving clustering results."
We propose a privacy-preserving federated spectral clustering method that approximates the kernel matrix without exposing pairwise data and provide theoretical guarantees and empirical validation.
"This work analytically and empirically quantifies the validation data requirements and quality guarantees of threshold-based auto-labeling (TBAL) systems, revealing that even weak models can auto-label large data segments accurately but TBAL can require prohibitively large validation sets."
"ANPL is an interactive programming system that enables users to refine LLM-generated code via structured decompositions, outperforming baseline systems on challenging tasks and multiple programming domains."
"Rewritten abstract:  
We reinterpret popular KGE score functions as circuits, enabling exact maximum-likelihood learning, efficient sampling, logical constraint integration, and better scalability for link prediction."
"We propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (SILD), a DyGNN method that uses Fourier-transformed spectral features and invariant pattern filtering to handle spectral domain distribution shifts in dynamic graphs."
IMPRESS evaluates the effectiveness of imperceptible perturbations in protecting images from unauthorized diffusion-based generation by exploiting perceptible inconsistencies between original and reconstructed images.
"Imitation learning in mean-field games is harder when dynamics depend on the population, necessitating a mean-field control-based adversarial formulation."
"We propose EI², which enhances video editing consistency in TTI diffusion models by mitigating covariate shift with a Shift-restricted Temporal Attention Module and a Fine-coarse Frame Attention Module."
"Elastic Decision Transformer (EDT) enables trajectory stitching by adaptively adjusting history length during inference, outperforming both Decision Transformer and Q-learning baselines in multi-task settings."
"We propose Symbol-LLM, a symbolic system for visual activity understanding that leverages large language models to generate broad-coverage symbols and derive activity semantics via rational rules, outperforming previous methods on multiple tasks."
"We introduce ""idealized runtime,"" a hardware- and software-agnostic metric to fairly compare inference efficiency across large language models, and use it to analyze the tradeoff between efficiency and capability."
"We present a multi-task representation learning model for animal behavior with an action-prediction objective and a multi-scale architecture that captures short- and long-term dynamics, outperforming baselines on robot and multi-agent behavior benchmarks."
"REST is a single-source, edge-wise GNN model that learns relevant rule-induced subgraph representations for inductive relation prediction without node labeling, improving performance and preprocessing efficiency."
"Adacom improves code comment generation by dynamically retraining on relevant samples per target code, boosting BLEU4, METEOR, and ROUGE-L by up to 14.9%, 12.2%, and 7.4%, respectively."
"We propose a heterogeneous representation learning framework that incorporates public opinion field and social circle influence effects for trending topic diffusion, outperforming existing methods on real-world datasets."
The LEHD model enables neural combinatorial optimization to generalize and solve large-scale TSP and CVRP instances with high performance using only small-scale training data.
"We prove a consistency–robustness tradeoff for single-trajectory time-varying MDPs with Q-value advice and partial knowledge of its generation, showing improved near-optimal performance over black-box advice."
"HQ-SAM enhances SAM’s mask prediction by introducing a learnable High-Quality Output Token and feature fusion with minimal added parameters, trained on 44K fine-grained masks to significantly improve segmentation accuracy while preserving SAM’s efficiency and zero-shot capabilities."
PROTES is a black-box optimization method using low-parametric tensor train-based probabilistic sampling that outperforms popular discrete optimization algorithms on high-dimensional and real-world problems.
"We propose stochastic bilevel optimization methods with improved complexity bounds of $\mathcal{O}(\max\{1/\epsilon_f^2,1/\epsilon_g^2\})$ for convex upper-level functions and $\mathcal{O}(\max\{1/\epsilon_f^3,1/\epsilon_g^3\})$ for non-convex ones, and $\mathcal{O}(\sqrt{n}/\epsilon)$ and $\mathcal{O}(\sqrt{n}/\epsilon^2)$ stochastic oracle calls in the finite-sum setting."
"We introduce and validate ""semifactual"" explanations that enhance already positive automated decisions, demonstrating their greater user utility for loan acceptance outcomes compared to traditional counterfactuals."
"Dynamo-Depth jointly learns monocular depth, 3D independent flow, and motion segmentation from unlabeled video to resolve depth ambiguity in dynamic scenes, achieving state-of-the-art results on Waymo and nuScenes."
"Additive decoders enable latent variable identification and out-of-support image generation under weak assumptions, supporting nonlinear ICA and requiring additivity for both identifiability and extrapolation."
"SGFormer achieves competitive node property prediction on large graphs using a single-layer simplified attention without positional encodings, preprocessing, or complex designs, scaling efficiently even to web-scale graphs."
"We demonstrate that scaling MLPs achieves high accuracy on vision benchmarks, showing that inductive bias is not necessary for strong performance and that MLPs faithfully mimic modern models while being computationally efficient."
"HomoODE is a new implicit model that unifies Deep Equilibrium Models and Neural ODEs via homotopy continuation, achieving higher accuracy and lower memory than existing implicit models on image classification tasks."
"Enforcing sparse coding via Top-K operations in neural networks induces a shape bias, enhancing robustness to texture and improving structural decomposition in both recognition and generative tasks."
"We introduce Equal Opportunity of Coverage (EOC) and a post-processing method (BFQR) to achieve uncertainty-aware, fine-grained group fairness with informative prediction intervals."
"Rewritten abstract in one short sentence:

We propose a trend detection method for positive and unlabeled learning by modeling classifier scores as a temporal point process, improving performance especially in highly imbalanced settings without extra tuning."
"$k$-DisGNNs overcome the geometric limitations of Vanilla DisGNNs by leveraging distance matrices, enabling higher expressive power and achieving new state-of-the-art results on MD17."
SOAR is an orthogonality-amplified residual loss-based data indexing technique that achieves state-of-the-art approximate nearest neighbor search performance with fast indexing and low memory usage.
"We present learning-augmented sorting algorithms that achieve theoretically optimal comparison complexity, scaling smoothly from O(n) to O(n log n) as prediction accuracy degrades, and validate their practical effectiveness experimentally."
"PDE-Refiner, inspired by diffusion models, improves long-term stable and accurate time-dependent PDE solution rollouts by refining all frequency components and enhancing data efficiency and uncertainty estimation."
SFTL uses Gaussian process-based state-space models with online filtering and parallel smoothing to capture the temporal evolution of tensor factors from streaming data.
We prove identifiability of true latents for a broad class of contrastive losses without independence assumptions and validate the results empirically while discussing practical limitations.
"We study how to optimally request and use a limited number of predictions during online execution for the ski rental, secretary, and non-clairvoyant job scheduling problems."
We present a selective sampling algorithm for imitation learning with noisy expert feedback that achieves tight bounds on regret and query complexity using online regression and depends only on the optimal policy’s margined states.
"We present an exact algorithm for detecting fixed points, cycles, and bifurcations in ReLU-based RNNs and show it can explain and prevent certain training bifurcations."
"We prove convergence of actor-critic methods with deep neural networks of arbitrary depth to a neighborhood where the average squared gradient is $\tilde{O}(1/\sqrt{m}) + O(\epsilon)$, closing the gap between theory and practice."
We propose a model-based planning and graph-based value aggregation method that corrects value estimation artifacts to extract goal-conditioned behavior from unsupervised exploration data without additional environment interaction.
We propose a weakly-supervised concealed object segmentation method that uses multi-scale feature grouping for segmentation coherence and leverages the Segment Anything Model with robust training strategies to achieve state-of-the-art performance with sparse annotations.
ProPILE is a probing tool that enables data subjects to assess the potential leakage of their personally identifiable information in large language models trained on web data.
"LogicHOI replaces pre-composed human-object pairs with neural-logic reasoning over ⟨human, action, object⟩ triplets guided by affordances and proxemics, improving HOI detection and enabling zero-shot generalization."
"We introduce privacy filters enabling adaptive composition of differentially private and ex-post private mechanisms with overall privacy guarantees, demonstrating that noise reduction mechanisms significantly outperform existing composition bounds for releasing counts with relative error under a privacy budget."
"Rewritten abstract in one short sentence:

We show that under standard public-key cryptography assumptions, answering more than Θ(n²) adaptively chosen queries by a balanced adversary—where the adversary does not know the sampled distribution—is computationally hard, and such hardness implies the existence of public-key cryptography."
"We propose a new algorithm that satisfies counterfactual fairness by leveraging all available features, with theoretical and empirical guarantees, unlike previous methods that restrict feature use."
"CoDA is a unified framework that simultaneously localizes and classifies novel 3D objects from arbitrary categories using 3D box priors, 2D open-vocabulary semantics, and iterative cross-modal alignment, achieving an 80% mAP improvement over prior methods on SUN-RGBD and ScanNet."
"We derive a finite-depth corrected rank measure $R^{(1)}$ that bounds Jacobian rank and exhibits subadditivity, showing that deep, L2-regularized DNNs learn representations of dimension $R^{(0)}(f)$ and have weight matrices with $R^{(0)}(f)$ singular values near 1, while large learning rates are needed for proper infinite-depth convergence."
"FaceDNeRF reconstructs and enables semantic editing and relighting of high-quality 3D faces from single images using a combination of 3D GAN inversion and latent diffusion models, outperforming prior 2D and 3D face editing methods."
Simplifying the complexity of training background images improves deraining network generalization by preventing overfitting to rain patterns.
"CADet, a fully self-supervised method using contrastive learning, detects both unseen-class and adversarial OOD samples without requiring labeled in-distribution or OOD data."
"Policy gradient methods can fail in continuous reinforcement learning due to extremely non-smooth or fractal optimization landscapes, which we analyze using chaos and non-smooth theory and propose sample-based methods to detect, explaining some observed training failures."
Gradual adversarial self-training improves both adversarial and clean accuracy in gradual domain adaptation by leveraging adversarial training's robustness to noisy pseudo-labels.
"We introduce Where2Explore, a framework that leverages geometric similarity across object categories to enable efficient few-shot learning of manipulation affordances with minimal interactions."
"ResidualPlanner is a scalable, optimal Gaussian matrix mechanism for releasing noisy marginals that supports flexible convex objectives and efficiently computes marginal variances even on high-dimensional datasets."
"We propose causal normalizing flows for causal model identification, intervention, and handling mixed discrete-continuous data with partial causal knowledge."
"We propose a diffusion-based, end-to-end lossy image compression method that uses a content latent variable and achieves high reconstruction quality with few decoding steps."
RADAR is a robust AI-text detector trained adversarially with a paraphraser to improve detection under paraphrasing across multiple LLMs.
"We propose an optimization-based meta-learning method for large-scale neural field training that selects relevant context points online to save memory, incorporates a bootstrap correction to reduce initialization error and meta-learning myopia, and uses gradient re-scaling to accelerate high-quality reconstruction, achieving state-of-the-art results across multiple datasets and modalities."
"Neural oscillators, defined as a class of coupled oscillators, are proven to be universal approximators of continuous and causal operators between time-varying functions, using forced harmonic oscillators with nonlinear read-out."
"CAT-Walk is an inductive representation learning method for temporal hypergraphs that uses SetWalk for higher-order temporal walks and SetMixer for permutation-invariant pooling, achieving strong performance on temporal hyperedge prediction and competitive results on node classification."
"We present a differentially private min $s$-$t$ cut algorithm with efficient privacy and runtime, and a more privacy-efficient algorithm for multiway $k$-cut, both achieving near-non-private approximation quality."
"DiffAttack is a unified framework that effectively and efficiently attacks diffusion-based purification defenses by inducing inaccurate density gradient estimation and enabling memory-efficient backpropagation, significantly reducing robust accuracy compared to existing methods."
"SegRefiner improves object mask quality via a model-agnostic, diffusion-based refinement process that outperforms previous methods and enhances both segmentation and boundary metrics across tasks."
"ProtoSEG unifies semantic, instance, and panoptic 3D point cloud segmentation into a prototype-based classification framework using a Transformer, outperforming specialized models on standard benchmarks."
"We propose a method that refines diffusion-generated plans using a restoration gap metric and an attribution map regularizer, improving feasibility and explainability in long-horizon offline control tasks."
"VanillaNet is a simple, attention-free neural network that achieves performance comparable to deep and transformer-based models, demonstrating the effectiveness of minimalist architecture in resource-constrained settings."
"We present a spectral clustering algorithm that computes a vertex embedding using $O(\log k)$ power method vectors in nearly-linear time and recovers ground truth clusters with accuracy comparable to classical methods, but significantly faster."
The paper introduces a structural causal bandit framework for online decision-making with unknown interventional distributions and shows an algorithm that leverages causal structure to achieve logarithmic regret.
"We propose OneNet, an online ensemble method that dynamically combines univariate and multivariate time series models using reinforcement learning to reduce forecasting error by over 50% compared to state-of-the-art methods."
"We extend average Lipschitz smoothness to average Hölder smoothness and show that, in any totally bounded metric space, learning algorithms can achieve near-optimal rates based on this average measure, which can be much smaller than the classic worst-case Hölder constant and yield sharper guarantees."
"End-to-end visual reasoning with general-purpose neural networks, enabled by self-supervised video frame compression pretraining, outperforms traditional supervised pretraining methods on compositional visual reasoning benchmarks."
"We propose a patch-based random search method for black-box adversarial attacks against No-Reference Video Quality Assessment models, formulating attacks to mislead quality scores under just-noticeable difference constraints."
"We present efficient differentially private algorithms for high-dimensional stochastic block model recovery and mixture of spherical Gaussians learning, achieving near-optimal guarantees and removing prior restrictions on separation or center norms."
We present a projected stochastic gradient descent algorithm for efficient parameter estimation of log-concave exponential family distributions from truncated samples.
We propose Task Attribute Distance (TAD) to quantify task relatedness and theoretically and empirically show that it predicts few-shot adaptation difficulty across models.
We introduce an interpretable graph neural network with UA-conjecture-based datasets to validate and generate new conjectures in Universal Algebra.
We propose a self-supervised learning method that uses online re-ranking to mine informative neighbors and progressively relaxes boundary filtering to improve clustering accuracy within standard frameworks.
"The authors derive and analyze a stochastic differential equation describing the covariance evolution in infinite-depth-and-width shaped Transformers with modified attention and skip connections, showing stable covariance and preventing rank degeneracy."
"Metis is a framework that converts regular expressions into efficient neural models deployable on network devices, leveraging expert knowledge and labeled data to improve accuracy and throughput over REs and other baselines."
"We present a spectral algorithm that, given noisy Gaussian samples, outputs a list of covariances containing a good approximation with poly(1/α) error in relative Frobenius norm, and use it to obtain the first SOS-free robust GMM learning algorithm."
We systematically investigate what properties beyond task performance are transferred from teacher to student in knowledge distillation and reveal their practical implications.
"We introduce D5, a task and system for automatically discovering goal-driven differences between large corpora, and evaluate it on synthetic and real datasets, showing it can uncover both known and previously unknown differences while highlighting its limitations regarding causality and bias."
FABind is an end-to-end model that integrates ligand-informed pocket prediction and incremental docking to efficiently and accurately predict protein-ligand binding structures.
"We prove and show that gradient descent trajectories, after reparameterization, align on a bifurcation diagram at the edge of stability in linear and single-neuron nonlinear networks, explaining progressive sharpening and the saturation of sharpness near the stability threshold."
"We provide the first time-uniform finite sample guarantees for online principal component regression in the adaptive error-in-variables setting, and apply them to counterfactual treatment effect estimation in adaptively assigned panel data interventions."
We introduce algorithmic stability to neural processes to enhance generalization and robustness under noisy context points.
"We propose a formalization of the Neural Sampling Code that enables fitting and quantitative evaluation of generative models to neuronal responses in macaque V1, showing that flexible deep learning-based models outperform classical ones."
RIDO adaptively optimizes trajectory scheduling to minimize estimator variance in policy evaluation using Monte Carlo simulation.
"We derive an adaptive Metropolis-adjusted Langevin algorithm preconditioned by the inverse Fisher information outer product of log target gradients, showing superior high-dimensional performance."
"ViCA-NeRF is the first view-consistency-aware NeRF method that uses geometric and learned regularization to propagate text-driven 3D edits consistently across views, producing higher-quality, faster 3D editing than state-of-the-art approaches."
We investigate the adversarial fragility of modern structured pruning methods and propose a GKP-based method that enhances both benign and adversarial robustness at no extra cost.
"We analyze the implicit bias of gradient descent and SGD in rank-1 linear networks, revealing how depth, initialization, and stochasticity influence the solution, and show that this bias better reflects that of standard linear networks than diagonal variants."
"We propose AE2, a self-supervised method that learns viewpoint-invariant action features by aligning egocentric and exocentric videos in time using an object-centric encoder and contrastive objectives, outperforming prior work on fine-grained ego-exocentric video understanding tasks."
"We propose a behavior cloning framework using low-level controllers and noise-augmented generative models that, under total variation continuity, can closely match expert trajectory distributions in optimal transport distance."
"AdaB²N improves continual learning with Batch Normalization by adaptively balancing task-wise contributions and statistics, significantly boosting performance on standard benchmarks."
"We propose a bi-level framework for learning behavior alignment reward functions that robustly integrates designer heuristics with primary rewards, improving reinforcement learning performance even with poor auxiliary rewards."
"We present a method to train Transformers that compile into human-readable programs, enabling interpretable models that perform comparably to standard Transformers on diverse tasks."
"We propose the R\'enyi Kernel Entropy (RKE) as an information-theoretic score to estimate the number of modes in generated multi-modal data and use it to evaluate and rank state-of-the-art generative models on image datasets, finding that none fully capture the diversity of real data."
"We propose a general framework called RAI games for responsible AI objectives framed as min-max problems over uncertainty sets, and present two algorithm classes—game-play based and greedy stagewise—that effectively solve these problems, demonstrating their performance on subpopulation shift."
The paper introduces a risk-sensitive variational Bayesian framework for data-driven decision-making under intractable posterior computation and analyzes its convergence rates and predictive performance in parametric and nonparametric settings.
"Modifying ConvNeXt with ConvStem and adjusting the training scheme significantly improves adversarial robustness on ImageNet, with ConvNeXt + ConvStem being the most robust to seen $\ell_\infty$ attacks and ViT + ConvStem generalizing best to unseen $\ell_1/\ell_2$ attacks."
"NUDGE induces interpretable and explainable policies using neural guidance and differentiable logic, outperforming neural-only agents and adapting to varying environment conditions."
We derive the first finite-time error bounds for approximate policy iteration in average-reward MDPs that vanish as policy evaluation and improvement errors approach zero.
"We propose ALIM, a plug-in framework for noisy partial label learning that reduces the negative impact of label detection errors by adjusting label importance based on candidate labels and model outputs."
"We propose a system that alternates sparse feature learning with differentiable decision tree construction to build small, interpretable trees with strong performance."
We propose a multi-objective reinforcement learning framework that learns user preferences via policy comparisons and finds a near-optimal policy with a quasilinear number of queries in the number of objectives.
"SHOT is a versatile gradient-based meta-learning algorithm that suppresses the Hessian along the optimization trajectory in the inner loop by minimizing the distance between target and reference model parameters, empirically outperforming baselines without increasing computational complexity."
"We prove that, under certain conditions, the asymptotic joint statistics of a family of generalized linear estimators trained on data from a Gaussian mixture depend only on the class-conditional means and covariances of the features, establishing universality of training, generalization errors, and estimator geometry."
"The paper rigorously defines and demonstrates that infinite-width, one-hidden-layer neural networks exhibit simplicity bias by relying on low-dimensional input projections even in the presence of more complex, robust features, and shows an ensemble method to mitigate this by promoting feature diversity and improving robustness to noise."
"The Base-(k+1) Graph achieves fast consensus and low maximum degree, enabling decentralized SGD to converge faster and communicate more efficiently than existing topologies."
"We propose Policy Space Diversity PSRO (PSD-PSRO), a PSRO variant with a theoretically sound diversity metric and sample-based optimization that yields less exploitable policies than existing PSRO variants."
"We identify and resolve the scorer-recursive cell entanglement bottleneck in BT-RvNN, reducing its memory usage by 10–16×, achieving new state-of-the-art results on ListOps while maintaining performance on other tasks, and enabling BT-RvNN to output token-level contextualizations for broader integration with deep learning models."
"We propose a fair adaptive experimental design that efficiently uses data, ensures envy-free assignment without parametric assumptions, and improves participant welfare, with theoretical guarantees on group-level convergence and near-optimal allocation."
"Re-sampling improves generalization in long-tail learning only when training images lack irrelevant contexts, otherwise it induces spurious correlations; a context shift augmentation module using head-class contexts as a bank mitigates this and outperforms other methods."
"We introduce the Decision Adapter, a neural architecture that conditions an agent's policy on context to improve generalisation to new transition dynamics, outperforming previous methods and showing robustness to distractor variables."
"We present a generic transformation from item-level to user-level approximate differential privacy that improves user requirements by a factor of $O_{\varepsilon,\delta}(\sqrt{m})$ in the example-scarce regime, and a simple adaptation of the exponential mechanism for pure differential privacy yielding near-optimal user-level bounds for several learning tasks."
"We propose a backdoor-free dataset ownership verification method using a ""hardly-generalized domain"" watermark, trained on modified samples to enable correct classification of hard samples by watermarked models, and validate its effectiveness and robustness experimentally."
"We present a polynomial-time, label-free algorithm that learns high-dimensional halfspaces with margin under affine transformations of product logconcave distributions, achieving polynomial sample and time complexity in dimension and accuracy, using only contrastive first and second moments and establishing unique identifiability via a new monotonicity property."
"SLGD is a self-learning framework that discovers latent patient domains in EHR data and trains personalized classifiers per domain, achieving up to 11% higher AUPRC than baselines under domain shift on eICU and MIMIC-IV."
"AtMan is a memory-efficient, modality-agnostic method that explains generative transformer predictions by perturbing attention mechanisms instead of using backpropagation."
"We propose a new effective robustness metric that controls for in-distribution accuracy across multiple test sets, enabling fair evaluation of OOD robustness in models trained on different data distributions."
"We introduce Type-to-Track, a natural language caption-based multiple object tracking method with a new dataset (GroOT) and evaluation protocols, where our MENDER framework achieves up to 14.7% higher accuracy and 4× faster speed than two-stage baselines."
"We show that likelihood-based causal direction selection under LSNMs is highly sensitive to noise misspecification when conditional variances differ between directions, whereas residual independence testing is more robust."
We propose a temporal threat model for data poisoning using earliness and duration as key metrics and introduce a benchmark and a provably robust baseline defense leveraging temporal aggregation.
"We propose a distribution deviation detection method for deep neural networks that achieves high accuracy with drastically reduced computational cost and no dependence on dataset size, making it suitable for large-scale real-world deployment."
"EvoFed integrates evolutionary strategies with federated learning to drastically reduce communication costs by sharing fitness-based similarity measures instead of model parameters, achieving performance comparable to FedAvg at the expense of increased local computation."
"We introduce HGRN, a gated linear RNN with layer-wise lower-bounded forget gates that effectively models both short- and long-term dependencies and outperforms baselines on language modeling, image classification, and long-range tasks."
"DP-RandP improves the privacy-utility tradeoff of DP-SGD by learning priors from randomly generated images and transferring them to private data, achieving new state-of-the-art accuracy on CIFAR10, CIFAR100, MedMNIST, and ImageNet for various privacy budgets."
"The authors propose Backward Propagation Attack (BPA), which modifies backward propagation in surrogate models to reduce gradient truncation and significantly improve adversarial transferability on ImageNet."
"We propose FdeHBO, a Hessian/Jacobian-free bilevel optimizer achieving O(ε^{-1.5}) sample complexity for nonconvex-strongly-convex stochastic bilevel optimization."
"We propose VIP-token centric compression (VCC), a method that selectively compresses long sequences by focusing on key tokens, achieving over 3× efficiency gains and improved accuracy on tasks with ultra-long sequences up to 128K tokens."
"Jigsaw is a learning-based framework that assembles physically broken 3D objects by matching hierarchical geometric features and integrating segmentation and correspondence to robustly recover piece poses, outperforming prior methods on diverse 3D fracture assembly tasks."
SCRUB is a novel unlearning algorithm that outperforms previous methods across diverse applications and metrics for deep machine unlearning.
"We propose ICT, a method using pseudo-label-driven co-teaching and meta-learning-based sample reweighting to improve offline model-based optimization with state-of-the-art results on multiple design-bench tasks."
MA-BIRDS solves distributed online bi-level optimization to infer multiple interacting experts' reward functions and constraints from streaming demonstrations with sub-linear regret and bounded constraint violation.
DAOL improves open-world OOD detection by reducing the distribution discrepancy between auxiliary and unseen OOD data through Wasserstein-ball-based augmentation.
"The authors propose a weakly supervised method that distills open-vocabulary knowledge from CLIP and DINO into a NeRF for 3D open-vocabulary segmentation using only text descriptions, outperforming fully supervised baselines in some cases without any manual segmentation annotations."
"MoNODEs improve neural ODEs by introducing trajectory-specific, time-invariant modulator variables that enhance generalization and forecasting for systems with static factors of variation."
"We present a method that enables detailed semantic control of pretrained unconditional image generators using proxy masks derived from user-provided guiding masks via a semantic mapper and cross-attention-based rearranging network, supporting semantic image synthesis and related tasks across multiple datasets."
"We propose a relevance attention mechanism with a global representation memory that adaptively selects relevant historical reference features for each search region, improving visual object tracking performance and speed."
"MAViL is a self-supervised audio-visual representation learning method that achieves state-of-the-art classification results on AudioSet and VGGSound by combining masked reconstruction and contrastive learning with self-training, and improves unimodal representations without cross-modal fine-tuning."
"We propose federated multi-objective optimization algorithms (FMGDA and FSMGDA) that enable distributed, privacy-preserving multi-objective learning with client-specific objectives, achieving convergence rates comparable to single-objective federated learning."
"KD-Zero uses evolutionary search to automatically discover effective distillers for any teacher-student architecture, outperforming state-of-the-art methods across multiple tasks."
"We propose a Value-at-Risk based dynamic programming algorithm that optimizes the percentile criterion in reinforcement learning without explicitly constructing large uncertainty sets, yielding less-conservative robust policies."
"We introduce Abnormal Adversarial Examples Regularization (AAER), which mitigates catastrophic overfitting in single-step adversarial training by reducing abnormal adversarial examples, thereby improving adversarial robustness with minimal computational cost."
"We show that in a synthetic Dyck language learning task, Transformer models can achieve similar performance with widely differing attention patterns and weight structures, indicating that interpretability methods focusing on individual components may be misleading."
We propose an augmentation-aware self-supervised discriminator that predicts augmentation parameters to improve GAN training efficiency on limited data.
"We present quantum algorithms for minimizing Lipschitz convex functions and finding critical points of smooth non-convex functions with dimension-dependent accuracy trade-offs not achievable classically, using quantum variance reduction and mean estimation."
"EgoDistill improves efficient egocentric video understanding by combining sparse video frames with IMU data, reducing computational cost by 200× while outperforming state-of-the-art methods on Ego4D and EPIC-Kitchens."
"ICL in transformers is often transient, giving way to IWL during training, but L2 regularization can make ICL more persistent."
"We propose a Rényi differentially private hyperparameter tuning method that uses a random data subset and extrapolation to reduce both privacy cost and computation, improving the privacy-utility trade-off over existing randomized DP tuning."
"The paper introduces a benchmark for interpretability tools by implanting known trojans in models and evaluating their ability to help humans discover these bugs, revealing that existing methods often fail even under ideal conditions."
"GRACE is a lifelong model editing method that enables thousands of sequential, targeted edits to deployed language models without degrading performance or retraining, by storing edits in a discrete latent-space codebook rather than altering model weights."
"In fixed-design online linear regression, no efficient algorithm can approximate the optimal regret within a factor better than $d^{1/\poly(\log \log d)}$ for arbitrary datasets under the Exponential Time Hypothesis, but for datasets drawn i.i.d. from the uniform sphere distribution, a greedy algorithm achieves a $\log d$-approximation to the optimal regret."
"SyncDiffusion synchronizes multiple image diffusions via gradient-based perceptual loss to generate coherent montages without scene blending, outperforming prior methods in user studies while preserving fidelity and prompt compatibility, and is adaptable to layout-guided, conditional, and 360° panorama generation tasks."
"Scaling vision models does not improve mechanistic interpretability; in fact, newer models are often less interpretable than older ones."
"We derive sharp asymptotic results for Naïve Mean Field approximation in high-dimensional linear regression under general priors and model mismatch, showing its inaccuracy for log-normalizing constants and overconfidence in uncertainty quantification using Gaussian comparison inequalities."
"We propose distilling convolution-based sequence models into low-dimensional state-space models to achieve O(1) per-token inference cost, matching Transformers' throughput while outperforming Hyena in speed and efficiency at 1.3B parameters."
"QLoRA enables efficient fine-tuning of large language models on a single GPU using 4-bit quantization and low-rank adapters, achieving ChatGPT-level performance with significantly reduced memory and time requirements."
"MOEA/D finds all extreme points of the multi-objective minimum weight base problem's non-dominated front in expected fixed-parameter polynomial time, outperforming GSEMO on random bi-objective minimum spanning tree instances."
"Aligned language models can be made to generate harmful content via adversarial inputs, especially in multimodal models, suggesting current defenses are inadequate."
"RoCLIP defends CLIP against targeted data poisoning and backdoor attacks by matching images with random captions and using augmentations, reducing attack success rates to near zero while maintaining or improving model performance."
"We propose EICIL, a novel spiking neural network training method integrating excitatory and inhibitory behaviors that outperforms traditional methods on CIFAR10 and CIFAR100."
"EB-TCε is a novel, anytime Top Two sampling rule for ε-best arm identification in stochastic bandits that is asymptotically optimal and outperforms existing algorithms in simulations."
"LLMScore improves automatic evaluation of text-to-image synthesis by using LLMs to assess multi-granularity compositionality, achieving higher correlation with human judgments than CLIP and BLIP."
We propose a visual model-based RL method that learns a latent representation robust to spurious visual variations and enables quick test-time adaptation to distribution shifts.
We propose a method that personalizes large text-to-image diffusion models using only one facial photo and 1024 parameters to seamlessly integrate a new individual with improved concept combination ability.
"We introduce Jaccard Metric Losses (JMLs), a flexible IoU-based loss compatible with soft labels that improves semantic segmentation performance over cross-entropy across multiple datasets and architectures in label smoothing, knowledge distillation, and semi-supervised learning."
We propose a novel method for uniformly bounding estimation error in heteroskedastic variance under linear measurement models and demonstrate its near-optimal performance and significant sample complexity gains in adaptive experimental design tasks.
"We present a unified framework for analyzing SGD with biased gradient estimators, introducing weaker assumptions and demonstrating cases where biased estimators outperform unbiased ones, supported by theoretical and experimental results."
"We propose a primal-dual algorithm with shadow reward and κ-hop neighbor truncation for safe multi-agent reinforcement learning with nonlinear utility-based objectives and constraints, achieving O(T^{-2/3}) convergence in the exact setting and ~O(ε^{-3.5}) sample complexity in the sampled setting."
Specialized skip-connections enable biologically plausible RNNs to learn cognitive tasks with long temporal dependencies more efficiently than conventional methods.
"S-CLIP is a semi-supervised method that enhances CLIP training in specialized domains using unpaired images and pseudo-labeling strategies, significantly improving zero-shot classification and retrieval with fewer image-text pairs."
"We show that the observed double descent in classical machine learning models arises from changes in the complexity axis rather than a true deviation from the U-shaped generalization curve, and that conventional smoothing theory explains the phenomenon as a folding of the curve when measured by effective model complexity."
"The paper introduces a statistical method to quantify LLM choices and uncertainties in moral scenarios and finds that most models align with commonsense in unambiguous cases, show sensitivity to wording in ambiguous ones, and that closed-source models exhibit consistent preferences."
"We formulate a Markov signaling game to address non-stationarity and selective attention in multi-agent reinforcement learning with information design, introducing signaling gradients and extended obedience constraints and providing an efficient algorithm for mixed-motive tasks."
"ConPE, a contrastive prompt ensemble framework using a pretrained vision-language model and domain-specific visual prompts, enables zero-shot policy adaptation and improves sample efficiency for embodied agents across navigation, manipulation, and driving tasks."
We introduce sheaf-enhanced hypergraph Laplacians and corresponding neural networks that outperform standard hypergraph models on node classification tasks.
"LOKT is a novel label-only model inversion attack method that transfers knowledge from a target model to surrogate models, enabling effective white-box attacks with only hard labels and outperforming existing SOTA by over 15%."
B-LATTICE is a blocked collaborative bandits algorithm achieving sublinear per-user regret under per-arm budget constraints by clustering users and collaboratively learning their reward models.
"SSL training inherently promotes semantic label-based clustering through its regularization component, resulting in representations aligned with semantic classes at deeper network levels."
"We derive a lower bound for sample complexity of differentially private Best Arm Identification and propose AdaP-TT, an adaptive DP algorithm whose asymptotic upper bound matches the lower bound in the high-privacy regime."
Shuffling SGD converges to a global solution for non-convex functions under over-parameterized settings with relaxed assumptions while preserving optimal computational complexity.
"We propose a distributionally robust regression-based method for learning the exact skeleton of general discrete Bayesian networks from potentially corrupted data, with non-asymptotic guarantees and empirical validation."
"IPMix is a data augmentation method that improves model robustness across multiple benchmarks without sacrificing clean data accuracy by integrating image-, patch-, and pixel-level augmentations with random mixing and structural complexity."
"Human pattern learning is best explained by probabilistic inference over an expressive space of programs, as shown by comparing human performance to Bayesian program synthesis models."
"Diffusion model objectives can be expressed as weighted integrals of ELBOs over noise levels, and under monotonic weighting, they correspond to ELBO with Gaussian noise augmentation, yielding state-of-the-art FID scores."
"Mix-of-Show introduces an embedding-decomposed LoRA and gradient fusion to enable decentralized, conflict-free, and scalable multi-concept customization in text-to-image diffusion models with regionally controllable sampling."
"We analyze the return landscape in deep reinforcement learning for continuous control, revealing that popular algorithms traverse noisy regions and introducing a distribution-aware optimization method that improves policy robustness by avoiding these regions."
"We propose an Aggregating & Decoupling SSL framework with a diffusion encoder and three decoders that improves performance across SSL, class-imbalanced SSL, UDA, and SemiDG by capturing distribution-invariant features and preventing overfitting to labeled or domain-specific data."
"We construct and analyze lexinvariant language models that use no fixed token embeddings and achieve near-standard perplexity by relying solely on contextual co-occurrence, demonstrating effective in-context deciphering and improved reasoning, and showing potential for regularizing conventional LMs."
Aligning the global structure of neural network representations with human similarity judgments improves few-shot learning and anomaly detection performance while preserving local representational structure.
B4B is an active defense for MLaaS APIs that thwarts model stealing by adaptively limiting representation utility based on users' embedding space coverage and individually transforming user representations to hinder aggregation by adversaries.
"We propose a generalizable 3D segmentation framework using implicit representations and multi-view features with a soft voting and visibility mechanism, achieving performance comparable to scene-specific methods using only 2D supervision."
"We propose a three-stage hybrid RL algorithm that efficiently combines offline and online data without reward information during exploration, achieving better sample complexity than pure offline or online RL, under a new notion of single-policy partial concentrability."
"We derive new generalization error bounds for representation learning, based on the Minimum Description Length and multi-letter relative entropy, that are non-vacuous for deterministic encoders and apply to Information Bottleneck methods."
Robust temporal difference learning with dynamic gradient clipping achieves provably efficient policy learning under heavy-tailed reward distributions with finite (1+p)-th moments.
"We extend diffusion models to infinite-dimensional non-Euclidean spaces with geometric priors and equivariant neural networks, enabling symmetry-preserving generation of complex scalar and vector fields on real and synthetic data."
"DEMIPL improves multi-instance partial-label learning by representing bags as single vectors with disambiguation attention and a momentum-based label selection strategy, outperforming existing methods on benchmark and real-world datasets."
"We propose learning a vision proxy directly from unlabeled vision data using a text proxy to improve zero-shot transfer, theoretically justifying and refining this process to boost accuracy from 77.02% to 80.21% on ImageNet within one minute."
We propose a (pseudo)-Gibbs sampling method with moment matching that enables effective sampling from the true data distribution using noisy models trained by Denoising Score Matching.
"We propose a method that predicts mass spectra by decoding molecular subformulae as ordered multisets using a prefix tree, avoiding combinatorial explosion and rigid constraints."
"We propose an online method that uses text-to-image and depth models to generate 3D-consistent long-term videos from text and camera poses, enabling diverse scene synthesis via a progressively built unified mesh."
"Under log-concavity of the context distribution, adversarial linear contextual bandit regret can be bounded by $\tilde O(K\sqrt{d V_T})$ or $\tilde O(K\sqrt{d L_T^*})$, improving over the worst-case $\tilde O(\sqrt{KdT})$ when the environment is benign, using a truncated exponential weights algorithm."
"We introduce the Ω-RIP condition and prove, under this and an improved incoherence assumption, that the matrix sensing over graphs problem satisfies the strict saddle property with tight bounds, ensuring polynomial-time convergence for saddle-avoiding methods."
We introduce social-attribute-aware group fairness metrics and a multi-objective training algorithm for recommendation systems that balances direct and social utility while maintaining accuracy.
LBP-WHT is a low-rank backpropagation method that reduces computation and improves accuracy during ViT fine-tuning.
"We propose StarSSE, a modified Snapshot Ensemble method that better explores the pre-train basin during fine-tuning to improve ensemble performance without degrading transfer learning benefits."
We propose a method to extract a reward function from the difference between two decision-making diffusion models and demonstrate its effectiveness in learning reward functions for navigation and image generation tasks.
"We introduce robust policy gradient (RPG), a policy-based method that efficiently computes the robust policy gradient for rectangular robust MDPs using a closed-form worst occupation measure and rank-one kernel perturbation, matching the time complexity of standard policy gradients."
"We show that a simple feature mapping enables deep models to outperform traditional non-deep models on molecular property prediction tasks, including COVID-19 and activity cliff datasets."
We combine pretrained encoder-decoder language models with latent diffusion in their latent space to achieve more effective language generation than prior diffusion language models.
We propose a robust low-rank training method that maintains low-rank weights with orthonormal constraints to improve adversarial robustness without sacrificing accuracy or increasing computational cost.
"We derive and analyze posterior contraction rates for variational Bayesian Gaussian process priors with inducing variables in linear inverse problems, showing minimax-optimal rates for ill-posed problems such as the heat equation and Radon transform."
"BoundaryDiffusion enables efficient, lightweight, learning-free semantic control of frozen pre-trained denoising diffusion models by guiding the denoising trajectory toward critical semantic boundaries in the latent space."
"We show that in a class of zero-sum multi-agent Markov games with pairwise interactions described by a state-dependent graph, approximate Nash equilibria can be efficiently computed by reducing the problem to computing coarse-correlated equilibria."
"Boltzmann Tree Search (BTS) and Decaying ENtropy Tree-Search (DENTS) improve exploration in Monte-Carlo Tree Search while maintaining alignment with the original objective, outperforming Maximum ENtropy Tree-Search on benchmarks like Go."
"FreD is a frequency-domain dataset distillation method that efficiently synthesizes small datasets by optimizing selected frequency components, outperforming spatial-domain baselines and improving existing distillation methods."
"We show that in alternating Online Linear Optimization, regret can be reduced to $\mathcal{O}((\log n)^{4/3} T^{1/3})$ for the simplex and $\mathcal{O}(\log T)$ for two actions, invalidating the standard $\Omega(\sqrt{T})$ lower bound."
"We propose Student-Label Mixing (SLaM), a theoretically grounded method for knowledge distillation with unlabeled data that improves student model performance and yields new convergence and sample complexity results."
"CVV-Pro is an online algorithm achieving $\sqrt{T}$ regret and $1/\sqrt{T}$ convergence to a slowly time-varying feasible set using local linear approximations, without requiring knowledge or optimization over the entire feasible set."
We present a differentially private linear regression algorithm robust to adversarial corruption that achieves optimal sample complexity with linear runtime via full-batch gradient descent and adaptive clipping.
"We propose interactive global adversarial training (iGAT), which, guided by a new error theory showing provable 0-1 loss reduction, selectively distributes adversarial examples and regularizes base classifiers to boost ensemble adversarial defense performance by up to 17% on CIFAR10 and CIFAR100 under white- and black-box attacks."
"PackQViT is an activation-aware, fully sub-8-bit quantization-aware training framework for Vision Transformers that improves accuracy over prior 8-bit methods and achieves significant speedups on mobile CPUs via integer-only operations and optimized hardware mapping."
"We propose NHDE, a neural heuristic with diversity enhancement for multi-objective combinatorial optimization that uses indicator-enhanced reinforcement learning, heterogeneous graph attention, and a multiple Pareto optima strategy to generate more diverse Pareto solutions, outperforming decomposition-based methods and being applicable to existing neural MOCO approaches."
"We propose conformal arm set-based algorithms for stochastic contextual bandits that balance cumulative and simple regret with near-optimal guarantees, but show no algorithm can simultaneously achieve both optimal cumulative regret and instance-dependent simple regret."
We propose an active learning method for species range estimation that leverages transfer learning from large crowdsourced datasets and outperforms existing methods using only a fraction of the data.
"We prove that weight sharing and locality in CNNs break different symmetries, enabling efficient learning of sparse functions with logarithmic sample complexity in input dimension, whereas locality alone requires linear samples and fully-connected networks require quadratic samples."
"We unify concept extraction in neural networks as dictionary learning and importance estimation as attribution, enabling new evaluation metrics, leveraging modern attribution methods, and introducing a visual tool to reveal shared classification strategies across ImageNet classes."
"We propose a post-hoc method, Post-hoc Bias Mitigation (PBM), that reduces bias in text-based image retrieval without sacrificing performance, outperforming existing methods on real-world and large-scale datasets."
"SPF is a simple, frequency-domain method that predicts Fourier-transformed future state sequences to learn expressive representations, improving sample efficiency and performance in deep reinforcement learning."
We propose a non-Euclidean geometry-based decorrelation objective for Graph-based Collaborative Filtering that prevents embedding space degradation and improves recommendation for unpopular items.
"We present a $(1+\epsilon)$-approximate, near-linear time algorithm for estimating Chamfer distance between high-dimensional point sets, and show that sub-quadratic time algorithms for approximating the mapping are unlikely."
"We propose a dataset condensation plugin that matches datasets in a low-dimensional manifold using low-rank matrices, improving condensation efficiency, performance on downstream tasks, and mitigating catastrophic forgetting in continual learning under memory constraints."
"Convolutional neural networks, adapted as convolutional neural operators (CNOs), can accurately approximate PDE solution operators and outperform baselines on diverse PDE benchmarks."
The paper proposes a minimax-based pre-training method to achieve downstream-task robustness by improving worst-case performance across related downstream tasks.
"We prove that interpretability, efficiency, and consistency cannot all be achieved simultaneously and introduce polynomial-based methods that substantially reduce interpretation error compared to existing techniques."
"Shallow ReLU networks with vanishing weight decay converge to the minimum norm interpolant, and optimization algorithms exhibit an implicit bias toward such interpolants."
"Birder is a 1-bit adaptive optimizer with hierarchical all-reduce that achieves comparable inference accuracy to Adam/SGDM and up to 6.3× training speedup on BERT-Base, without warmup or heavy computation overhead."
"We propose a multi-agent reinforcement learning method that generates diverse, adaptable conventions via self-play reward maximization and adversarial cross-play minimization, enabling strong generalization and human-level performance in collaborative games."
"We propose delayed stochastic algorithms whose convergence rates depend on expected delay rather than maximum delay, and for composition weakly convex problems, we introduce a method where delays only impact higher-order terms, with robustness and delay-insensitive rates achievable via a safeguarding step."
We derive ranking-based loss functions for optimizing strictly optimal heuristics in forward search and show experimentally that directly optimizing h* is unnecessarily difficult.
"We introduce NOS, a gradient-based guidance method for discrete diffusion models enabling sequence-level protein design with LaMBO-2, which achieves high expression and binding in antibody optimization under practical constraints."
"We propose algorithms with provable guarantees for maximizing inter-group clustering criteria—minimum spacing and minimum spanning tree spacing—under both unconstrained and minimum-size constraints, and demonstrate their effectiveness empirically."
"We propose the Energy Transformer, an attention-based architecture that minimizes a designed energy function for effective representation learning, demonstrating strong performance on image completion, graph anomaly detection, and classification tasks."
"We propose a non-iterative marginal inference algorithm for probabilistic graphical models that constructs a sequence of linked clique tree forests with bounded clique sizes and achieves consistent marginals for Bayesian networks under topological order, offering better or comparable accuracy with smaller runtimes than existing methods."
"Graph contrastive learning exhibits distinct behaviors from visual contrastive learning due to the inductive bias of GNNs, necessitating careful consideration of architecture and sample selection when designing GCL methods."
"We propose a modified unadjusted Langevin algorithm using nonequilibrium thermodynamics and sequential Monte Carlo to efficiently estimate the cross-entropy gradient for training energy-based models, outperforming contrastive divergence on Gaussian mixtures and image datasets."
We show that adding a simple randomization step to deterministic voting rules can improve efficiency while maintaining explainability.
"We propose MCUFormer, a hardware-algorithm co-optimized method that enables vision transformer deployment on microcontrollers with ≤320KB memory, achieving 73.62% ImageNet top-1 accuracy via NAS-based architecture search and memory-efficient inference operators."
We propose a simulation-based framework for the infinite-horizon restless bandit problem that achieves O(1/√N) optimality without requiring the uniform global attractor property in continuous time or with a weaker assumption in discrete time.
"We propose novel random feature approximations for the Tanimoto kernel that enable scalable similarity computation for large molecular datasets and extend the kernel to real-valued vectors, with theoretical guarantees and empirical validation on molecular tasks."
"We propose a distance-based regularization for graph posterior networks to improve uncertainty quantification in interdependent node-level classification, outperforming state-of-the-art methods on OOD and misclassification detection."
"HEDNet is a hierarchical encoder-decoder network that improves 3D object detection in point clouds by capturing long-range spatial dependencies, outperforming previous state-of-the-art methods on Waymo Open and nuScenes with competitive efficiency."
"We propose ME_MaBiD, a two-sample test using multiple Mahalanobis kernels and a bi-directional hypothesis to detect local significant differences by partitioning the embedding space and controlling error rates."
"ScoreOpt is a test-time adversarial defense that optimizes adversarial samples toward clean data using score-based priors, achieving superior robustness and speed over existing diffusion-based methods."
"We propose Diff-Instruct, a data-free framework that uses pre-trained diffusion models to instruct the training of arbitrary differentiable generative models by minimizing an Integral Kullback-Leibler divergence."
Bridge3D improves 3D object detection and semantic segmentation by using foundation model features and masks to bridge domain gaps and enhance representation learning.
"SeqRank improves feedback efficiency in RLHF by using sequential preference ranking with two trajectory comparison strategies, yielding at least 39.2% higher efficiency and better task performance than baseline methods."
"We introduce a stochastic extrapolation technique that reduces bias in conditional stochastic optimization, improving sample complexity for both general and finite-sum settings."
GraphSplineNets is a deep learning method that accelerates physical system forecasting by reducing grid size and iteration steps using differentiable orthogonal spline collocation and adaptive spatial sampling.
"We prove that in approximating a large shallow erf-activated teacher network by a smaller student network under Gaussian input, the optimal configuration has n–1 student neurons copy n–1 teacher neurons and the nth averages the rest, and show similar results for ReLU."
"RandomCoordinateCut achieves the optimal $O(\log k)$ competitive ratio for explainable $k$-medians in $\ell_1$, matching the lower bound."
"Mix-IRLS is a simple, fast algorithm for mixed linear regression that excels on imbalanced and other challenging data settings, offering theoretical and empirical advantages over existing methods."
"The paper presents a linear bandit algorithm achieving Nash regret bounds of $O\left(\sqrt{\frac{d}{T}} \log(T |{\cal X}|)\right)$ for finite arms and $O\left(\frac{d^{5/4}}{\sqrt{T}} \log T\right)$ for general arm sets under non-negative, sub-Poisson rewards, using successive elimination with tailored concentration and optimal design."
"We present adversarial attack strategies that force OLTR algorithms to repeatedly select a target item with sublinear cost, demonstrating vulnerabilities across multiple OLTR models."
"We test large language models on a programmable synthetic dataset to evaluate generalization in deductive reasoning across proof complexity and rule types, finding that LLMs generalize to compositional proofs but struggle with longer proofs and require explicit demonstrations for certain proof strategies."
"We propose Rank-1 Lattice Targeted Sampling (RLTS), a fast, scalable black-box optimization method with closed-form lattice construction and O(n log n) complexity for GP training, inference, and targeted sampling, improving query efficiency on high-dimensional problems and LLM prompt tuning."
GATr is an E(3)-equivariant Transformer using projective geometric algebra to efficiently and scalably model diverse geometric data across multiple domains.
Label-destroying augmentations can outperform label-preserving ones in foundation model learning by enabling diverse feature learning across multiple downstream tasks.
"We propose an optimal transport-based adaptive image-mixing method that generates semantically meaningful synthetic samples for minority classes in class-imbalanced datasets, improving long-tailed classification performance."
"We propose a federated, multiply-robust, and privacy-preserving method using transfer learning to estimate causal effects from multi-site data, addressing covariate shift and mismatch, and demonstrate its efficiency and robustness in estimating the effect of PCI on hospitalization duration for AMI patients using CMS data."
"A simple recursive criticize-and-improve prompting scheme enables large language models to perform computer tasks from natural language instructions with few demonstrations and without task-specific rewards, outperforming prior supervised and reinforcement learning methods on MiniWoB++."
"The paper identifies data heterogeneity as the main barrier to achieving both fairness and robustness in distributed learning, proposes H-nobs with a fairness-promoting objective and norm-based screening for certified guarantees, and provides theoretical convergence analyses while empirically evaluating the trade-off between robustness and fairness."
"Transformers can learn to implement multiple iterations of preconditioned gradient descent on random linear regression instances through training, as shown by theoretical analysis of their loss landscape."
"Pre-trained neural network layers exhibit diffuse redundancy, enabling subsets of neurons to perform downstream tasks nearly as well as the full layer, with the degree of redundancy depending on pre-training conditions and task."
"We derive non-asymptonal learning bounds for Koopman eigenvalues and eigenfunctions in time-reversal-invariant stochastic dynamical systems using EDMD and RRR, showing similar variance but larger EDMD bias and explaining spurious eigenvalues."
PromptTPP integrates a continual learning approach with neural temporal point processes using a prompt-based memory to effectively learn shifting event streams without storing past data.
"We introduce the effective horizon, a new MDP complexity measure that better predicts deep RL performance than prior sample complexity bounds by capturing when optimal actions align with greedy choices under a random policy."
"We introduce a new regularization and network that stabilize implicit neural shape representations, enabling more precise shape and topology reconstruction than state-of-the-art methods."
"We derive mild conditions on the data-generating process and model architecture that ensure compositional generalization, and validate them empirically."
"We propose Flow Factorized Representation Learning, a generative model that learns efficiently structured, composable, and generalizable latent representations by defining distinct latent probability paths for input transformations via optimal transport, achieving better disentanglement and equivariance than existing methods."
"We prove that in 2-layer ReLU networks, overfitting transitions from tempered in low to benign in high dimensions, showing input dimension critically determines overfitting type."
"NESDE is a neural stochastic differential equation algorithm that enables robust, individualized, and policy-generalizable dosing predictions under high noise and limited medical data, with fast closed-form solutions and simulated environment applications."
"We theoretically analyze the expressive power of equivariant GNNs for node prediction tasks on large graphs, showing how input features and positional encodings expand the function space and guiding practical normalization improvements."
"We propose PCFG-NAT, a non-autoregressive Transformer that uses probabilistic context-free grammar to improve translation quality and explainability compared to autoregressive models."
"In two-layer ReLU networks trained with gradient descent and hinge loss on noisy, linearly separable binary data with flipped labels, we show that depending on the clean data margin, training yields either benign overfitting, overfitting, or non-overfitting, characterized by distinct loss and generalization behaviors and by two phases of neuron dynamics analyzed via clean versus corrupt sample updates."
We address label noise by inferring invariant content factors across style variations using data augmentation and regularization to improve classification with hard examples.
We show that achieving O(√(K \tilde{L} T)) dynamic regret in the K-armed dueling bandits problem under significant preference shifts is impossible for Condorcet and SST distributions but possible for SST ∩ STI.
"AlberDICE is an offline multi-agent reinforcement learning algorithm that performs centralized, alternating single-agent training to avoid out-of-distribution joint actions and converges to Nash policies with superior performance on standard benchmarks."
"Markov Neural Processes extend Neural Processes with flexible, consistent Markov transitions in function space, outperforming baselines on multiple tasks."
"HYPO is a reinforcement learning algorithm that uses a small number of imperfect demonstrations to accelerate online learning by training an offline guider policy that guides efficient exploration, outperforming baselines on sparse-reward and real-world tasks."
"We prove that popular dynamic programming decompositions for CVaR and EVaR in risk-averse MDPs are inherently suboptimal due to violated saddle-point properties, whereas the decomposition holds for Value-at-Risk."
"We propose an ODE-based recurrent model integrated with model-free reinforcement learning to effectively solve partially observable Markov decision processes, demonstrating robustness to irregular observations."
Residual Q-learning enables policy customization by balancing imitation of a prior policy with downstream task requirements without requiring knowledge of the prior policy's reward or value function.
"EchoFusion directly fuses raw radar spectral features with other sensor data in Bird's Eye View space, outperforming existing radar-based methods and approaching LiDAR performance on the RADIal dataset."
"We propose and analyze explore-then-commit algorithms for high-dimensional linear contextual bandits without sparsity, leveraging overparameterized models and effective rank, and show optimal rates with adaptive balancing of exploration and exploitation."
We present a policy iteration method using a large language model that learns RL tasks from scratch via in-context learning without expert demonstrations or gradient updates.
"DPT is a simple semi-supervised training strategy that uses a classifier and diffusion models to generate pseudo-labels and pseudo-images, achieving state-of-the-art results in both generation and classification with very few labels per class."
"We propose a selective, model-driven question decomposition method for VQA that improves accuracy, especially on medical and zero-shot tasks."
"PIPS is a machine learning method that samples molecular transition paths without requiring collective variables, demonstrating success on Alanine Dipeptide, Polyproline, and Chignolin."
VisorGPT explicitly learns the visual prior from discretized visual locations via generative pre-training to enable customizable sampling and demonstrates effectiveness in modeling and generalizing visual priors.
"Width primarily affects neural network behavior early in training on simple tasks but introduces distinct finite-width biases later and on harder tasks, which can be partially mitigated by ensembling but at the cost of performance compared to a single wide network."
We propose a sample-efficient reinforcement learning method for mixed stochastic–pseudo-stochastic systems that uses augmented data to reduce sample complexity and accelerate learning.
"Three Towers (3T) enhances contrastive vision-language learning by combining pretrained image embeddings with trainable towers, improving retrieval and classification performance over existing baselines and matching or surpassing LiT depending on the pretraining dataset."
"We provide optimal or near-optimal sample complexity bounds for pure exploration in infinitely many bandit arms under a PAC-style guarantee, characterizing both the expected and fixed sample complexity in terms of the arm distribution's Fisher information."
"We discover an inverse scaling law in CLIP training that enables high-accuracy models with drastically reduced computational resources, achieving up to 83.0% zero-shot ImageNet-1k accuracy with ~33x speedup."
"The authors propose quantizing the latent space with learnable scalar codebooks and high weight decay to induce disentangled representations, which, when combined with new information-theoretic disentanglement metrics, yields more modular and explicit latent variables than prior methods without sacrificing reconstruction quality."
"We derive an optimal weight function using calculus of variations to reduce bias in kernel density estimates for density ratios, improving prediction posteriors and information-theoretic measures."
"DEMOTE is a neural tensor decomposition method that leverages dynamic embeddings, graph diffusion, and reaction processes to model sparse, temporally structured multiway data."
"We systematically evaluate federated learning for visual recognition across diverse architectures and datasets, demonstrating that thoughtful architectural design significantly improves FL performance and narrows the gap with centralized training."
"We propose a differentiable regularizer based on a lower bound to the data's distance from the decision boundary, enabled by a scalable method for differentiable Lipschitz constant bounds, improving adversarial robustness and outperforming state-of-the-art methods on MNIST, CIFAR-10, and Tiny-ImageNet."
"We establish conditional lower bounds showing that achieving subquadratic time for low-rank approximation of entrywise transformed matrices requires both symmetric factors and PSD kernels, and provide matching upper bounds and extensions to matrix-vector products."
We propose replacing element-wise activations with affine-constrained convolutions and channel-wise sort pooling in neural networks to enforce normalization-equivariance without sacrificing performance.
"We propose algorithms for a cascading bandits setting with frequency-dependent user abandonment and delayed feedback, and empirically validate them on the AmEx dataset."
"SNAP is a deep network that learns detailed, semantically rich 2D neural maps from raw ground-level and overhead images, outperforming traditional methods in localization and enabling data-efficient semantic scene understanding."
"We develop an interpretable recurrent spiking neural network fitted to simultaneous neural and behavioral recordings using optimal transport to match trial-to-trial variability, revealing both task-relevant and irrelevant movement modes."
"We propose a Shapley value-based method for explaining Gaussian processes that yields stochastic, axiomatic-compliant explanations with tractable covariance and supports predictive explanation via a Shapley prior."
"GradOrth detects out-of-distribution data by measuring the norm of the gradient's projection onto the lower-rank subspace important for in-distribution data, achieving up to 8% lower false positive rate at 95% true positive rate than state-of-the-art methods."
"We propose a method to infer latent global force fields from observed dynamics by combining SE(3)-equivariant graph networks for local interactions with neural fields for global effects, and demonstrate its effectiveness on charged particles, traffic, and n-body systems."
"We propose an efficient batched algorithm for large-action contextual linear bandits using a linear optimization oracle, achieving $\tilde{O}(\sqrt{T})$ regret and optimal batch counts with a novel soft elimination approach."
"PrObeD improves object detection by finetuning detectors on images encrypted with learned templates, enhancing performance on both generic and camouflaged datasets."
"We propose a scale-equivariant convolutional neural network with discrete, anti-aliased down-scaling and Fourier layers that achieves zero equivariance-error and competitive classification performance on MNIST and STL-10."
"SMILE modifies maximum likelihood estimation to promote richer, more detailed image captions by preventing conciseness-driven optimization."
"Critical band masking reveals that humans use a one-octave-wide spatial frequency channel for object recognition, while neural networks use broader channels that become even wider with adversarial training, indicating a fundamental difference in frequency selectivity between humans and networks."
"We present a fully polynomial-time tester-learner for halfspaces that achieves near-optimal error on any labeled distribution satisfying a Poincaré inequality, using SOS-based hypercontractivity checks."
DEPS is an LLM-based interactive planning approach that improves multi-task agent performance in Minecraft and other domains by refining achievable sub-goal ordering.
"We propose IDRNet, a novel intervention-driven relation network that models contextual pixel relations via deletion diagnostics and achieves improved dense prediction performance on segmentation benchmarks."
"We propose using Transformer-based point cloud methods to extract features from raw circuit designs for efficient congestion and DRC violation prediction, achieving state-of-the-art results without preprocessing."
"We derive novel, time-independent information-theoretic generalization bounds for SGLD under smoothness and dissipativity, using KL divergence and sub-exponential loss to eliminate step size dependence and improve excess risk bounds."
"StyleDrop is a method that fine-tunes very few parameters in text-to-image models to faithfully synthesize images matching a specific style from as little as a single example, outperforming existing style transfer methods."
"The paper proposes Personalized Empirical Risk Minimization (PERM), a distributed learning paradigm that learns distinct models for each client by personalizing loss aggregation based on estimated data distribution discrepancies, enabling scalable and resource-aware personalized learning without strict model or resource constraints."
"We propose a disentanglement framework using three Gaussian inference layers with strengthened transition models to extract speaker and content representations from speech, achieving significant EER and minDCF reductions on VoxCeleb and SITW without extra data or training."
"We generalize importance weighting to handle all distribution shift cases by decomposing test support into in- and out-of-training parts and weighting only the in-training portion, outperforming standard importance weighting on partial and novel support shifts."
"We derive an information-theoretic analysis of VICReg that links its objective to mutual information optimization and generalization, leading to improved SSL methods."
We propose a method to generate adversarially robust embeddings for tabular data with categorical features and transfer this robustness to tree-based classifiers without adversarial training.
"We propose algorithms that reduce online label shift adaptation to online regression and achieve optimal dynamic regret without prior drift knowledge, showing 1–3% accuracy improvements in simulated and real-world experiments."
"VoxDet is a 3D voxel-based framework that improves unseen instance detection under occlusion and pose variation by aggregating multi-view 2D features into 3D templates and matching query voxels, outperforming 2D baselines on novel benchmarks."
"Graph attention mechanisms cannot prevent oversmoothing and still lose expressive power exponentially, as shown by analyzing attention-based GNNs as nonlinear time-varying dynamical systems."
We present an $\widetilde{O}(m\epsilon^{-1})$-time sketching algorithm for estimating effective resistances in expander graphs with $(1\pm\epsilon)$-accuracy and improved conditional lower bounds of $\widetilde{\Omega}(n^2\epsilon^{-1/2})$ for such problems.
"We reframe adversarial attacks on deep neural networks using Wasserstein distributionally robust optimization, enabling non-uniform input perturbations and providing new first-order attacks and asymptotic accuracy bounds with out-of-sample guarantees, validated on standard image datasets."
"We propose LABOR, a graph sampling algorithm that reduces vertex sampling by up to 7 times and allows up to 112 times larger batch sizes compared to Neighbor Sampling, without sacrificing estimator variance or convergence speed."
"We propose Blockwise Parallel Transformer (BPT), a memory-efficient architecture enabling training on sequences up to 32 times longer than vanilla Transformers by blockwise computation of self-attention and feedforward fusion."
"We propose decomposing the value function into permanent and transient components updated at different timescales to improve continual reinforcement learning, with theoretical and empirical support."
ReContrast is a novel epistemic unsupervised anomaly detection method that jointly optimizes encoder and decoder on the target domain to reduce pre-training bias and improve anomaly detection across diverse image domains.
"We introduce Greedy Rejection Coding (GRC), a general and efficient relative entropy coding algorithm with proven optimal expected codelength and near-optimal runtime for unimodal continuous distributions, and demonstrate its effectiveness in a compression pipeline on MNIST."
"We propose a multi-armed bandit-based algorithm that jointly selects data sources and performs fair online allocation without observing protected attributes, achieving O(√T) regret despite correlated fairness penalties."
"We present Fast and Forgetful Memory, a memory model for reinforcement learning that replaces RNNs, achieves higher rewards without hyperparameter changes, and trains two orders of magnitude faster with logarithmic time and linear space complexity."
"We propose GuidedDiffTime, a diffusion-based method for constrained time series generation that efficiently samples realistic series without retraining and achieves up to 92% lower carbon footprint compared to existing deep learning methods."
HopCPT is a novel conformal prediction method for time series that leverages temporal dependencies and outperforms state-of-the-art methods on real-world datasets.
We propose a method that aligns diffusion model sampling with physics-based optimization trajectories to improve precision and efficiency in data-limited structural design tasks.
"DAW addresses the trade-off in semi-supervised semantic segmentation pseudo-label selection by modeling confidence distributions and using a hard step function as the optimal weighting, improving generalization with distribution alignment and outperforming state-of-the-art methods on benchmarks like mitochondria segmentation."
"We propose SOC, a method that improves referring video object segmentation by modeling video-level visual-linguistic alignment and enhancing temporal coherence."
"We propose a fairness-aware continual learning framework for semantic segmentation that mitigates catastrophic forgetting and background shift via a novel Prototypical Contrastive Clustering loss and achieves state-of-the-art performance with improved fairness on ADE20K, Cityscapes, and Pascal VOC."
"When the entries of input matrices are below a certain threshold ($B = o(\sqrt{\log n})$), attention computation can be done in nearly linear time, but above this threshold ($B = \Theta(\sqrt{\log n})$), it requires quadratic time under standard assumptions."
We propose a one-pass distribution sketch that efficiently measures data heterogeneity in federated learning and improves client selection and cold-start integration.
"We propose a unified, assumption-light analysis of first-order gradient methods for stochastic optimization and variational inequalities under Markovian noise using randomized batching, achieving optimal dependence on mixing time and providing matching lower bounds for strongly convex problems."
"We propose an SE(3)-and permutation-equivariant coupling flow that enables fast sampling and density estimation for Cartesian-based physical systems, outperforming existing methods on molecular datasets and enabling direct modeling of the Boltzmann distribution from energy functions."
"Semantic Guidance (SEGA) enables flexible, subtle semantic control over text-to-image diffusion models by interacting with the diffusion process, generalizing to various architectures and tasks."
"MeCo is a novel zero-cost proxy for neural architecture search that uses a single random forward pass to estimate architecture quality without training, data, or labels, and enables efficient NAS with improved accuracy."
"We propose h-GPI, a model-based extension of GPI that improves zero-shot transfer in RL by leveraging approximate environment models and shows better performance and robustness as the planning horizon increases, with theoretical bounds and empirical validation under approximation errors."
We improve high probability generalization bounds for stochastic mirror descent with quadratically bounded losses by analyzing a novel supermartingale and removing the log T or poly T factors present in prior work.
"We establish the first polynomial-time convergence guarantees for the probabilistic flow ODE implementation with a corrector step in score-based generative modeling, achieving improved $O(\sqrt{d})$ dimension dependence over prior $O(d)$ results for DDPM under data distribution smoothness."
"FreeSel is a single-pass, inference-only data selection method that efficiently identifies the most informative samples using semantic patterns from a general-purpose model, achieving over 530x speedup compared to active learning methods."
"LIMA shows that pretraining captures almost all knowledge in large language models, and only a small amount of supervised fine-tuning is needed to achieve high-quality, instruction-following outputs comparable to or better than models trained with reinforcement learning or human feedback."
"PromptIR is a prompt-based, all-in-one image restoration method that generalizes to various degradation types and levels without prior corruption knowledge."
We propose and validate a decentralized method integrated with All-Reduce that reduces worker compute time variability to mitigate scalability limitations caused by straggling in synchronous distributed DNN training.
"The authors propose Graph Mixture of Experts (GMoE), a GNN variant that lets nodes adaptively select expert modules for diverse graph structures, improving performance on graph benchmarks without increasing computational cost."
We analytically link the temporal scaling group to recurrent circuit dynamics by modeling TS-equivariant neuronal population responses with control inputs that specify scaling factors and time reversal.
"We propose a corruption-robust offline RL algorithm with a suboptimality bound that degrades by at most an additive term proportional to the corruption level times a coverage-dependent factor, matching known lower bounds for linear MDPs."
"Adversarial training of linear regression yields the minimum-norm interpolator in the overparameterized regime and can be equivalent to parameter shrinking methods in the underparameterized regime, with the adversarial radius determining equivalence to ridge or Lasso regularization."
We propose a generative machine learning-based neural modulator that reduces NAND flash memory errors and extends device lifetime by modeling the memory channel with a cGAN and optimizing programming operations.
We propose using the probability of necessary and sufficient causes (PNS) to improve out-of-distribution generalization and provide a theoretically grounded method validated on synthetic and real-world benchmarks.
"MAG-GNN uses reinforcement learning to select a small, expressive subset of subgraphs instead of enumerating all subgraphs, achieving strong performance with much lower computational cost."
"We propose ProjUnit, a computationally and communication-efficient algorithm for locally private high-dimensional mean estimation that achieves near-optimal error via random low-dimensional projection."
"We propose Orthogonal Finetuning (OFT) and Constrained Orthogonal Finetuning (COFT), which provably preserve hyperspherical energy to better adapt text-to-image diffusion models to subject-driven and controllable generation tasks, outperforming existing methods in quality and convergence speed."
"We propose Mask-aware Fine-tuning (MAFT), a method that fine-tunes CLIP to be responsive to mask proposals, significantly improving zero-shot segmentation performance on COCO, Pascal-VOC, and ADE20K."
We propose a shared low-rank block-diagonal preconditioner that reduces the time and memory cost of adaptive second-order optimization while maintaining or improving performance compared to existing methods.
"We propose a method that decouples verb and object recognition using an object-agnostic verb encoder and a prompt-based object encoder with CLIP, enabling open-vocabulary action recognition and superior novel object recognition in egocentric video datasets."
"We analyze sample complexity and propose PEVIO for offline RL with options, showing improved finite-time convergence and performance under limited data or well-designed options compared to action-based offline RL."
CODA is a two-stage framework that improves open test-time domain generalization by compacting known class representations and disambiguating known-from-unknown classes during test time.
"DASpeech is a non-autoregressive direct speech-to-speech translation model that uses a two-pass architecture to generate high-quality translations at significantly faster speeds than autoregressive models, without relying on knowledge distillation or iterative decoding."
"RoML, a meta-algorithm that over-samples harder tasks, achieves robust meta-reinforcement learning by removing gradient bias and improving data efficiency without prior knowledge of test tasks."
"For one-hidden-layer ReLU networks with unidimensional data, the minimal norm interpolator's parameter norm equals the weighted total variation of the second derivative, with the weighting factor disappearing if bias norm is unregularized, thus bias regularization induces sparsity."
"We propose T2T, a training-to-testing framework that combines generative modeling and gradient-based search to significantly improve solving of Combinatorial Optimization problems like TSP and MIS."
"The authors introduce Attentive Transfer Entropy, an attention-based generalization of transfer entropy that enables accurate reconstruction of sparse, weakly coupled dynamical networks from data."
"ALGO is a framework that generates algorithmic programs with LLM-created oracles to guide and verify correctness, significantly improving code generation performance on algorithmic problems compared to existing models."
Stochastic Gradient Descent recovers the unknown direction θ* with constant probability in the high-dimensional regime for single-index models with known φ under mild assumptions beyond the Gaussian setting.
"SyMat is a symmetry-aware deep learning framework for generating periodic materials by encoding atomic types, lattice parameters, and atom coordinates with invariance to material symmetries."
OPTIMA addresses nonstationary module contributions in layerwise distillation of multimodal models by formulating module selection as a nonstationary multi-armed bandit problem and using modified Thompson sampling to improve efficiency.
"We propose two optimal estimators for location under shift: one minimax-optimal for error with high probability, and one minimizing expected interval width subject to a coverage guarantee."
"NFTs, permutation-equivariant neural functional Transformers using attention over weight spaces, improve performance on weight-based tasks and enable a new method that boosts INR classification accuracy by up to +17%."
"Scissorhands reduces KV cache memory usage by up to 5× (and up to 20× with 4-bit quantization) by selectively retaining pivotal tokens, without degrading model quality."
We propose a meta-learning framework for Privacy-Preserving Action Recognition that improves generalization to novel privacy attributes and attack models by simulating task shifts with disjoint support and query sets.
"Dual Teacher introduces dual temporary teachers that alternate in generating pseudo-labels to decouple student and teacher updates, improving semi-supervised segmentation performance with fewer labels and less training time."
SDVT improves meta-RL generalization to non-parametric tasks by decomposing them into reusable subtasks and training with virtual subtask compositions.
"We introduce ImageReward, a text-to-image human preference reward model trained on 137k expert annotations, and Reward Feedback Learning (ReFL), a direct tuning method that outperforms existing approaches for optimizing diffusion models using preference feedback."
"DIFUSCO, a graph-based diffusion framework, improves neural solving of Traveling Salesman and Maximal Independent Set problems, narrowing the performance gap with exact solvers."
"Diff-PGD is a diffusion-guided adversarial attack framework that generates realistic, effective adversarial samples with improved transferability and robustness to purification compared to traditional gradient-based methods."
"Megabyte, a multi-scale decoder architecture, enables efficient end-to-end modeling of very long sequences with sub-quadratic complexity and improved performance and scalability compared to standard transformers."
Neural Galerkin schemes using randomized sparse parameter updates per time step improve accuracy and efficiency for sequential-in-time training of time-dependent PDEs by preventing error accumulation and reducing computation.
"Lattice injects temporally-correlated noise into the policy's latent state, improving exploration and performance in high-dimensional, overactuated motor control tasks compared to unstructured noise."
"We introduce MMD-Fuse, a permutation-based two-sample test that combines MMD statistics over multiple kernels with data-dependent selection, offering improved power and theoretical guarantees without data splitting."
"We demonstrate that pointwise learning of Kohn-Sham charge densities enables combinatorial generalization to unseen element combinations in catalysts, reducing DFT convergence iterations by 13% compared to baselines."
"We propose an optimal slicing distribution for estimating mutual information in high-dimensional data, achieving better accuracy than uniform slicing methods."
"We introduce GenAgg, a parametrized aggregation operator that generalizes standard aggregators and improves GNN performance across tasks."
CaSED is a training-free method that uses a pre-trained vision-language model and an external vision-language database to perform vocabulary-free image classification by retrieving and selecting the most semantically relevant class from a large unconstrained semantic space.
"We propose a determinant-minimizing dictionary learning formulation with unit $\ell_1$-norm row constraints that guarantees global identifiability of the dictionary and coefficients under mild conditions, without requiring incoherence, and show efficient recovery via linearized-ADMM."
"We present a randomized clustering algorithm that achieves expected runtime $O(\mathsf{nnz}(X) + n\log n)$ for $k$-means on sparse $d$-dimensional data, with a $\widetilde{O}(k^4)$ approximation guarantee, and demonstrate improved practical cluster quality and new time–quality tradeoffs."
"DatasetDM generates diverse, annotated synthetic images using a diffusion model and a small set of labeled examples, enabling state-of-the-art performance on semantic and instance segmentation tasks with improved efficiency and flexibility compared to real data."
"We propose a probabilistic graphical model with stochastic gradient variational Bayes to infer ground-truth labelings in partial multi-label learning, outperforming state-of-the-art methods on multiple datasets."
Data concentration on low-dimensional subspaces enables robust classifiers with data-dependent guarantees.
"We propose FLAD algorithms with bandit-inspired exploration-exploitation that scale to large numbers of auxiliary datasets and achieve superior few-shot generalization, even outperforming much larger language models."
We propose a deep-learning-based method that reconstructs hidden volumes from under-scanning non-line-of-sight measurements faster and more accurately than existing methods.
"We propose a decoding method that generates action sequences likely under both language models and grounded robotic models, enabling LLMs to operate effectively in embodied, real-world settings."
"MEFT is a reversible, memory-efficient parameter-efficient fine-tuning method for large language models that achieves full fine-tuning performance with minimal trainable parameters and up to 84% reduced activation memory."
"LLM-pruner compresses large language models in a task-agnostic and data-efficient manner using structural pruning and LoRA fine-tuning, preserving zero-shot classification and generation capabilities."
"MACO improves feature visualization in deep networks by optimizing only the phase spectrum of images while keeping the magnitude fixed, producing more interpretable and natural visualizations and enabling quantitative evaluation without prior image models."
Glance-Focus generates dynamic event memories from videos and uses them with multi-level cross-attention to achieve state-of-the-art results on multi-event video question answering benchmarks.
"Randomly pivoted Cholesky generates efficient quadrature nodes in reproducing kernel Hilbert spaces, achieving high accuracy with less computational cost and adaptability to complex geometries."
"We propose a stability-penalty-adaptive (SPA) learning rate for FTRL that achieves sparsity-agnostic, best-of-both-worlds, and game-dependent adaptivity in multi-armed bandit problems, including the first sparsity-dependent BOBW algorithm and game-dependent BOBW in partial monitoring."
"We propose a description-conditioned paradigm that leverages large language models to generate rich object descriptions and uses context-sensitive queries, improving zero-shot object detection performance on LVIS and OminiLabel beyond GLIP and FIBER."
"We propose an information-theoretic framework (PID) to quantify redundancy, uniqueness, and synergy among modalities in multimodal tasks, introduce scalable estimators, and demonstrate its utility in analyzing datasets, evaluating models, guiding model selection, and informing real-world multimodal applications."
"The paper introduces a graphon-signal cut distance that enables a universal similarity measure and Lipschitz continuity analysis for message passing graph neural networks, yielding generalization and subsampling stability bounds."
"The paper introduces a novel Soft Uniform Block Pruning (SUBP) method that trains 1×N sparse CNNs from scratch with uniform block sparsity, improving efficiency and performance without relying on pre-trained dense weights."
"We present efficient algorithms with polynomial regret for prediction and simulation in piecewise affine systems under a weak smoothness assumption, measuring regret via Wasserstein distance."
"Swap Agnostic Learning is equivalent to swap variants of Omniprediction and Multicalibration for convex losses, despite a strong adversary."
"Conditional Adapter (CoDA) is a parameter- and inference-efficient transfer learning method that accelerates inference by 2x–8x with minimal accuracy loss across language, vision, and speech tasks."
"BC-PnP is a new plug-and-play method that jointly estimates unknown images and measurement operators in blind inverse problems using learned denoisers, with theoretical convergence guarantees and demonstrated effectiveness in MRI coil sensitivity estimation and blind deblurring."
"Score matching-based causal discovery methods show robust empirical performance under violated assumptions in observational iid data, and we provide theoretical insights and a new benchmark for algorithm stability and evaluation."
"We introduce a differentiable, exact, and efficient count loss for weakly-supervised learning from data with inferred class frequency constraints."
We propose perceptual adjustment queries (PAQs) for metric learning and develop a two-stage estimator with sample complexity guarantees for learning Mahalanobis distance from PAQ feedback.
We propose conformal meta-learners that provide valid and efficient predictive intervals for individual treatment effects by applying conformal prediction to existing CATE meta-learners.
We show that using unlabeled prior data with an optimistically-labeled reward model accelerates learning in sparse-reward reinforcement learning tasks.
"We introduce camouflaged data poisoning attacks that hide malicious data until unlearning triggers their activation, enabling clean-label targeted misclassification attacks that are effective even when models are retrained from scratch or using approximate unlearning."
"RandQL is a novel model-free algorithm that achieves improved regret bounds in episodic MDPs through learning rate randomization, without using bonuses."
"We propose an information-theoretic, post hoc saliency-based framework to identify which observed timesteps influence probabilistic multivariate time-series forecasts, enabling data-driven hypothesis generation about feature causal relationships."
"We propose a distribution mapping module that debiases pretrained generative models without retraining, producing semantically uniform outputs by sampling from a fair noise distribution."
"We develop a method to predict memorization of sensitive data by large language models before full training by extrapolating from lower-compute trials, and release code and data for reproducibility."
"We improve the objective perturbation mechanism for differentially private training, making it competitive with DP-SGD on unconstrained convex generalized linear problems through tighter privacy analyses and better computational methods."
"We propose and validate a framework that orders training data by skill dependencies, enabling more efficient learning and improved performance in language models with limited data."
"We propose a smoothed first-order Lagrangian method with theoretical convergence guarantees for nonconvex functional constrained optimization with nonconvex, nonlinearly coupled constraints, and demonstrate its effectiveness on bilevel optimization and hyper-data cleaning problems."
"We propose theoretically grounded configuration procedures for heuristic algorithms that maximize user utility rather than minimize expected runtime, providing empirical and theoretical guarantees with efficient runtimes."
"Competition among model providers can cause overall predictive accuracy to decrease or become non-monotonic with scale, even when individual models improve."
"We propose Easy Sample Matching Attack (ESMA), a generative targeted attack that improves transferability by perturbing easy samples in the target class's high-sample-density regions, achieving higher success rates and lower computational cost than state-of-the-art methods."
StableFDG introduces style-based learning and an attention-based feature highlighter to enable effective federated domain generalization under data-poor conditions where source and target domains differ.
We analyze and improve out-of-distribution adversarial robustness with theoretical insights and algorithms.
"We introduce differentiable kernel-based calibration metrics that improve calibration, sharpness, and decision-making in regression and classification by integrating calibration directly into empirical risk minimization, outperforming post-hoc recalibration methods."
"We propose a fast, guaranteed to converge EM-based method for optimal ridge regularization that outperforms LOOCV, especially with sparse covariates, and scales as O(min(n, p)) per iteration versus O(n min(n, p)) for LOOCV."
"SpanRL is the first computationally efficient, model-free algorithm for sample-efficient exploration in Low-Rank MDPs with general function approximation under a weak reachability assumption."
"We analyze the performance of regression-based conditional independence tests under model misspecification and introduce a robust variant, validating our results empirically."
"We propose OPGD, an online algorithm that efficiently finds Nash equilibria in decision-dependent bandit games with strategic coupling by leveraging online stochastic approximation and projected gradient descent."
We reformulate group distributionally robust optimization as a saddle-point problem and propose sampling-efficient algorithms that achieve near-optimal sample complexity and convergence rates under non-uniform sampling budgets.
"We study how and when a teacher LLM should provide natural language explanations to improve a student LLM's performance within a communication budget, finding that targeted, personalized interventions based on mental models enhance learning, while misaligned teachers can harm learning."
"A physics-informed deep learning framework corrects off-resonance blurring in MRI caused by non-Cartesian Spiral sampling using synthetic data and enables faster, motion-robust scans."
"SRTTA is an efficient test-time adaptation framework for image super-resolution that predicts the test image's degradation type and adapts the SR model via feature-level reconstruction using second-order degraded inputs, outperforming existing methods on diverse real-world datasets."
"We demonstrate that quantization performance cliffs are not inherent to scale but can be mitigated by optimizing training to reduce activation outliers, challenging the notion that such emergent phenomena are inevitable."
"We introduce the Optimized Kalman Filter (OKF), which, when parameters are optimized similarly to neural networks, can become competitive, revealing flaws in previous non-linear filtering comparisons and enabling straightforward replacement of KF in real systems."
"FLMR improves RA-VQA's knowledge retrieval by enhancing image representations and using multi-dimensional embeddings, boosting PRRecall@5 by 8% and achieving ~62% VQA accuracy on OK-VQA."
"We show that emergent abilities in large language models are often artifacts of nonlinear metrics and not fundamental changes in model behavior, demonstrating this by reanalyzing model performance with alternative metrics across multiple tasks and datasets."
"Constant-stepsize gradient descent for logistic regression on linearly separable data in the edge-of-stability regime converges to a max-margin direction while minimizing loss over the long term, but diverges catastrophically under the exponential loss."
"CD-GraB extends provably faster permutation-based example ordering from centralized to distributed SGD with linear speedup and negligible overhead, outperforming distributed random reshuffling."
We propose using mutual information as an augmented reward and a multi-agent policy iteration algorithm with approximate marginal distributions to enable robust zero-shot generalization to dynamic team compositions in cooperative multi-agent reinforcement learning.
"We establish a linear query lower bound for exact Nash equilibria in $K\times K$ matrix games and show that existing techniques cannot prove tight non-trivial lower bounds for approximate equilibria, while introducing a new method to achieve a $\tilde\Omega(\log(1/(K\epsilon)))$ lower bound for small $\epsilon$."
"HQA-Attack is a simple, effective black-box hard-label text attack framework that reduces perturbation and query count while increasing adversarial semantic similarity, outperforming existing baselines across multiple datasets and APIs."
"We introduce a neural network using signed distance function zero-level sets to provide a continuous, scalable representation of protein dynamics, enabling accurate interpolation, extrapolation, and analysis of large-scale motions."
TopoSRL is a self-supervised learning method for simplicial complexes that uses simplicial augmentation and a contrastive loss to capture higher-order topology and outperforms existing graph and supervised simplicial methods.
TEMPO is a bi-level optimization framework that enhances maximum-likelihood environment models with task-aware sample weighting to improve reinforcement learning performance across continuous and discrete control tasks.
"We propose a two-step Bayesian Optimization termination method that halts search when local regret in a detected convex region falls below a threshold, achieving up to 80% computational savings with minimal performance loss across diverse benchmarks."
"EmbodiedGPT is an end-to-end multimodal foundation model for robotics that, trained on a new planning dataset and using LLM-generated task features, significantly improves success rates on embodied control tasks compared to BLIP-2."
"Over-parameterization in neural networks affects privacy leakage via the expected squared gradient norm, which depends on initialization and increases or decreases with depth depending on the initialization scheme."
"We propose an optimization framework that removes label position bias from graph structure in GNNs, improving node classification and reducing bias."
"QuACK is a framework that accelerates gradient-based quantum optimization by combining Koopman operator theory with natural gradient methods, achieving over 200x speedup in overparameterized regimes."
Sequence modeling with Goal-Conditioned Predictive Coding (GCPC) improves goal-conditioned policy learning by effectively encoding future trajectories in offline reinforcement learning.
"We propose a multi-task supervised approach to learn disentangled representations from real-world data without direct access to latent factors, validated on image and text benchmarks under distribution shift."
"C-MCR connects pre-trained multi-modal contrastive representations without paired data by aligning them via overlapping modalities and semantic enhancements, achieving state-of-the-art results on audio-visual and 3D-language tasks without using paired data."
"SHAP-IQ is a theoretically grounded, efficient sampling-based method for approximating Shapley interaction indices that satisfy key axioms, with improved computational performance and variance estimation for feature attribution and interaction explanation."
We propose a multiply robust estimator for g-identifiable causal effects based on generalized multi-outcome sequential back-door adjustments and demonstrate its theoretical and practical validity.
"We introduce the Machine Personality Inventory (MPI), a standardized tool based on the Big Five to quantitatively assess and induce specific personalities in LLMs."
"We establish the discrete diffusion model's probability flow via optimal transport and propose a sampling method that produces more certain outcomes, validated on synthetic and CIFAR-10 datasets."
SRPO improves data efficiency and performance of context-based RL algorithms by regularizing policy optimization with a shared stationary state distribution learned from data with dynamics shift across homomorphous MDPs.
"We introduce KRaM, a distance metric learning objective that effectively erases specified concepts from distributed representations across diverse domains while preserving other information."
"We propose Natural Program, a natural language deductive reasoning format that enables language models to perform step-by-step, verifiable reasoning and improves correctness on complex reasoning tasks."
"We present a randomized algorithm and matching lower bound for the adaptive experts problem with memory constraints, showing near-optimal space–regret trade-offs and demonstrating robustness against adaptive adversaries."
"INVPROP efficiently computes over-approximations of input sets mapping to constrained outputs on GPUs, outperforming prior methods in precision and speed for neural network verification tasks."
"We improve the approximation guarantee for $\MMS$ allocation to $\XOS$ valuations to $3/13$ deterministically and propose a randomized allocation achieving $1/4$-$\MMS$ ex-ante and $1/8$-$\MMS$ ex-post, with an upper bound of $3/4$ on the ex-ante guarantee."
"GeoCLIP is a CLIP-inspired image-to-GPS retrieval model that encodes GPS coordinates via random Fourier features and hierarchical representations to enable continuous, accurate global geo-localization."
We propose a model-free off-policy evaluation method for POMDPs using future-dependent value functions and a minimax learning algorithm with PAC guarantees under sufficient information in histories and futures.
"We propose a reconstruction-based domain adaptation method that improves pose estimation on real-world data using only unlabeled target samples, achieving 8% higher PCK than prior methods, especially on challenging keypoints."
"Chameleon augments large language models with modular tools for compositional reasoning, improving performance on multi-modal knowledge-intensive tasks like ScienceQA and TabMWP."
"Text conditioning in diffusion models significantly contributes to data replication, and we propose caption randomization and augmentation to reduce this effect during training and inference."
"We algorithmically reinterpreted the ALIF model to enable efficient GPU parallelization, achieving over 50× speedup and accurate simulation of cortical neurons with sub-millisecond precision."
"We introduce Causal Component Analysis (CauCA), a generalization of ICA that incorporates known causal structure to learn unmixing functions and causal mechanisms, yielding new identifiability results for ICA and providing a foundational case for CRL."
Assessor360 is a multi-sequence network for Blind Omnidirectional Image Quality Assessment that models the observer's browsing process via generalized Recursive Probability Sampling and achieves state-of-the-art results on ODI quality datasets.
"We present a unifying, grammar-based framework for constructing expressive hierarchical neural architecture search spaces and an efficient Bayesian Optimization strategy that enables effective search over these vast spaces, outperforming existing NAS methods."
"We propose a diverse ensemble deep reinforcement learning method for VRPs that improves generalization across distribution shifts by enforcing sub-policy diversity via Bootstrap and regularization, outperforming state-of-the-art neural baselines on diverse VRP instances."
"For high-dimensional spiked covariance data with polynomial target functions, kernel ridge regression requires the spike strength to scale at least as $\theta \asymp d^{1-1/p}$, while two-layer neural networks trained by gradient descent require $\theta \asymp d^{1-1/k}$ with $k \leq p$, showing neural networks adapt better to low-dimensional structure."
"We propose a probabilistic active domain adaptation framework using variational inference and diffusion sampling that selects target samples based on predictive uncertainty and ambiguity, yielding more calibrated predictions and better performance on domain adaptation tasks."
We show that inverse correlations between in-distribution and out-of-distribution performance occur in real-world datasets and argue that using in-distribution performance for model selection can miss the best out-of-distribution performers.
"SLIBO-Net improves 2D floorplan reconstruction from unstructured 3D point clouds via a transformer-based model with efficient representation, geometric priors, and a scale-independent evaluation metric, achieving new state-of-the-art results on Structure3D."
"There is a U-shaped relationship between AI models' representational alignment with humans and their few-shot learning performance, with high alignment correlating with greater robustness but not being strictly necessary for effective data use or generalization."
"VIPER uses pretrained video prediction models to generate reward signals from unlabeled expert videos, enabling reinforcement learning agents to achieve expert-level control across various environments without programmatic rewards."
"We provide the first theoretical convergence and sample complexity analysis of DQNs with ε-greedy exploration, showing geometric convergence under decaying ε and analyzing the trade-off between convergence speed and region of convergence."
"The authors present a polynomial-sample online RL algorithm for linearly $q^\pi$-realizable MDPs by identifying and skipping ambiguous states, reducing the problem to a linear MDP and achieving polylogarithmic sample complexity even in the misspecified setting."
"Using multiple observational datasets from different environments and assuming independent causal mechanisms, we present a method to detect hidden confounding by testing conditional independencies and demonstrate its effectiveness in most cases, especially when confounding bias is large."
"We propose an efficient federated learning approach that leverages pre-trained models, demonstrating that fitting a Nearest Class Means classifier exactly is much more efficient and achieves strong performance, and that a two-stage fine-tuning strategy further improves convergence and generalization while reducing communication and compute costs."
Direct Preference Optimization (DPO) enables stable and efficient alignment of large language models with human preferences in a single training stage without needing a reward model or reinforcement learning.
"We propose an end-to-end method using dense pixel-wise flow and residual convolution to estimate 3-DoF camera pose from ground to satellite images, achieving large median error reductions across multiple datasets."
"We theoretically link diffusion-based manifold learning to geodesic distances via the heat kernel and propose heat geodesic embeddings, which outperform state-of-the-art methods in preserving manifold structure and cluster information in high-dimensional datasets."
"This paper proposes a distributionally robust meta-learning method that optimizes for worst-case fast adaptation risk, improving robustness to task distribution shifts."
"We unify particle-based and adversarial generative models by generalizing particle models to optionally include a generator, enabling novel hybrids and generator-free GANs."
"MultiMix and Dense MultiMix generate many more interpolated examples in the embedding space than previous mixup methods, improving classification performance and embedding structure."
We propose a Bayesian preference model (d-PM) that captures the distribution of human disagreements and a contrastive learning method that outperform SOTA on two NLG tasks.
"ResMem improves generalization by explicitly memorizing training residuals using a nearest-neighbor regressor, outperforming baseline models on vision and language benchmarks."
"We show that achieving backpropagation-like efficiency in training parameterized quantum models requires access to multiple copies of the quantum state, and we present a shadow tomography-based algorithm that attains this scaling but with unresolved classical overheads."
"We propose a method to train transformers using INT4 arithmetic for all matrix multiplications, achieving competitive accuracy and significant speedups by designing specialized quantizers for activations and gradients and enabling deployment on current GPUs."
"F-GP-UCB provides the first regret bounds and convergence guarantees for black-box optimization with complex, unknown failure regions by leveraging a Gaussian process with a mild feasibility assumption and demonstrates effectiveness on benchmark functions."
"We systematically analyze and design random walk-based positional encodings for GNNs on simplices of varying orders, connecting them to Hodge Laplacians and demonstrating improved theoretical expressivity and empirical performance."
"We conduct formal statistical tests showing that parameter-wise stochastic gradients in DNNs typically exhibit power-law heavy tails, while iteration-wise gradients and minibatch-induced noise do not, and reveal a previously overlooked power-law structure in gradient covariance spectra with implications for deep learning optimization."
We present a continuous normalization flow method for robust shape generation on the Grassmann manifold that eliminates the influence of extraneous transformations and outperforms state-of-the-art methods in log-likelihood.
"We propose TreeDSB, a diffusion-based algorithm for entropic multi-marginal optimal transport with tree-structured quadratic costs, enabling computation of Wasserstein barycenters and applications in high-dimensional image interpolation and Bayesian fusion."
We propose a parameterization and learning rate adaptation method for efficient neural network growth that maintains performance while reducing training computation and saving real time.
"We present the first certified backdoor detector using statistic local dominant probability that provides detection inference, guaranteed detection conditions, and a false positive rate bound, with strong empirical and certified detection performance on multiple datasets for resilient, small-perturbation backdoor attacks."
"The empirical risk minimization problem for two-layer fully connected neural networks with ReLU is $\exists\mathbb{R}$-complete and requires algebraic weights to solve some instances optimally, even with rational data, implying that combinatorial search algorithms are impossible for multi-output networks unless $\text{NP} = \exists\mathbb{R}$."
We extend Renyi differential privacy to infinite-dimensional functional outputs and demonstrate improved privacy-utility trade-offs in kernel-based differentially private generative models.
"We propose a VAE with a decoder based on an infinite mixture of asymmetric Laplace distributions, enabling enhanced expressive power and controllable privacy without sacrificing computational efficiency."
"Seg2Seg is a unified adaptive segment-to-segment framework for simultaneous sequence generation that learns optimal source-to-target mappings via latent pivots and expectation training, achieving state-of-the-art performance across multiple tasks."
We propose a shape-adaptive kernel regression method that improves generalization in feedforward models for implicit shape reconstruction from unoriented point clouds.
"We propose DTG-AuxL, a joint data-task generation framework that optimizes a generator via bi-level optimization to produce helpful auxiliary data and tasks for the primary task, outperforming reweighting-based methods especially when manual auxiliary data are unhelpful."
"We propose an unbiased covariance estimator that handles missing values without imputation and outperforms state-of-the-art methods in high-dimensional, low-rank settings with cell-wise outliers."
"REFINE is a deep learning-based medication recommendation system that models patient health trends and drug interaction severity to provide fine-grained, safer, and more personalized medication recommendations than existing systems."
"We propose STXD, a structural and temporal cross-modal knowledge distillation framework for multi-view 3D object detection that improves student detector performance by 2.8%–4.5% on nuScenes by regularizing feature cross-correlation, encoding temporal feature relations, and distilling response outputs."
The minimax optimal error for demographic parity-constrained linear regression with multiple sensitive attributes is Θ(dM/n) and increases with larger model bias.
"The paper introduces Hilbert Diffusion Model (HDM), a score-based generative model based on stochastic evolution equations in Hilbert spaces, which generalizes diffusion models to function spaces and demonstrates superior performance on functional data and motion synthesis tasks."
"We introduce ε-fractional core-stability for Hedonic Games, providing efficient algorithms for its computation in Simple Fractional and Anonymous HGs, and analyze its feasibility under complex sampling and PAC-learning settings."
"We propose MDP, a lightweight, pluggable defense that detects backdoor attacks on few-shot pretrained language models by exploiting masking-sensitivity gaps between poisoned and clean samples."
"CORE-3DVG improves 3D visual grounding by learning contextual object–relation information with text-guided modular networks and weak supervision, achieving state-of-the-art results on Nr3D, Sr3D, and ScanRefer."
We derive novel confidence intervals and an adaptive online algorithm for agnostic multitask regression that leverage task similarity to improve regret bounds and enable effective multitask active learning with theoretical guarantees and empirical validation.
"The authors propose Neural Latent Geometry Search (NLGS), a query-efficient Bayesian optimization method that automatically identifies the optimal latent geometry for machine learning models using Gromov-Hausdorff distance and manifold embeddings."
"We introduce and analyze zero-sum Markov games with networked separable interactions, showing that Markov CCE collapse to Markov NE, that finding stationary CCE is PPAD-hard except on star networks where fictitious play converges, and that non-stationary NE can be computed efficiently with value-iteration."
"We propose a bi-level structured policy optimization algorithm for offline RL that learns a policy robust to distributional shift using a confidence set of value estimates, with theoretical guarantees and competitive empirical performance."
"We propose the Successor Features Keyboard (SFK) with the Categorical Successor Feature Approximator (CSFA) to enable transfer learning with discovered state-features and task encodings in 3D environments, outperforming baselines in speed and scalability."
"We propose PreDiff, a conditional latent diffusion model with knowledge alignment, to generate physically plausible and uncertainty-aware spatiotemporal Earth system forecasts by incorporating domain-specific constraints."
CoDrug improves conformal prediction under covariate shift in drug discovery by using energy-based models and KDE to reduce coverage gaps by over 35%.
"Enforcing a consistency property during training of diffusion models corrects distribution drift caused by imperfect score-matching, improving generation quality on CIFAR-10, AFHQ, and FFHQ."
We show that aligning classifier confidence with the decision maker’s own confidence enables monotonic trust policies and can improve decision making in AI-assisted classification.
"We propose a force-centric pretraining model for 3D molecular conformations that uses atomic forces for off-equilibrium data and force-based regularization for equilibrium data, achieving improved force accuracy and state-of-the-art molecular simulation and property prediction performance."
"Calibrated Q-learning (Cal-QL) improves offline RL initializations for rapid and effective online fine-tuning by learning conservative, calibrated Q-values, outperforming prior methods on fine-tuning benchmarks."
"We propose a matrix-variate normalizing flow variational inference framework for Gaussian graphical models that jointly learns sparse regression solutions for all regularization parameters and l_q norms, unifying frequentist and Bayesian approaches."
"FineMoGen improves fine-grained text-driven motion generation using a transformer with spatio-temporal mixture attention and a new dataset, outperforming prior methods and enabling zero-shot motion editing via LLMs."
"We propose neural PI controllers with strictly convex neural networks to achieve provable stability and exact output tracking for MIMO systems, validated on traffic and power networks."
We propose a hierarchical comparison framework using LLMs to build class hierarchies for improved and explainable zero-shot open-vocabulary image classification with CLIP.
"We introduce PSG-4D, a 4D scene representation and Transformer-based model for dynamic scene understanding from RGB-D videos, and demonstrate its application with a large language model."
"Adversarial training, after addressing key limitations and introducing a novel multi-node attack with global and local constraints, becomes a state-of-the-art defense against adversarial structure perturbations for GNNs."
"D4Explainer is a unified framework that generates in-distribution, reliable GNN explanations—both counterfactual and model-level—by integrating generative graph distribution learning, outperforming existing methods on accuracy, faithfulness, diversity, and robustness."
"We propose sketched kernel-based Koopman operator estimators that efficiently learn dynamical systems from long trajectories with theoretical guarantees and empirical speedup, matching the accuracy of standard methods while reducing computation time."
"We propose and analyze H-consistent surrogate loss functions for two-stage expert deferral learning, proving their Bayes-consistency and realizability under constant cost, and validating them empirically on CIFAR-10 and SVHN."
"We develop a neural network that accurately classifies Q-Fano varieties among eight-dimensional toric Picard rank two algebraic varieties, suggesting new mathematical insights and enabling a combinatorial criterion for terminal singularities."
PLATO improves d ≫ n tabular learning by regularizing MLP weights using a feature knowledge graph to enforce similarity based on node proximity.
We propose a dual feature extraction and image reconstruction framework for domain-adaptive visual imitation learning that outperforms prior methods on domain-shifted tasks.
CORNN is a highly efficient training method for data-constrained recurrent neural networks that enables real-time or near-real-time reconstruction and analysis of large-scale neural population dynamics from recordings.
We propose a method to identify causal mechanism shifts between related datasets under general nonlinear additive noise models without reconstructing the full causal graph.
"We propose Transition-Constant Normalization (TCN), a parameter-free, plug-and-play normalization method that improves performance across multiple image enhancement tasks."
"We prove an exponential generalization bound with near-optimal rates for randomized algorithms under a new notion of L₂-uniform stability, and show it applies to bagging and SGD with time-decaying learning rates."
"We propose DP-SignOPORP, a differential privacy algorithm based on sign random projection with optimal Gaussian mechanisms that achieves significantly better utility than previous methods and thus promises wider adoption of DP in practice."
"S4WM, a world model based on Structured State Space models, outperforms Transformer-based models in long-term memory and efficiency for model-based reinforcement learning."
"We propose training deep learning models using a task loss–parameterized metric to emphasize downstream task–relevant information without altering the prediction model, and validate it in portfolio optimization/budget allocation and noisy reinforcement learning settings."
We propose an adaptive sample exclusion method that reduces deep neural network training time by up to 22% with only a 0.4% accuracy loss by selectively hiding low-importance samples based on loss and confidence metrics.
"Adaptive Return-conditioned Policy (ARP) improves imitation learning generalization by using fine-tuned multimodal encoders to generate adaptive rewards from natural language instructions, outperforming existing text-conditioned policies on unseen tasks."
"We propose the Double Power Law to accurately predict and optimize translation direction performance trade-offs in multilingual NMT under data imbalance, outperforming existing methods with reduced training budgets."
"CSLP-AE, a contrastive split-latent permutation autoencoder, extracts separable content and style representations from EEG, enabling generalizable signal characterization and zero-shot subject conversion."
"We build Pick-a-Pic, an open dataset of user preferences for text-to-image generation, and train PickScore—a CLIP-based model that outperforms other metrics in predicting human image preferences and is recommended for evaluating text-to-image models."
"We analyze the implicit bias of gradient descent in training two-layer (leaky) ReLU networks on nearly-orthogonal data, showing that leaky ReLU drives stable rank to 1 while ReLU bounds it by a constant, and in both cases, normalized margins equalize asymptotically."
"We show that GP-UCB achieves nearly optimal regret for kernelized bandit problems, including sublinear regret for Matern kernels, by regularizing kernel ridge estimators according to kernel smoothness and using a key concentration inequality."
We propose an evolutionary neural architecture search with convolution-enhanced Transformers to automatically select input features and balance local/global context modeling for improved knowledge tracing.
"We developed a dual-stream vision model inspired by human visual pathways that processes complementary visual inputs through separate spatial and object recognition branches, revealing that differences in dorsal and ventral stream representations arise primarily from distinct learning objectives rather than retinal input biases."
"The paper introduces a benchmark suite and spectral inspection tools to evaluate and understand the out-of-distribution robustness of neural image compression models, revealing new insights into their strengths and limitations for real-world deployment."
We formalize data quality for imitation learning as the ability of the dataset to minimize distribution shift at test time by quantifying action divergence and transition diversity.
"We present near-optimal differentially private sampling algorithms for multi-dimensional Gaussian and product distributions under varying covariance assumptions, achieving pure-DP for binary hypercubes where only approximate-DP was previously known."
"LSTMs outperform other neural network architectures in real-time decoding of finger movements from intracortical signals, achieving higher throughput and maintaining functional control even with limited input signals."
We propose an online algorithm for joint dynamic pricing and advertising in a Bayesian persuasion setting that achieves O(T^{2/3}(m log T)^{1/3}) regret for linear valuations without requiring smoothness of the demand function.
Genfer is an exact Bayesian inference tool using probability generating functions and automatic differentiation that solves a broad class of discrete inference problems efficiently and accurately compared to existing methods.
RiskQ is a multi-agent reinforcement learning method that satisfies the Risk-sensitive Individual-Global-Max principle for risk metrics like VaR by modeling joint return distributions as weighted quantile mixtures.
"We propose an image-based method to reconstruct a scene as a compact set of textured 3D primitives via differentiable rendering, enabling interpretable, editable, and physics-ready 3D representations without requiring 3D input."
"We present faster, more accurate private algorithms for counting subgraphs, including triangles in social networks and constant-size subgraphs, using approximations to sensitivity metrics with significantly reduced noise."
"We propose a 3D-grounded model that learns intuitive physics from raw images of fluid scenes without dense point supervision, enabling accurate long-horizon predictions and strong generalization."
"NU-MCC improves single-view RGB-D 3D reconstruction by introducing a Neighborhood decoder for efficient query processing and a Repulsive Unsigned Distance Function for better surface completeness, surpassing MCC by 9.7% F1-score on CO3D-v2 with over 5x faster inference."
"ERNIE is a robust multi-agent reinforcement learning framework that enforces Lipschitz continuity via adversarial regularization to improve policy robustness against environmental changes, with experimental validation in traffic light control and particle environments."
"The authors propose an entropy-dissipation-based neural network framework (EINN) to solve McKean-Vlasov equations with singular interaction kernels, demonstrating improved effectiveness over state-of-the-art methods."
"Transformer LLMs reduce multi-step compositional reasoning to linearized subgraph matching rather than developing systematic problem-solving skills, leading to performance decay as task complexity increases."
"We propose a preference-based reinforcement learning algorithm that directly learns from human preferences using contrastive learning, outperforming existing methods—including those using ground-truth rewards—on offline RL and LLM fine-tuning tasks."
"UDIL is a unified framework for domain incremental learning with memory that generalizes existing methods, provides tighter theoretical bounds via adaptive coefficients, and outperforms state-of-the-art methods empirically."
"We propose a metadata-driven analysis showing that more balanced degree distributions in graph data improve GNN performance, validated by theoretical analysis and controlled experiments."
Weight normalization in spiked covariance settings enables gradient-based learning of single index models to achieve sample complexity independent of the information exponent and outperform kernel methods.
"Balancing intra- and inter-class diversity in pre-training datasets maximizes downstream performance, and the optimal class-to-sample ratio is invariant to dataset size, enabling improved pre-training strategies as demonstrated on ImageNet."
CCPO efficiently trains safe reinforcement learning agents that can zero-shot adapt to varying safety constraints without retraining.
"PaintSeg is an unsupervised, training-free segmentation method using adversarial masked contrastive painting with alternating inpainting and outpainting to achieve state-of-the-art performance from coarse prompts like masks, boxes, scribbles, or points."
Mnemosyne is a learnable optimizer based on spatio-temporal low-rank implicit attention Transformers that can train neural networks without task-specific optimizer tuning and matches or exceeds state-of-the-art hand-designed optimizers in accuracy and scalability.
"The authors introduce Hummingbird, a vision model pretrained with cross-image attention to enable in-context learning of dense scene understanding tasks via nearest neighbor retrieval from annotated feature prompts, matching specialist performance without task-specific finetuning."
"StyleGAN can produce intrinsic images by adding fixed offsets to the latent code, but cannot generate all image transformations this way, and its generated intrinsic images are robust to relighting compared to SOTA regression methods."
"We propose Heterogeneous Neural Processes (HNPs), which leverage meta-knowledge and task-relatedness to improve multi-task learning under data insufficiency in episodic settings."
"We propose an efficient experimental design for estimating dynamic treatment effects and minimizing welfare loss under non-stationary linear trends, while establishing theoretical limits and the trade-off between these objectives."
"We propose token-scaled logit distillation, the first method to achieve less than 1.0 perplexity degradation for ternary quantization-aware training of large generative language models, improving performance on reasoning and understanding tasks."
"We introduce Markovian sliced Wasserstein (MSW) distance, which imposes a first-order Markov structure on projecting directions to improve metricity and computational efficiency over max K sliced Wasserstein, and demonstrate its favorable theoretical and practical properties across multiple applications."
"We achieve near-optimal convergence rates for Oja's algorithm on streaming PCA with Markov chain data, eliminating the logarithmic dependence on sample size caused by downsampling."
BCDiff is an encoder-free bi-directional diffusion framework that improves instantaneous pedestrian trajectory prediction accuracy by mutually refining predicted future and unobserved past trajectories with a gating mechanism.
"GM-VAE, a variational autoencoder with a latent space of Gaussian distributions on a Gaussian manifold, outperforms standard hyperbolic and Euclidean VAEs on image density estimation and is numerically stable in model-based reinforcement learning."
"We present improved core-set algorithms for maximizing diversity under fairness/partition constraints in metric spaces, achieving constant-size independent core-sets for sum-of-pairwise and sum-of-nearest-neighbor diversity, and demonstrating a 100x speedup with minimal diversity loss in a real-world message summarization system."
"We propose a federated meta-learning approach that learns personalized batch-norm and learning rate strategies for each client via federated learning, improving performance over hand-crafted baselines in label and feature shift scenarios."
"We derive efficient, high-precision score matching for Riemannian diffusion models on symmetric manifolds, enabling scalable learning on high-dimensional non-Euclidean data such as SU(n) lattices and improving contrastive hyperspherical embeddings."
"We developed a simulation-based inference method, filter SBI, that identifies complex, co-active plasticity rules in spiking networks satisfying biological constraints."
"We introduce Pseudo-Likelihood Inference (PLI), a neural approximation method for simulation-based inference that outperforms existing approaches on high-dimensional and stochastic Bayesian system identification tasks by using an adaptive bandwidth based on integral probability metrics and enabling gradient-based posterior optimization without summary statistics."
"Gradient flow on two-layer linear networks with small-scale initialization implicitly biases the hidden layer toward low-rank structures, as characterized by a variational analysis and supported by numerical experiments."
"The paper introduces C-Disentanglement, a framework that leverages domain expert labels to model confounders and achieve causally disentangled representation learning, outperforming SOTA methods under domain shifts."
"SNEFYs are tractable, flexible probability distributions formed by squaring the 2-norm of a neural network and normalising, generalising exponential families and showing strong performance in density estimation tasks."
"The paper introduces ACRL, an algorithm for Anytime-Competitive Markov Decision Processes that guarantees bounded per-round cost constraints and achieves asymptotic optimality in expected reward."
"We propose a causally identifiable functional linear structural equation model with a low-dimensional embedded space for cycle-containing causal structure learning from multivariate functional data, and validate it with simulations and EEG data."
"We introduce long-range modulatory feedback pathways in vision models that enable goal-directed visual encoding, improving recognition and steering performance while maintaining fidelity to the input."
We present a pre-trained deep-learning variational Monte Carlo model that achieves high-accuracy zero-shot wavefunction predictions and requires only minimal fine-tuning for accurate relative energies across diverse molecules.
TKNN-Shapley is a privacy-friendly alternative to KNN-Shapley that maintains data valuation performance while enabling differential privacy with better privacy-utility tradeoffs.
"DesERT improves distant-supervised NER by addressing structural noise in distant labels and self-training bias, achieving a +2.22% F1 score increase on five benchmarks."
"We derive tighter PAC-Bayesian generalization bounds for uniformly stable randomized algorithms, removing strong convexity requirements and improving previous bounds by up to a factor of $\sqrt{n}$ via sub-exponential stability and concentration of weakly dependent variables."
"We propose an LDP regression algorithm that uses public data to guide decision tree partitioning, achieving optimal convergence rates under mild assumptions and outperforming existing methods even when public and private data differ significantly."
"The authors propose GRAND-SLAMIN, a differentiable framework for fitting sparse Generalized Additive Models with interactions under structural constraints, offering improved scalability, variable selection, and prediction guarantees compared to existing methods."
We propose a causal-inference-based regularization that improves anomaly detection robustness to covariate and domain shifts by enforcing partial distribution invariance.
"GlucoSynth is a differentially private GAN framework that generates high-quality synthetic glucose traces by preserving motif relationships and temporal dynamics, outperforming prior methods on real-world data."
"We introduce K-FAC variants for weight-sharing layers in neural networks, achieving up to 75% faster training with comparable performance on modern architectures."
"Spiking PointNet is the first spiking neural network for efficient 3D point cloud recognition, overcoming training and inference challenges to outperform ANN counterparts and achieve significant speed and storage gains."
"OpenShape is a multi-modal method that scales up 3D representation learning with large, filtered datasets and specialized training strategies, achieving state-of-the-art zero-shot 3D classification and compatibility with CLIP-based models."
We derive tight $O(\log n)$ regret bounds for Bayesian bandits using a new proof technique that matches the Lai (1987) lower bound and extends to linear bandits.
GALA proposes a framework with minimal assumptions for invariant graph representation learning that leverages an assistant model to identify invariant subgraphs for out-of-distribution generalization.
We derive a lower bound for the sample complexity of goal-conditioned hierarchical reinforcement learning and empirically validate it with a simple hierarchical Q-learning algorithm across varying task complexities.
"We present an efficient, single-pass algorithm using AboveThreshold to compute differentially private quantiles, including unbounded maximums, with improved accuracy and tighter privacy guarantees."
"SENSEI is a novel network embedding model that satisfies the discrimination property and partial monotonicity within top-K rankings, outperforming state-of-the-art methods by addressing the inherent conflict between these properties in existing sampling strategies."
"We propose a GNN-based transfer learning method with kernel mean embeddings for efficient learning of interatomic potentials on small catalytic datasets, outperforming GNNs and ridge regression."
We introduce a formal definition and graphical criteria for deception in structural causal games and show they can be used to mitigate deception in AI agents.
"We propose an auxiliary reinforcement learning task that learns disentangled representations of correlated features by minimizing conditional mutual information, improving generalization and training performance under feature correlation shifts."
ImageBrush introduces a diffusion-based method that learns visual instructions from paired transformation images to accurately perform image manipulation without relying on external language descriptions.
"We show that representation denoising does not reliably indicate which model weights to edit to change stored facts, suggesting that mechanistic understanding of language models does not always improve model editing performance."
"We propose a raw-input neural network for smartphone screen moiré removal that uses color-separated and feature-mixed branches with channel and spatial modulation, and introduce a new raw video dataset with efficient temporal alignment."
We propose an SE(3)-equivariant model combining coefficient learning and residual operator layers with graphon-based convolution (InfGCN) that outperforms state-of-the-art methods on large-scale 3D Euclidean function mapping tasks.
Timewarp is a transferable enhanced sampling method using a normalising flow trained on molecular dynamics trajectories to accelerate sampling of equilibrium properties over long timescales.
IB-POMCP uses entropy-guided tree search with an I-UCB function to improve online planning and convergence in partially observable environments with sparse rewards.
"We propose a method using conditional normalizing flows and invalid action rejection to efficiently learn stochastic policies for large, unordered, discrete action spaces with validity constraints in reinforcement learning."
$f$-Policy Gradients minimize f-divergence between the agent’s state distribution and the goal to improve exploration and policy optimization in sparse-reward RL.
"SwapPrompt is a test-time adaptation framework for vision-language models that uses dual prompts and contrastive swapped predictions to improve zero-shot generalization without backbone fine-tuning, achieving state-of-the-art performance."
"Differential privacy degrades selective classification performance, and while an approach using off-the-shelf private checkpoints is effective, recovering non-private performance requires significant coverage reduction as privacy increases."
"STREAMER is a self-supervised, streaming architecture that hierarchically segments and learns robust representations from perceptual inputs using prediction error and layerwise communication."
"Big Little Decoder (BiLD) is a plug-and-play framework that accelerates text generation by using a small autoregressive model with occasional non-autoregressive refinement by a large model, achieving up to 2.12x speedup with minimal quality loss on multiple tasks."
"IQVAEs is an unsupervised framework that learns a structure-preserving, distance-preserving representation of the quotient manifold $\mathcal{M}/G$ using variational auto-encoders."
"RRHF is a simpler, more scalable alternative to PPO for RLHF that uses ranking loss on sampled responses to align language models with human preferences, achieving comparable performance with less tuning and fewer models."
"We propose BCR, a distribution-balanced self-supervised learning framework for turn-level dialogue evaluation models that improves human correlation and robustness by balancing coherence in training and regularizing score uniformity."
"We propose RWSADMM, a novel optimization algorithm for federated learning in infrastructure-less settings with isolated, heterogeneous nodes that leverages random server movement and hard inequality constraints to achieve fast convergence, improved accuracy, reduced communication, and better scalability."
We propose and prove optimal dynamic treatment allocation strategies and OPE-based estimators to minimize variance and bound mean squared error of treatment effect estimates in non-Markov and time-varying Markov settings.
"We present a self-supervised method that learns time-invariant neuronal representations from population recordings, improving transcriptomic subclass prediction by over 35% and class prediction by over 20% compared to state-of-the-art methods."
"A disentangled Wasserstein autoencoder with an auxiliary classifier automates identification and editing of functionally important residues in proteins, outperforming existing methods on T-cell receptors with improved efficiency."
"We provide near-optimal sample complexity and regret bounds for contextual bandits with low-rank context groups, achieving $\widetilde O(r(S+K)/\epsilon^2)$ sample complexity and $\widetilde O(\sqrt{r^3(S+K)}T)$ regret, and extend these results to more general low-rank bandits."
"Applying DreamerV3's tricks to PPO does not generally improve performance, but they help on Atari games with reward clipping."
"We propose a method that learns generative neural fields as weighted sums of latent-space implicit basis networks, enabling efficient sampling and competitive generation across image, voxel, and NeRF data using a denoising diffusion model for coefficient sampling."
We introduce a differentially private feature selection method based on Kendall rank correlation that improves the utility of private linear regression in high dimensions without increasing user burden or privacy cost.
"SlotDiffusion is an object-centric latent diffusion model that improves unsupervised object segmentation and generative quality for both images and videos, and scales to real-world datasets when paired with self-supervised encoders."
"AttrSeg improves open-vocabulary semantic segmentation by decomposing ambiguous or incomplete category names into attribute descriptions and aggregating them into discriminative classifiers, outperforming existing methods on datasets with attribute annotations."
"PrimDiffusion introduces a diffusion-based framework that generates 3D humans by denoising volumetric primitives, enabling efficient, high-quality, and flexible 3D human synthesis with real-time rendering and support for conditional generation tasks."
"We propose H3T, a framework that automatically integrates memory optimization and parallelism to efficiently train large Transformer models, achieving up to 4.3× speedup and 80.5% memory reduction compared to Megatron-DeepSpeed, and enabling GPT-3-175B training on just 64 A100 GPUs."
"We propose and analyze Byzantine-robust algorithms for distributed variational inequality problems, improving upon and extending prior work."
AUDIT is an instruction-guided latent diffusion model for audio editing that uses triplet data to edit only necessary segments with minimal text input and achieves state-of-the-art results on multiple tasks.
A deep graph generative model trained on aggregate graph statistics under local differential privacy can generate realistic graphs competitive with models trained on full adjacency matrices.
The long N-step surrogate stage (LNSS) reward estimator reduces Q-value variance and improves deep RL performance in continuous control by using long future reward trajectories.
"LayoutPrompter uses large language models with in-context learning to achieve versatile, training-free, and data-efficient conditional graphic layout generation that outperforms state-of-the-art methods."
"Top-$k$ generalizes standard decision tree algorithms by considering the $k$ best attributes for splitting, achieving higher accuracy than greedy methods and better scalability than optimal algorithms."
"We propose sequential, nonparametric auditing methods for fairness in deployed models that work with non-uniform sampling and changing policies or distributions, leveraging anytime-valid inference for efficient and interpretable real-world monitoring."
"We propose PICProp, a physics-informed method that computes valid confidence intervals for solutions of partial differential equations without strong assumptions."
"We propose a UCB-based algorithm with phased updates for the bandit task assignment problem, achieving gap-dependent regret $O(MN(1/\Delta)\log T)$ and gap-free regret $\tilde{O}(\sqrt{MNT})$, nearly matching known lower bounds."
We propose a multi-modal information bottleneck method for interpretable vision-language models that improves attribution without requiring ground truth labels.
We propose a prototype-based framework using Dempster-Shafer and Subjective Logic theories to quantify aleatoric uncertainty and improve reliability in cross-modal retrieval.
"This paper introduces the first algorithm and analysis for finding optimal solutions in performative prediction under inequality constraints, achieving O(√T) regret and constraint violations with √T + 2T samples."
"We propose a two-stage LfVO framework using a pretrained STG Transformer for intrinsic reward prediction that enables learning policies from visual observations alone, outperforming baselines on Atari and Minecraft without requiring task-specific information or full environment rewards."
We rigorously characterize the learning curve of kernel ridge regression under realistic assumptions and show that benign overfitting in over-parameterized neural networks occurs only when noise is small.
"We propose a coupled PDE framework modeling feedback-driven distribution shift in learning systems and prove its asymptotic convergence under gradient retraining in cooperative and competitive settings, demonstrating its ability to capture complex distributional phenomena like polarization and disparate impact."
"SPUIR improves contextual batched bandits by imputing unobserved rewards via sketching-based ridge regression, achieving lower regret than baselines."
"VeriX is a system that generates optimal robust explanations and counterfactuals along model decision boundaries using constraint solving and feature sensitivity ranking, evaluated on image recognition and autonomous aircraft taxiing tasks."
"We propose the first federated conditional stochastic gradient algorithm with momentum and variance reduction for nonconvex conditional stochastic optimization in federated learning, achieving optimal sample and communication complexity and validating its efficiency experimentally."
"SAMS-VAE is a sparse, interpretable generative model for perturbed observations that disentangles and composes perturbation-specific latent effects, outperforming baselines on biological datasets and aligning with known mechanisms."
We establish convergence guarantees for sequential federated learning with heterogeneous data and show experimentally that it can outperform parallel federated learning in extremely heterogeneous settings.
"Learnability implies robust learnability under additive but not subtractive contamination, so realizable learnability does not imply agnostic learnability in distribution learning."
We combine graph curvature descriptors with topological data analysis to robustly evaluate graph generative models.
Ecosystem-level analysis reveals that systemic failure in deployed machine learning models persists across contexts and can introduce new forms of disparity not observed at the individual model or human level.
"We present a dynamic data structure for the metric $k$-center problem with $z$ outliers in bounded doubling dimension metric spaces that achieves worst-case query time $\varepsilon^{-O(\text{dim})}k \log n \log\log\Delta$ and update time $\varepsilon^{-O(\text{dim})}\log n\log\Delta$, improving upon previous $(k+z)^2$ query dependence."
"No explicit positional encoding is necessary for decoder-only Transformers to generalize well to longer sequences, as models without positional encoding outperform those with common schemes like ALiBi, Rotary, and Absolute Position Embeddings on length generalization tasks."
"We extend Gaussian Differential Privacy to general Riemannian manifolds using a Riemannian Gaussian distribution based on the Bishop-Gromov theorem, providing improved utility over prior mechanisms on curved spaces like the unit sphere."
We present a systematic evaluation framework addressing key pitfalls in Active Learning benchmarks and provide empirical evidence clarifying AL's effectiveness relative to semi-supervised and self-supervised learning in image classification.
We propose a UCB-based decentralized multi-agent bandit algorithm that achieves optimal instance-dependent $\log T$ and near-optimal instance-free $\sqrt{T}\log T$ regret bounds under general time-varying random communication graphs and heterogeneous sub-Gaussian or sub-exponential reward distributions.
Selective multi-agent experience sharing of a few highly relevant transitions outperforms full sharing and baseline decentralized training in multi-agent RL with minimal communication.
"Conditional Permutation Importance (CPI) is a model-agnostic, computationally efficient method that accurately controls type-I errors and outperforms permutation-based variable importance methods, especially in correlated covariate settings."
"Gradient flow on linear equivariant steerable networks converges to the unique group-invariant maximum-margin classifier, and under a unitary input representation, these networks are equivalent to data augmentation and achieve better margins and generalization than non-invariant networks."
"We formally show that incorporating task-specific symmetries in models improves generalization and that the optimal model symmetry closely matches the data's approximate symmetry, quantitatively relating model and data equivariance errors."
"We propose a diffusion-model-based method that generates task-specific labeled graph examples from unlabeled graphs, significantly improving performance on graph property prediction tasks compared to existing methods."
"Sharpness minimization in overparameterized neural networks does not reliably ensure generalization, as flat minima do not always generalize and sharp minima can, indicating that the link between sharpness and generalization depends on data and architecture, and other explanations are needed."
We derive a scaling rule for model EMA updates that preserves training dynamics across batch sizes and enables efficient training of SSL methods like BYOL at much larger batch sizes.
"We present a $\sqrt{T}$-regret algorithm (up to logarithmic factors) for bandit LQR and LQG with semi-adversarial perturbations and time-varying adversarial loss, improving upon the previous $T^{3/4}$ rate."
"NAR-Former V2 is a modified Transformer-based model that learns efficient neural network representations by integrating GNN inductive capabilities and tokenizing networks as graphs, outperforming GNN-based methods on latency prediction and matching SOTA on cell-structured architecture tasks."
Auto-STPP is a novel method that efficiently learns and integrates flexible intensity functions for 3D spatiotemporal point processes using a decomposable parametrization that avoids computational complexities of previous approaches.
"We propose a low-rank sketching method using Frequent Directions to reduce the memory and compute costs of adaptive regularization in deep learning, achieving near-full-matrix performance with sub-linear memory."
"We propose PSGeg, a method using non-learnable prototypical regularization to consistently supervise group tokens in weakly open-vocabulary semantic segmentation, achieving state-of-the-art results."
"How2comm is a collaborative perception framework for multi-agent driving that uses mutual information-based communication, delay compensation, and a collaboration transformer to improve perception under communication and temporal noise."
"We propose an adaptive token-resolution method for ViTs that clusters tokens spatially to reduce sequence length for dense prediction tasks, significantly accelerating inference and fine-tuning with minimal performance loss."
"RoboCLIP is an online imitation learning method that generates rewards from a single in- or out-of-domain demonstration using pretrained video-and-language models, achieving 2–3× higher zero-shot robot manipulation performance than competing methods without manual reward design or finetuning."
"We propose robust fairness regularization (RFR) that improves fairness under distribution shifts by considering worst-case model weight perturbations within each sensitive attribute group, achieving better fairness-accuracy trade-offs than baselines."
R-divergence is a model-oriented divergence measure that estimates distribution discrepancy via expected risk difference and achieves state-of-the-art performance in detecting distribution shifts and improving robustness to noisy labels.
"We interpret Transformer self-attention as estimating a causal structure among input tokens via partial correlations in the deepest attention layer, enabling zero-shot causal discovery demonstrated on sentiment classification and recommendation tasks."
"We propose a non-uniform image resizing method that maintains high target resolution in crop-based transformer trackers, enabling speed-oriented trackers to achieve performance close to or even surpassing performance-oriented trackers while significantly increasing speed and reducing computation."
Affine model transfer provides a theoretically grounded framework for regression that unifies existing transfer learning methods and clarifies generalization properties by separately modeling domain commonalities and specificities.
"Unsupervised machine translation is theoretically feasible for animal communication when the communication system is sufficiently complex and there is sufficient common ground, as demonstrated by sample complexity bounds on stylized language models."
"We propose an OPE framework for human feedback that reconstructs immediate rewards using environment-knowledge-regularized latent dynamics, improving HF signal estimation in real-world and simulated domains."
WCDQN is a deep reinforcement learning algorithm for weakly coupled Markov decision processes that trains multiple subagents with a shared network and achieves faster convergence than DQN in high-dimensional structured problems.
DiffCSP is a periodic-E(3)-equivariant diffusion model that generates crystal structures using fractional coordinates and outperforms existing methods with lower computational cost.
We propose a theoretically grounded algorithm for Continual Meta-Learning that dynamically adjusts meta-parameters to balance stability and plasticity in non-i.i.d. task sequences.
"Unregularised Policy Mirror Descent with adaptive step-size achieves the optimal γ-rate and requires adaptive step-size for rate-optimality under exact policy evaluation, with dimension-optimal sample complexity in the inexact setting."
"The paper links sparse neural network performance to the weighted spectral gap of Ramanujan-like graph structures in the network, introducing a full-spectrum coordinate that combines structure and parameters, and proposes a pruning method that outperforms existing approaches by maximizing this coordinate during initialization."
"The paper introduces a stochastic optimization framework with arbitrary oblivious noise and provides an efficient algorithm that, under a minimum inlier probability, outputs either a single solution or a small list containing a solution."
"We prove that under the Erdős–Rényi graph model, GNN classifiers including graph convolutional networks converge to deterministic outputs as graph size increases, establishing a zero-one law that limits their representational capacity."
A context distillation method effectively updates and propagates entity knowledge in language models without degrading general performance.
"We introduce DreamSim, a perceptual image similarity metric trained on synthetic human judgments that better captures semantic and layout differences than pixel- or patch-based metrics and generalizes to real images, outperforming prior approaches on retrieval and reconstruction tasks."
"We present a deep learning framework that jointly reconstructs inner, midthickness, and outer cortical surfaces with topological correctness and cortical thickness estimation from 3D MRI, outperforming state-of-the-art methods on ADNI and OASIS datasets."
"We introduce discrete-smoothness and achieve it for online covering problems (facility location and set cover), improving consistency, robustness, and smoothness guarantees over prior work."
Brant is a large-scale foundation model for intracranial neural signals that achieves state-of-the-art performance on multiple downstream tasks via pre-training on extensive data and modeling temporal and spatial dependencies.
"We propose a test-time backdoor detection method using paraphrasing with prompt-engineered ChatGPT to remove triggers while preserving semantics, outperforming existing baselines on style-based and other backdoor attacks."
"We propose and analyze minimal-information matrix multiplicative weights algorithms for quantum and semidefinite games with scalar payoff feedback, achieving convergence rates comparable to full-information methods under limited information."
"We propose VP-SVGD and GB-SVGD, finite-particle SVGD variants with provably fast convergence rates and double-exponential improvement over prior analyses, achieving polynomial dimension dependence and removing the curse of dimensionality."
"Feedback-Feedforward Alignment (FFA) co-optimizes classification and reconstruction in vision tasks, inducing feedback-driven visual inference capabilities like denoising and imagination in a biologically plausible manner."
"ESCFR addresses mini-batch sampling effects and unobserved confounder effects to accurately estimate individual treatment effects from observational data using a relaxed mass-preserving regularizer and a proximal factual outcome regularizer based on optimal transport, outperforming existing methods."
"We present a differentially private prefix sum mechanism with constant-time-per-step noise generation, a 4× variance reduction, and identical noise distributions, outperforming prior methods in efficiency."
"We propose a generator-based, meta-learned dataset distillation method that efficiently produces synthetic datasets of arbitrary size in a single step, achieving a 22× speedup over iterative approaches."
"We introduce a practical randomized paper assignment method that outperforms existing approaches in achieving robustness, diversity, anonymity, and alternative evaluation while maintaining reviewer expertise."
We propose a predictive bias-based greedy search method to find near-optimal prompts that improve large language models' in-context learning performance.
"We show that the robustness of downstream linear predictors depends on the robustness of their pretrained representations, providing theoretical bounds and practical criteria for transfer learning robustness."
"We propose and theoretically justify an optimal transport-based distributional robustness framework in model space that generalizes Sharpness-Aware Minimization and yields improved robustness across single models, ensembles, and Bayesian neural networks."
"Flow matching posterior estimation (FMPE) is a scalable, accurate simulation-based inference method using continuous normalizing flows that outperforms discrete flow methods in high-dimensional scientific problems like gravitational-wave inference."
We propose a time-step-aware quantization method that improves diffusion model performance on mobile devices with no inference overhead and compatibility with PTQ and QAT.
"We propose dataset pruning methods tailored for transfer learning that prune up to 80% of source data classes without sacrificing downstream performance, significantly accelerating pretraining by up to 5×."
"We provide an efficient algorithm achieving a nearly tight $\widetilde{O}(\sqrt{TK})$ regret bound for adversarial losses and unknown i.i.d. contexts in the cross-learning bandit setting, with applications to first-price auction bidding and stochastic sleeping bandits."
"We introduce cone attention, a hierarchy-aware replacement for dot product attention using hyperbolic entailment cones, which improves performance and parameter efficiency across diverse tasks."
"We present an algorithm to approximate the Rashomon set of sparse generalized additive models, enabling domain expert interaction and addressing variable importance, user constraints, and model changes."
"Neural Priming adapts large pretrained models to distribution shifts and downstream tasks with few or no labeled examples by recalling and conditioning on relevant pretraining data at test time, improving accuracy on ImageNet and transfer benchmarks."
"VERDE is an improved machine learning attack that efficiently recovers sparse and narrow Gaussian LWE secrets with smaller moduli and larger dimensions, and includes a theoretical explanation for ML LWE attack success."
"The authors present a Poisson compressed sensing model of the olfactory bulb that accurately detects multiple odors within a single sniff and performs Bayesian uncertainty estimation, requiring a neural code geometry matched to receptor properties."
We derive the first non-asymptotic upper bound on the expected sample complexity of Top Two sampling for bandit identification and show that a UCB-based Top Two algorithm achieves it with competitive empirical performance.
"We propose a diffusion probabilistic model framework for structured node classification on partially labeled graphs that learns a joint label distribution and conditions predictions on known labels using manifold-constrained sampling, with a new training algorithm and theoretical justification."
We improve zero-shot generalization in vision-language models by minimizing feature distribution shift at test time through prompt tuning aligned with source domain statistics.
"The authors derive deterministic bounds for approximate POMDP planning by bounding observation and state subsets, enabling guaranteed suboptimal solutions that can be integrated into existing sampling-based solvers."
"CMMN adapts EEG signal power spectra to a Wasserstein barycenter at test time, improving model performance across subjects, sessions, and hardware without retraining."
V-InfoR is a robust GNN explainer that infers latent graph representations via variational inference and uses a graph information bottleneck to generate explanations resilient to structural corruption.
"STAR addresses class distribution bias in class-incremental semantic segmentation by storing compact class prototypes and replaying them with background pixels to balance class frequencies, while introducing feature-preserving and similarity-aware losses, outperforming prior methods on Pascal VOC 2012 and ADE20K."
"We propose an imitation learning method that learns from vague feedback distinguishing only widely differing expert and non-expert demonstrations, and show it outperforms standard and preference-based methods."
"We present replicable learning algorithms for estimating coin biases and PAC learning with optimal list and certificate complexities, achieving sample complexity $\tilde{O}(d^2/\varepsilon^2)$ and showing that list complexity is optimal for these tasks."
PEDESTAL is a new decentralized stochastic algorithm that achieves second-order optimality with non-asymptotic guarantees and gradient complexity $\tilde{O}(\epsilon^{-3})$ for finding approximate second-order stationary points in decentralized nonconvex optimization.
"We analyze how link recommendations in online social networks affect both user engagement and opinion conflict, finding that some recommendation algorithms can reduce conflict while promoting relevance."
"We propose HiNeRV, a hierarchical INR-based video codec with depth-wise convolutions, MLPs, and advanced positional encoding that achieves significant rate-distortion improvements over prior INR methods and competes with state-of-the-art learning-based codecs."
"We propose a Newton–Cotes-based method that integrates time-varying velocity estimates to improve state prediction in reasoning system dynamics, outperforming GNN baselines on multiple benchmarks."
"RSM is a modular framework that models object dynamics via slotwise communication and reusable mechanisms using Central Contextual Information, outperforming prior methods and generalizing better to complex, out-of-distribution scenes."
Uni-UVPT is a parameter-efficient source-free domain adaptive semantic segmentation framework using unsupervised visual prompt tuning with an adaptive pseudo-label correction strategy that achieves state-of-the-art results on GTA5→Cityscapes and SYNTHIA→Cityscapes.
PAPR is a novel point-based scene representation and differentiable renderer that learns accurate geometry and fine texture from sparse point clouds for various 3D applications.
"We propose a general method to add rotational equivariance to any point-cloud model, enabling the use of more flexible architectures like the Point Edge Transformer while maintaining physical invariances in atomic-scale machine learning."
"Stable Feature Boosting (SFB) learns to use stable and conditionally-independent unstable features without test-domain labels, achieving asymptotically-optimal prediction by leveraging stable-feature pseudo-labels to adapt unstable-feature predictions."
"LLMs can assume different roles via in-context impersonation, which reveals both their capabilities and biases in vision and language tasks."
"We propose a mesh-morphing and Gaussian process-based method that handles geometrical variability without shape parameterization or graph neural networks, providing predictive uncertainties and competitive accuracy on large meshes."
"We propose a non-parametric Bayesian survival model with a permanental process for time-varying covariates that enables fast, scalable estimation without MCMC."
"DiffSketcher generates high-quality, vectorized free-hand sketches from natural language using a pretrained text-to-image diffusion model and extended score distillation sampling."
We propose a physics-informed surrogate forward model embedding linearizations of the forward model to improve inversion accuracy in ocean acoustic tomography.
"SPAE enables frozen LLMs to process and generate non-linguistic modalities by translating pixels into interpretable tokens, surpassing state-of-the-art image understanding performance by over 25%."
"Fine-Grained RLHF improves language model behavior by enabling dense, segment-level, and multi-criteria reward signals from human feedback during training."
"We introduce and solve the problem of clustering of bandits with misspecified user models, proposing two robust algorithms with provably near-optimal regret and validating their superiority empirically."
We introduce optimization improvements that enable structured variational autoencoders with discrete latents to model multimodal uncertainty in missing data and compete with state-of-the-art time series models while remaining interpretable.
"We show that in the turnstile model with insertions and deletions, any differentially private mechanism for continuously releasing the number of distinct items incurs at least a $T^{1/4}$ additive error, but by bounding the maximum flippancy $w$ of the stream, we achieve $O(\sqrt{w} \cdot \mathrm{poly}\log T)$ additive error, which is optimal for item-level privacy."
"We derive tighter norm-based generalization bounds for sparse ReLU neural networks using filter and sparsity norms, showing improved estimation of generalization and highlighting the importance of sparsity and architecture."
"We propose a decompositional method that evaluates and improves text-to-image alignment by scoring disjoint assertions in prompts via a VQA model, achieving an 8.7% improvement in alignment accuracy over previous state-of-the-art."
"We propose the manifold attack model, a unified theoretical framework explaining the transferability of adversarial examples through the curvature of the data manifold."
We propose a differentiable two-step method for inferring an unknown number of mutually exclusive subsets that enables gradient-based optimization in variational inference tasks.
"We propose a group-oriented multi-agent reinforcement learning method that automatically forms dynamic groups to improve team cooperation and policy learning efficiency, validated on StarCraft II and football tasks."
"The authors propose Dual Mean-Teacher, a semi-supervised framework with two teacher networks that improves audio-visual source localization accuracy by 8.9–9.6% using only 3% labeled data."
"We propose learning with respect to a class of perturbation sets, enabling improved learnability for infinite Littlestone classes with a perfect-attack oracle and showing that abstention is sometimes necessary for robust learning without query access."
Meet in the Middle (MIM) is a bidirectional pre-training method that improves data efficiency for left-to-right language models and enhances infilling by training and combining left-to-right and right-to-left models.
We connect randomized linear algebra-based active learning for regression with robust mechanism design for revenue maximization under high-dimensional unknown valuations using topic models.
"A novel robust framework for low-level vision tasks learns neural module weights directly from corrupted test sequences without external data, using spatio-temporal coherence and a spatial pyramid loss for improved noise robustness, achieving state-of-the-art results on standard video datasets."
"EPNS is an equivariant probabilistic neural simulator that outperforms existing methods by improving simulation quality, data efficiency, stability, and uncertainty quantification for stochastic dynamical systems."
"NDT2, a spatiotemporal Transformer pretrained across sessions, subjects, and tasks, enables robust neural spiking data decoding for intracortical BCIs."
"We show that Orthogonal Matching Pursuit (OMP) approximates Information Pursuit (IP) with random projections and propose an IP-OMP variant for computationally efficient, explainable image prediction using sparse combinations of interpretable concept embeddings."
"We derive theoretical results on the frequentist size and coverage of pointwise credible sets for sparse variational Gaussian processes with eigenvector inducing variables, characterizing when such credible sets are conservative or overconfident."
"We introduce a meta-mechanism that leverages arbitrary side information to simultaneously achieve strong welfare and revenue guarantees in multidimensional mechanism design, even when types lie on low-dimensional subspaces."
"We present S-FCI, a constraint-based algorithm that learns causal structure in non-Markovian, multi-domain systems with latent confounders by leveraging the S-Markov property derived from causal invariances, ensuring soundness and subsuming existing methods."
"We propose a two-metric projection method with variable partitioning for efficiently estimating M-matrices in MTP₂ Gaussian precision estimation, achieving significant computational gains over existing methods."
"We propose Constrained Self-Play (CSP), an offline policy adaptation method that learns risk-free adaptation policies from target agent behavior data, outperforming non-conservative baselines in multi-agent environments."
"We propose Cross-modality Noisy Supervision (CNS), a method that uses CLIP and SAM to supervise 2D and 3D scene understanding without labels, significantly improving semantic segmentation accuracy on multiple datasets."
"We propose a discretization-free scalable framework that enforces self-consistency of the velocity field via an iterative biased gradient method to solve mass-conserving PDEs such as the time-dependent Fokker-Planck equation and Wasserstein gradient flow, achieving accurate solutions and strong scalability in high dimensions."
"Chase enables end-to-end training of channel-level sparsity in deep networks, achieving 1.7× GPU inference speedup on ResNet-50 without accuracy loss by exploiting inherent channel-wise sparsity in dynamic sparse training."
"Clustered Compositional Embeddings (CCE) combine clustering-based quantization with dynamic compositional embeddings to efficiently handle large categorical tables in recommendation systems, achieving high compression and dynamic adaptability during training, with theoretical convergence guarantees."
"1-Lipschitz neural networks trained with an optimal transport dual loss produce saliency maps that are concentrated, low-noise, align with human explanations, encode transportation and adversarial directions, and enable scalable, interpretable models."
"We propose an end-to-end tone mapping method that combines global 3D LUT-based tone adjustment with learned, data-driven local Laplacian filter parameter maps to simultaneously preserve global tone and local edge details, outperforming state-of-the-art methods on benchmark datasets."
"We introduce a stepwise self-evaluation guided stochastic beam search that improves multi-step LLM reasoning accuracy on GSM8K, AQuA, and StrategyQA."
"The improved knowledge gradient (iKG) policy, which selects measurements to maximize the one-step improvement in the probability of identifying the best arm, is asymptotically optimal and more easily extendable than the standard knowledge gradient (KG) algorithm for best arm identification and its variants."
"IMDer uses a diffusion model guided by available modalities to recover missing modalities in incomplete multimodal emotion recognition, improving accuracy under various missing modality patterns."
"Transformers pretrained on sufficiently diverse tasks can solve fundamentally new linear regression tasks in-context via in-context learning, deviating from a prior limited to the pretraining distribution."
"We propose a framework that automatically adapts user prompts to improve text-to-image generation by fine-tuning and reinforcement learning, outperforming manual prompt engineering on Stable Diffusion."
"We propose LongMem, a framework that freezes a backbone LLM as a memory encoder and adds an adaptive side-network to retrieve and utilize long-term memory, enabling effective memorization and use of long-context information for improved language modeling and in-context learning."
"We propose a normalizing flow method that maps normal image feature distributions to location-specific distributions with shared means but varying variances, and maps abnormal features to a distinct mean, improving visual anomaly detection by better modeling diverse normal distributions."
"We reinterpret self-attention as a kernel density estimator to develop robust transformers resistant to adversarial attacks and data contamination, improving performance in image, language, and time series tasks."
"We introduce Sparsified Online Newton (SONew), a memory-efficient second-order optimizer that achieves faster convergence and better performance on large-scale models with minimal computational overhead by using structured sparsity, outperforming first-order and existing second-order methods."
"We propose a noise-adaptive Thompson sampling algorithm for linear contextual bandits with heteroscedastic noise that achieves regret $\widetilde O\left(d^{3/2} + d^{3/2} \sqrt{\sum_{t=1}^T \sigma_t^2}\right)$, smoothly interpolating between the constant-variance and deterministic reward regimes."
"The authors propose Koopman Kernel Regression, a framework with convergence guarantees and generalization error bounds for learning linear time-invariant dynamical system models from data, demonstrating improved forecasting performance over existing Koopman and sequential prediction methods."
"DISCO-DANCE improves unsupervised skill discovery by selecting a guide skill to explore uncharted states and dispersing other skills to maximize discriminability, outperforming baselines in complex environments."
"This paper formalizes and analyzes the trade-off between sample density, input dimension, and the generalization error for learning smooth functions, focusing on practical constants and transitory regimes."
We model mouse social conflict behavior and whole-brain neural data using a first-level Theory of Mind framework and identify brain regions encoding beliefs about opponents in partially observable interactions.
"We introduce participatory systems that allow individuals to opt into personalized model predictions at inference time, improving performance and privacy in clinical tasks compared to standard personalization and imputation methods."
"CaML is a causal meta-learning framework that predicts personalized effects of novel interventions using intervention and individual features, outperforming baselines in zero-shot settings on real-world datasets."
"MG-ViT introduces a two-stage, multi-granularity patch splitting method that significantly reduces ViT's FLOPs for classification and downstream tasks without performance loss."
"The contextual lasso fits a sparse linear model whose coefficients depend on contextual features via a nonparametric neural network regularized to produce sparse, interpretable models."
"LANCE generates language-guided counterfactual images to stress-test visual models, revealing significant performance drops and hidden biases without modifying model weights."
"We propose the Waypoint Transformer, a decision transformer variant conditioned on intermediate waypoints, which significantly improves offline reinforcement learning performance on challenging environments."
"OMIGA is an offline multi-agent RL algorithm that uses implicit global-to-local value regularization to improve policy learning from offline data, outperforming state-of-the-art methods on MuJoCo and StarCraft II tasks."
"mip-Grid integrates anti-aliasing into grid-based radiance field representations, achieving fast training and high-quality rendering comparable to mip-NeRF while using minimal additional parameters."
"PMR is a method that retrofits pre-trained masked language models into machine reading comprehension models using Wikipedia-based data and a Wiki Anchor Extraction task, improving extraction and classification performance, especially in low-resource settings and enhancing prediction explainability."
"We analyze the theory behind pruning plus fine-tuning in overparameterized matrix sensing and neural networks, showing that proper regularization enables greedy pruning to retain a minimal, accurate model which fine-tuning then rapidly improves."
"We reformulate combinatorial optimization problems as bisimulation-quotiented MDPs to exploit symmetries and improve out-of-distribution generalization, demonstrating superior performance and robustness on multiple classical problems with learned policies."
"Passive learning from observational data can enable agents to generalize causal intervention strategies at test time, especially when supported by explanations or few-shot prompts."
"We analyze adversarial membership inference in differentially private machine learning under realistic threat models where adversaries have only partial or related data, characterizing and validating their capabilities via hypothesis testing errors to inform practical privacy noise selection."
"Tri-factor contrastive learning introduces a learnable diagonal matrix to produce identifiable and interpretable features with ordered importance, improving contrastive learning methods like SimCLR and CLIP."
"We propose a method to detect and quantify social biases in pre-trained code generation models using a new prompting paradigm and evaluation metrics, revealing severe biases across Codex, InCoder, and CodeGen."
"We propose FMD, a fast, efficient debiasing method that identifies and removes model biases using counterfactual concepts and machine unlearning with minimal data and parameter updates, outperforming retraining-based methods on multiple datasets."
"We introduce RGW, a robust Gromov-Wasserstein distance with theoretically grounded optimization that improves performance on graph learning tasks under outlier contamination."
"We propose a method that, using a retained clean set, identifies and separates beneficial from detrimental information in potentially harmful training data to improve model performance without removing useful data."
We propose a generative retrieval method using Transformer-based autoregressive decoding of Semantic IDs to outperform state-of-the-art recommender systems and improve generalization for unseen items.
"Flow is a federated learning algorithm that achieves per-instance personalization by allowing each data instance to choose between local and global parameters, improving client accuracy under data heterogeneity."
OKRidge is a fast algorithm for sparse ridge regression that uses novel lower bounds and linear system solves to achieve provable optimality much faster than existing MIP-based methods.
"We propose an oracle-efficient algorithm for adversarial contextual bandits that achieves regret $O(T^{2/3}(K\log|\Pi|)^{1/3})$ with $O(K)$ oracle calls per round, improving upon previous bounds."
"We propose ToolkenGPT, which represents tools as embeddings to enable LLMs to learn tool use through token prediction, improving performance on reasoning, knowledge, and decision-making tasks without fine-tuning or extensive in-context examples."
"We introduce Regularity as an intrinsic reward that promotes structured exploration in reinforcement learning, improving zero-shot assembly task performance when combined with uncertainty-based rewards."
"We propose a Bayesian-inspired method for selecting optimal in-context learning demonstrations that significantly improves performance across multiple LLMs and datasets, supporting the view that LLMs infer latent task variables."
SidechainDiff is a diffusion-based Riemannian model that learns sidechain conformation representations from unlabeled protein structures and achieves state-of-the-art performance in predicting the effects of mutations on protein-protein binding.
A diffusion-based method using α-posterior MAP estimation with Gaussian smoothing separates superimposed discrete RF sources and achieves 95% BER reduction over existing methods.
"HyP-NeRF improves generalizable NeRF priors via latent conditioning and hypernetworks with denoising and fine-tuning, achieving state-of-the-art results in NeRF reconstruction, compression, and retrieval."
"TensorNet is an O(3)-equivariant message-passing neural network using Cartesian tensor embeddings that achieves state-of-the-art performance with fewer parameters and lower computational cost, enabling accurate prediction of molecular quantities including vectors and tensors."
We propose a feature connectivity-based method with object-centric regularizations that discovers high-quality object representations from real-world images more effectively than generative approaches.
"We propose Selective Amnesia, a continual learning-inspired method for inducing controllable forgetting of specific concepts in conditional generative models, including text-to-image diffusion models."
"We propose a subspace identification-based model that disentangles domain-invariant and domain-specific variables under weaker assumptions, improving multi-source domain adaptation performance on real-world datasets."
We generalize the Dikin walk for sampling from a log-concave distribution over a bounded polytope using a soft-threshold barrier that leverages function smoothness to achieve faster mixing.
We propose a method to generate less visually detectable adversarial patches for DNNs on ImageNet that maintains or improves attack performance while reducing image distortion.
TansBL is a novel algorithm that balances the generalization advantages of Bayesian learning with the sampling efficiency of optimization-based methods in neural networks.
"We propose using Gini deviation instead of variance as a risk measure in RL, addressing variance-based methods' limitations and demonstrating improved policy learning and risk mitigation."
"We propose GPOMCP, a POMDP-based method with belief tree reuse and a guessed target object, enabling faster and more successful indoor object search for mobile robots under occlusion and limited visibility."
"We present a general transformation from offline approximation algorithms with low average sensitivity to online algorithms achieving low $\epsilon$-approximate regret in the random-order model, demonstrating its applicability to online $(k,z)$-clustering, matrix approximation, and regression with polylogarithmic regret and low inconsistency."
"We propose DS-GDA, a single-loop first-order algorithm that converges for general nonconvex-nonconcave minimax problems without requiring specific regularity conditions and empirically eliminates limit cycles on challenging benchmarks."
"We propose FedBiOAcc, a communication-efficient algorithm for federated bilevel optimization with proven convergence rates and empirical superiority in federated data-cleaning and hyper-representation learning tasks."
"We propose disturbance-based model-free reinforcement learning policies with efficient optimization and online adaptation, achieving dimension-independent regret guarantees and improved robustness on standard benchmarks."
"We generalize the classical oracle framework to analyze minimax complexities of parallel stochastic gradient optimization under heterogeneous worker computation times, revealing surprising implications for asynchronous methods."
"Grammar prompting enables large language models to generate structured outputs by leveraging domain-specific grammars during in-context learning, improving performance on diverse DSL generation tasks."
"We propose a kernel-based algorithm that efficiently constructs a sparse similarity graph preserving cluster structure, outperforming scikit-learn and FAISS implementations on multiple datasets."
"We propose a recurrent neural network-based framework for temporal neighbor aggregation in temporal graphs that integrates information from all historical neighbors, achieving +9.4% higher averaged precision on a real-world Ecommerce dataset compared to existing methods."
We introduce an evaluation method for black-box abstaining classifiers that treats abstentions as missing data and estimates their counterfactual performance under identification conditions.
We propose a PEFT and in-context tuning-based method that prevents catastrophic forgetting and few-shot learning limitations in continual table semantic parsing.
"We propose a deep generative model for partially identifying counterfactual outcomes in continuous Markovian causal models by bounding curvature, addressing the limitations of existing point-identification methods."
We show that combining Estimation-to-Decisions with optimistic estimation yields improved regret bounds for interactive decision making and model-free reinforcement learning with value function approximation under more lenient estimation error notions.
"ProteinNPT, a non-parametric transformer for protein sequences, outperforms existing methods in predicting multiple protein properties and enables effective iterative protein design under label scarcity."
"ERDiff uses a diffusion model to preserve spatio-temporal structure in latent dynamics during neural signal alignment, outperforming existing methods in fit and decoding on motor cortex data."
R²-GNNs with a linear-time graph transformation achieve the expressiveness of FOC₂ for node classification on multi-relational and temporal graphs.
"We propose using an anisotropic Mahalanobis distance with frozen feature extractors and learned class covariance for exemplar-free class-incremental learning, achieving state-of-the-art results without backbone updates."
"We present the first method to learn provably stable neural Lyapunov control policies for a broad class of discrete-time nonlinear systems, using a mixed-integer linear programming verifier, a verified sublevel set computation, and a counterexample-based heuristic, achieving state-of-the-art performance on standard benchmarks."
Resetting the internal parameters of optimizers at each iteration in deep reinforcement learning significantly improves Atari benchmark performance by preventing contamination of gradient moment estimates when the loss landscape changes.
"Counterfactual fairness can be equated with certain group fairness metrics in specific causal contexts, enabling practical testing via observable group fairness measures."
"YOCO is a one-time dataset condensation method that generates smaller, flexible-sized datasets for on-device training using simple pruning rules, outperforming existing methods in accuracy with significantly reduced dataset sizes."
"We find four distinct optimization regimes in SGD-trained DNNs, with sharpness dynamics depending on learning rate, depth, and width, revealing a sharpness reduction phase at large depth and small width."
"We propose a Product-of-Experts-based post-hoc modification to early-exit neural networks that ensures prediction quality improves monotonically with computation time, enabling truly anytime predictive modeling."
"We develop a framework to analyze how market conditions affect firms’ incentives to share data for collaborative model training, finding that less product competition and harder learning tasks increase collaboration incentives."
"G2MILP is a deep generative model that creates realistic mixed-integer linear program instances by modeling them as bipartite graphs and using a masked variational autoencoder, without requiring expert-designed formulations."
"We establish sample complexity and sharp excess risk bounds for empirical risk minimization in $p$-norm linear regression, $p \in (1, \infty)$, under varying moment assumptions."
GOBLIN improves sample complexity in multi-task bilinear bandits by learning a shared low-dimensional representation across tasks.
"Repeating pre-training data in large language models causes overfitting and degradation, with dataset size, model size, and objective most affecting this, and only dropout effectively mitigates it when carefully tuned."
"We design incentive-compatible, no-regret algorithms for strategic expert advice with modular or submodular utility over multiple experts, extending beyond the single-expert case."
"We propose a cascaded hybrid optimization for vertical federated learning that applies zeroth-order optimization only to the most critical layer, achieving improved convergence and privacy with lower communication cost compared to state-of-the-art methods."
COIN is a simple yet effective cooperative MARL exploration method that combines curiosity-driven and influence-based intrinsic rewards to improve exploration in diverse scenarios.
"We present a unifying framework for model-heterogeneous federated learning with online model extraction and prove convergence to a stationary point under general conditions, introducing minimum coverage index and model reduction noise as key factors affecting convergence."
"FAPAT enhances session-based recommendation by using frequent attribute patterns to capture user intent, outperforming state-of-the-art methods by 4.5% on next-item prediction and related tasks."
"We introduce a reduction-based framework that converts multi-batched algorithms for sequential decision making into sample-efficient methods for settings with stochastic delayed feedback, achieving improved or first results across bandits, tabular and function approximation MDPs, and MGs."
We identify caption quality and density as key factors limiting compositional reasoning in Vision-Language models and propose a fine-tuning method that improves CLIP's performance by up to ~27% over the base model on compositional reasoning tasks using CC3M data.
"IndeCateR, a new unbiased gradient estimator based on the CatLog-Derivative trick, achieves significantly lower bias and variance than REINFORCE for products of independent categorical distributions."
"Transformers primarily improve the memory, but not the credit assignment, capabilities of RL algorithms."
SynGen improves text-conditioned image generation by aligning cross-attention maps with syntactic entity-modifier bindings during inference using a novel loss function.
"We introduce Super-CLEVR-3D and PO3D-VQA, a dataset and model for 3D-aware VQA that outperform baselines but reveal substantial room for improvement compared to 2D VQA."
"We propose a method that generates adversarial examples at flat local minima by penalizing gradient norm with an efficient approximation, significantly improving adversarial transferability on ImageNet-compatible datasets."
"We empirically analyze the relationship between margin, smoothness, and flatness measures and the robust generalization gap across over 1,400 adversarially trained models on CIFAR-10 and ImageNet, revealing when these measures predict robust test performance."
We propose and analyze an active learning algorithm for bipartite ranking that is PAC and provide theoretical and empirical evidence of its efficiency compared to naive methods.
"DIFFER is a framework that decomposes team rewards into individual rewards using gradient invariance, improving learning efficiency and fairness in cooperative MARL."
"Energy-based normalizing flow (EBFlow) enables efficient flow-based modeling by bypassing Jacobian determinant computation during score-matching optimization, achieving faster training and improved performance than maximum likelihood estimation."
"We propose a one-gradient-query-per-round online convex optimization algorithm with two-level adaptivity that achieves problem-dependent regret bounds for strongly convex, exp-concave, and convex losses, unifying worst-case and small-loss guarantees via a multi-layer ensemble with optimism and cascaded corrections."
NAS-X improves latent variable model inference and learning in sequential settings using neural adaptive smoothing with smoothing SMC and outperforms previous methods.
"We propose AGD, an adaptive optimizer using gradient differences as preconditioning and an auto-switch to SGD, which outperforms state-of-the-art optimizers across NLP, CV, and RecSys tasks."
"We propose a feature dispersion-based score to estimate OOD test accuracy, showing inter-class dispersion strongly correlates with generalization while intra-class compactness does not."
"Image captioning pretraining produces vision encoders that rival or surpass contrastive pretraining on classification and vision-language tasks when training data, compute, and model size are matched."
"We present a convergent polynomial approximation to the Gaussian mixture model entropy with theoretical guarantees and experimental validation, addressing the divergence of previous methods."
We present a single-feed-forward method that generates a 360-degree 3D textured mesh from a single image by using Zero123 to create multi-view images and reconstructing them into consistent 3D geometry via an SDF-based approach.
"We propose Self-Supervised Feature Adaptation (SSFA), a framework that adapts feature extractors to the unlabeled data distribution to improve pseudo-label quality in semi-supervised learning when labeled and unlabeled data differ."
"TEEN, a new ensemble deep reinforcement learning algorithm designed to maximize return and promote trajectory diversity, outperforms baseline ensemble DRL methods by 41% on average across tested environments."
"We propose a Laplace approximation with natural Gaussian parametrization and empirical Bayes regularization for scalable, well-regularized heteroscedastic neural regression that improves mean fits and yields epistemic uncertainty without hyperparameter tuning."
"We introduce a game-theoretic Combinatorial Group Testing framework with selfish agents and show that when k is known, adaptive strategies achieve near-optimal learning time O(k log(n/k)), but when k is unknown, learning time is Ω(n^k), demonstrating a strong separation from classic CGT."
"MEX is an unconstrained, easy-to-implement reinforcement learning framework that balances exploration and exploitation with general function approximators, achieving sublinear regret, lower computational cost, and improved performance over baselines in sparse-reward environments."
"Koopma, a Koopman-based forecaster, disentangles and models time-variant and time-invariant components in non-stationary time series to achieve competitive forecasting performance with significantly reduced training time and memory usage."
"The switching subgradient method achieves the same oracle complexity as double-loop methods for non-smooth non-convex constrained optimization with convex or weakly convex constraints, using only a single loop."
"We propose a multi-mode token-level prompt tuning framework using optimal transportation for fine-grained cross-modal alignment, improving generalization and few-shot performance in open-world visual concept comprehension."
"We apply knowledge distillation to accelerate molecular graph neural networks, significantly improving predictive accuracy for energy and force while maintaining inference speed."
"We propose a metric embedding–based initialization method for k-median clustering in general metric spaces that outperforms k-median++ in accuracy and efficiency for moderate k, and extends to differential privacy with improved approximation guarantees."
"The (d–1)-dimensional Weisfeiler–Lehman test distinguishes any two non-isometric point clouds in d-dimensional Euclidean space in at most three iterations, and one iteration suffices for the d-dimensional test."
"MultiFusion enables text-to-image diffusion models to generate images from multilingual, interleaved multimodal inputs by integrating pre-trained modules without retraining."
"VAPOR formulates efficient exploration in RL as a tractable Bayesian approximation of the posterior probability of state-action optimality, connecting to Thompson sampling and outperforming baselines in deep RL experiments."
"ALIA uses language-guided image editing with large vision and language models to augment limited fine-grained classification datasets, improving generalization beyond traditional augmentation."
"Unbiased compression in distributed optimization can reduce total communication cost by up to $\Theta(\sqrt{\min\{n,\kappa\}})$ if and only if the compressors used by all workers are independent and satisfy certain smoothness conditions."
We propose an optimal dynamic-regret algorithm for adversarial linear-mixture MDPs with unknown transitions and provide a near-optimal extension to the non-stationary setting.
The paper introduces gradient-free Stein discrepancies for posterior approximation with theoretical convergence guarantees and demonstrates their application to sampling and variational inference.
We introduce a mixed adversarial attack combining reward and action poisoning that effectively manipulates multi-agent reinforcement learning agents without prior environment or algorithm knowledge.
"We propose Generalized Logit Adjustment (GLA), an optimization-based debiasing method that significantly improves foundation model performance on diverse tasks by addressing inherent training data biases overlooked in previous fine-tuning and ensembling approaches."
"We propose a visual localization method that achieves accurate pose estimation using only a few posed images with coarse pseudo-3D labels from NeRF, outperforming state-of-the-art methods with up to 95% less training data."
"Geoformer, a geometric Transformer with Interatomic Positional Encoding, outperforms state-of-the-art models on QM9 and Molecule3D molecular property prediction tasks."
"We propose a test-time adaptation method that corrects label shift in the joint label-nuisance label space using EM and unlabeled target data, improving performance without modeling features."
"We propose an SE(3)-equivariant, category-agnostic architecture and online training strategy for unsupervised rigid segmentation and motion estimation in dynamic point clouds, achieving strong performance and efficiency with minimal parameters."
"RangePerception, a new RV-based 3D object detection framework with Range Aware Kernel and Vision Restoration Module, outperforms prior RV methods and matches BEV-based CenterPoint in accuracy while being 1.3× faster."
"We propose a neural network method to directly model the entire Pareto set in multiobjective optimization, linking it to hypervolume maximization and validating it on benchmark and real-world problems."
"The authors propose Concept Curation (CoCu), a method that uses CLIP to close the semantic gap between visual and textual modalities in language-supervised semantic segmentation pre-training, significantly improving zero-shot segmentation performance."
"We introduce a tighter, practically reliable upper bound on deep neural network error under distribution shift using unlabeled test data and a theoretically justified disagreement loss."
"Django is a backdoor detection framework for object detection models that uses a dynamic Gaussian weighting scheme to improve trigger inversion by addressing optimization objective misalignment caused by varying poison effects across bounding boxes, outperforming state-of-the-art baselines with up to 38% higher accuracy and 10× lower overhead."
"We propose DPM-Solver-v3, a fast ODE solver for diffusion probabilistic models that uses optimal parameterization based on empirical model statistics and achieves state-of-the-art sample quality with significant speedup."
"We propose a filter subspace method to compute neural network representational similarity via cosine distance of shared filter atoms, achieving massive efficiency gains and robustness compared to probing-based methods."
We reformulate discrete feature transformation as continuous space optimization and introduce a four-step embedding-optimization-reconstruction framework that improves stability and robustness while reducing search space complexity.
"MagDM extends diffusion maps to asymmetric data using the magnetic transform while preserving diffusion distance and avoiding divergence, and demonstrates effectiveness on synthetic and trophic network datasets."
"We analyze gradient descent in two-layer neural networks and prove a sharp step-size threshold below which learning threshold-like neurons fails, explaining how large learning rates can improve generalization via the edge of stability phenomenon."
"We present a grid-independent latent PDE model with an amortized variational inference and multiple shooting that learns from noisy, partial, and irregular spatiotemporal observations and outperforms prior methods on synthetic and real data."
"HiDe-Prompt improves continual learning under self-supervised pre-training by explicitly optimizing hierarchical prediction, identity inference, and adaptation using ensemble prompts and contrastive regularization."
"CAAFE uses a large language model to generate semantically meaningful features for tabular data, improving ROC AUC and providing interpretable explanations."
BrainDiVE enables fine-grained mapping of human visual cortex functional organization by synthesizing images that activate specific brain regions using brain-guided diffusion models.
"We introduce LuminAIRe, a conditional image repainting task that incorporates estimated 3D lighting and physically-based illumination rendering to produce lighting-realistic repainted images, validated on a new annotated dataset."
"We propose a diffusion-based generative sequence model for long-term, vision-based, and multi-task decision-making that outperforms offline RL methods on long-horizon, sparse-reward, and vision-based tasks."
"We provide a general framework proving uniform-in-time propagation of chaos for mean-field Langevin dynamics under finite-particle, time-discretization, and stochastic gradient errors, with quantitative convergence rates for mean-field neural network and MMD minimization using SGD and SVRG, and improved rates for standard Langevin dynamics."
"We derive identifiability conditions for expected outcomes under new interventions using interventional factor models and factor graphs, given data from multiple experimental regimes."
"We propose test-time training strategies, including a mask cycle consistency variant, that significantly improve matching-based video object segmentation under distribution shifts such as corruptions and sim-to-real transfer, and introduce DAVIS-C to benchmark these extreme shifts."
"We theoretically analyze and mitigate spurious feature learning in two-layer convolutional networks via a progressive, group-balanced training algorithm that improves worst-group accuracy by 2.8% and speeds training by up to 10×."
"We propose FedBR-BG, a federated learning protocol with a budget-balanced payment mechanism that achieves superior p-mean welfare compared to standard and best-response-based federated learning methods."
"LODE is a replay-based method for task-agnostic continual learning that decouples and separately weights the objectives of distinguishing new classes from old ones and between new classes, improving the stability–plasticity trade-off and outperforming state-of-the-art methods."
"The paper proposes a Dual-Guided Spatial-Channel-Temporal attention mechanism that adaptively extracts relevant multi-modal features using audio and visual prompts, improving performance in audio-visual downstream tasks including few-shot and zero-shot settings."
"OTCS is a conditional score-based diffusion model that leverages optimal transport to enable effective training on unpaired or partially paired datasets, improving generation in tasks like unpaired super-resolution and semi-paired image translation."
We analyze the convergence rates of Bayesian local optimization algorithms and show that local solutions of Gaussian process sample paths perform surprisingly well compared to global methods.
Curriculum learning with sparse-to-dense data enables 2-layer ReLU networks to learn high-degree parities in fewer steps than fully connected networks trained on unordered data with standard SGD.
The Spike-driven Transformer achieves state-of-the-art accuracy on ImageNet-1K using only sparse addition operations and an event-driven spike-based self-attention mechanism.
"We show that directly estimating the most likely function from data and model, under certain conditions, yields flatter minima, better generalization, and increased robustness compared to parameter-space MAP estimation with neural networks."
"Scale-teaching improves the robustness of deep neural networks to noisy labels in time series by combining fine-to-coarse cross-scale fusion, cross-teaching with small-loss sample selection, and multi-scale embedding graph learning for label correction."
"We present Motion Planning via Optimal Transport (MPOT), a gradient-free, highly parallelizable method that optimizes smooth, high-dimensional trajectories using a Gaussian Process prior and a Sinkhorn-based update rule acting as a trust region search, demonstrating superior efficiency over existing motion planners."
AmadeusGPT is a natural language interface that converts behavioral descriptions into executable code for animal behavior analysis using a dual-memory mechanism to overcome LLM context limitations.
"Scenario Diffusion is a diffusion-based architecture that generates controllable synthetic traffic scenarios by conditioning on maps and scenario tokens, modeling diverse agent poses, orientations, and trajectories simultaneously."
We derive a generalization bound for continuous-time parameterized ODEs and deep residual networks based on the magnitude of differences between successive weight matrices.
"We propose value-conditional state entropy maximization to balance exploration in supervised reinforcement learning, significantly accelerating learning across multiple benchmarks."
"We introduce data-algorithm compatibility to analyze generalization in overparameterized linear regression with gradient descent, showing that early stopping enables generalization under weaker conditions than last-iterate analysis."
We propose a single-view method for high-fidelity 3D head avatar reconstruction and animation that generalizes to unseen identities without person-specific optimization and captures detailed appearance beyond the face.
"We propose d-DRFWL(2) GNNs, which efficiently count up to 6-cycles by restricting message passing to node pairs within distance d, achieving strong cycle counting with low time and space complexity."
"UltraRE improves recommendation unlearning efficiency and utility by addressing redundancy, relevance, and combination losses in an ensemble-based framework."
We present a fully unsupervised method for multi-object segmentation in real-world video sequences that uses temporal slot binding and feature reconstruction with token masking and slot merging.
"We propose a distribution-free framework to control statistical dispersion of loss distributions with societal implications and demonstrate it in toxic comment detection, medical imaging, and film recommendation."
"GUST, a biologically constrained spiking network, learns to group sensory information unsupervised by leveraging neuronal coherence, progressing from global to local feature extraction and enabling compositional generalization."
We propose training neural operators with optimal transport or contrastive losses to better preserve invariant measures of chaotic attractors when forecasting in multi-environment chaotic systems.
"TMT-VIS improves video instance segmentation by incorporating taxonomy-aware joint training across heterogeneous datasets, achieving new state-of-the-art results on multiple benchmarks."
OpenMask3D is a zero-shot open-vocabulary 3D instance segmentation method that generates class-agnostic instance masks and aggregates per-mask features using multi-view CLIP embeddings to enable segmentation of novel object categories.
"We propose the intrinsic dimensionality of embedding manifolds as an invariant feature that robustly distinguishes human from AI-generated texts across languages and models, enabling a highly accurate model-agnostic AI text detector."
"We show that an O(1)-IP stable clustering always exists for general metrics and provide an efficient algorithm for it, and extend IP stability to maximum and minimum distances with efficient near-optimal algorithms."
"We propose OFA-KD, a simple one-for-all knowledge distillation framework that aligns heterogeneous teacher-student model features and enhances distillation performance across different architectures."
"We propose Feature-Level Self-supervised Learning (FLSL), a bi-level clustering SSL method that improves dense prediction performance by aligning feature clustering with semantic image content."
"We propose a self-supervised deep recurrent neural network framework that, without using position labels, learns multiple grid cell modules with strong generalization, integrating insights from dynamical systems, coding theory, optimization, and deep learning."
"This paper establishes theoretical foundations and demonstrates that sparse recurrent neural networks yield consistent estimation and reliable uncertainty quantification for dependent data such as time series, outperforming existing methods in prediction and model compression tasks."
"We propose Feature Shift Tuning (FST), a tuning-based method that disentangles backdoor and clean features to effectively defend against backdoor attacks in deep neural networks even at low poisoning rates."
"We show that neural network equivariance does not generally imply layerwise equivariance, but empirically trained CNNs tend to exhibit it, offering a new perspective on the permutation conjecture."
"We show that predictors approximately locally minimizing a proper loss under Lipschitz post-processing are smoothly calibrated, and vice versa."
"P-FWS is a polynomial-time algorithm for combinatorial semi-bandits that achieves instance-specific minimal sample complexity in the high confidence regime and polynomial sample complexity in the moderate confidence regime, closing the computational-statistical gap by solving the information-theoretic lower bound optimization online via a single Frank-Wolfe update per round."
We propose a Riemannian exponential augmented Lagrangian method with an adaptive inexact Riemannian Barzilai-Borwein method using Sinkhorn iteration to efficiently compute the Projection Robust Wasserstein distance.
"We present a unified Lyapunov-based framework for deriving time-uniform Wasserstein stability bounds for stochastic gradient descent in both convex and non-convex settings, revealing the role of ergodicity and additional noise in achieving such bounds."
HINT is a hierarchical neural network for interpolation that leverages residuals at observed points and local constraints to significantly improve interpolation accuracy across diverse datasets.
"RegORL is a provably efficient modular algorithm for offline reinforcement learning in unknown regular decision processes, providing the first sample complexity bounds for this setting."
"We generalize the non-uniform smoothness condition and provide a new gradient-trajectory bounding analysis that achieves classical convergence rates for SGD and Nesterov's AG under heavy-tailed noise, without gradient clipping."
"We propose a Bayesian-based active learning method for GNNs that efficiently selects unlabeled nodes using a closed-form acquisition function derived from expected model change, improving accuracy and efficiency without retraining."
We prove global convergence of encoder-only shallow Transformers under realistic settings by analyzing softmax in self-attention and showing quadratic overparameterization with standard initialization suffices.
"We introduce a ""weak discrete gradient"" concept enabling a unified, abstract framework that bridges continuous differential equation approaches and discrete convex optimization methods, recovering and generalizing convergence rates of standard algorithms."
"We propose two regularization methods that improve the interpretability and generalization of part-based representations in few-shot learning by mitigating incidental background correlations, achieving state-of-the-art results on benchmark datasets and robustness under domain shifts and data corruption."
"We present the first near-optimal $(1+\varepsilon)$-approximation algorithm for $(k,z)$-clustering in the sliding window model using near-linear space and introduce online coresets, showing they require more samples than offline coresets."
"We propose MixFormerV2, a fully transformer-based tracker without dense convolutions or complex score heads that uses special prediction tokens and efficient distillation to achieve high accuracy and real-time speeds on standard benchmarks."
"We find that popular ML frameworks lose over 40% of key functions and suffer severe performance slowdowns when ported to different hardware, revealing a significant exploration cost due to poor portability."
We extend inverse reinforcement learning to learn safety constraints from expert demonstrations in a multi-task setting to avoid overly conservative constraints.
A PointNet++-based neural network with a novel sampling scheme and class-balanced loss improves leaf vs. wood point classification in sparse UAV Lidar point clouds.
"We propose a task grouping method to mitigate task competition in real-world image super-resolution multi-task learning, significantly improving performance across diverse degradation scenarios."
"We analyze stochastic gradient descent for learning the optimal Kalman gain in linear systems with unknown noise covariances, providing convergence analysis and bias-variance error bounds that scale logarithmically with dimension."
We provide tight bounds on the sample complexity for tuning regularization coefficients in linear and logistic regression under $\ell_1$ and $\ell_2$ constraints by analyzing the pseudo-dimension of validation loss function classes and introducing a new approximation approach for logistic regression.
"Gauss-Newton converges faster than gradient descent for over-parameterized one-hidden-layer networks in the mean-field regime but, with small initialization variance and step size, exhibits a hidden learning phenomenon that trades off convergence speed for better generalization."
"We propose DPOK, an online reinforcement learning method with KL regularization for fine-tuning text-to-image diffusion models using a learned reward function, showing superior image-text alignment and image quality compared to supervised fine-tuning."
TEEN improves few-shot class-incremental learning by calibrating new class prototypes with weighted base class prototypes to enhance new class discriminability.
"We propose a Prompt-Transformer that predicts optimal text prompts for frozen LLMs to improve vision-language pre-training using only linguistic data, enhancing performance and reducing reliance on large image-text datasets."
"We present an open-world 3D indoor instance segmentation method that distinguishes known and unknown object classes, using auto-labeling for pseudo-labels and improving unknown class probability with objectness scores, and introduce realistic open-world splits."
DAC-DETR improves DETR training by decoupling and enhancing the cross-attention component to mitigate conflicting attention effects.
OCRA is a model that extracts explicit object and relational representations to achieve strong systematic generalization on complex visual reasoning tasks.
"We propose an enhanced time-based loss function and threshold-based weight scaling for training spiking neural networks, achieving superior performance and clarifying the interplay between rate and temporal coding."
"We propose a generative subgame solving framework that reduces subgame size via a diversity-based node selection function, significantly improving performance on medium and large games compared to blueprint methods."
"Contextualized World Models (ContextWM) enable effective unsupervised pre-training on in-the-wild videos by separating context and dynamics, improving sample efficiency in model-based reinforcement learning across diverse visual control tasks."
"We propose Conservative State Value Estimation (CSVE), a method that imposes penalties on out-of-distribution states to learn conservative value functions, improving policy optimization and performance in offline reinforcement learning."
"$\beta$D-Bayes enables more precise differentially private parameter estimation for general models, including complex classifiers and neural networks, by minimizing $\beta$-divergence in posterior sampling without altering the underlying model."
"We show that multiple Lipschitz network structures can be reparameterized as special cases of Lipschitz-bounded equilibrium networks without changing the Lipschitz constant, enabling improved certified robustness of deep equilibrium models."
"We propose a loss path kernel that connects gradient flow-trained neural networks to kernel machines and derive a tight, predictive generalization bound, which guides effective neural architecture search."
"We propose an affinity propagation-based method with local and global pairwise terms for weakly-supervised segmentation that improves performance on box-, point-, and scribble-supervised tasks."
"The paper automates the discovery of neural circuit connections in transformers, validating its method by rediscovering key components of a Greater-Than computation circuit in GPT-2 Small."
"We propose a monocular 3D prior-based method to calibrate 4 DoF intrinsic parameters from undistorted images using an incidence field representation, demonstrating superior performance and enabling downstream 3D sensing and image manipulation tasks."
"We propose ATOL, a data generation-based OOD detection method that leverages an auxiliary task using mistakenly generated OOD data to improve real OOD detection when ID and OOD supports are disjoint."
"We analyze how numerical instability in variational flows affects sampling, density estimation, and ELBO computation, and propose a diagnostic to validate results despite instability."
"GINO is a geometry-informed neural operator that efficiently learns solution operators for large-scale PDEs with varying shapes using signed distance functions and graph-Fourier neural architectures, achieving a 26,000x speedup over CFD and improved accuracy on unseen geometries and boundary conditions."
"We adapt the nearest neighbour rule to the contextual bandit problem, achieving polylogarithmic per-trial time and quasi-linear space with sublinear regret in stochastic settings and single-neighbour efficiency in online classification under certain conditions."
FC-CLIP is a single-stage open-vocabulary segmentation model that uses a shared frozen convolutional CLIP backbone to outperform previous two-stage methods on multiple benchmarks with significantly less computation and fewer parameters.
"We propose GIF, a guided dataset expansion framework that uses generative models to create informative new labeled samples, significantly boosting model accuracy on natural and medical image datasets."
"One-step differentiation provides a computationally efficient and user-friendly alternative to automatic and implicit differentiation for fast iterative algorithms, with rigorous theoretical analysis and numerical validation in optimization contexts."
BOSS is a scalable algorithm for learning DAGs that outperforms existing methods on fMRI data by efficiently searching over variable orderings with grow-shrink trees.
TAILO improves robustness and effectiveness of offline imitation learning from observations by weighting behavior cloning with a trajectory-aware discriminator-based discount factor.
"We propose LD2, a scalable heterophilous GNN that decouples graph propagation from embedding generation, achieving optimal time complexity and low memory use while improving speed and maintaining performance on large-scale heterophilous graphs."
"Target Charging Technique (TCT) is a unified privacy analysis framework for differentially private repeated dataset access that significantly reduces privacy cost for computations missing a specified target, generalizing existing techniques to arbitrary private algorithms."
Multimodal learning improves generalization over unimodal learning by up to a factor of O(√n) when modalities are connected and heterogeneous.
"HyTrel is a tabular language model that leverages hypergraphs to encode permutation invariance and structural properties of tables, yielding more robust and invariant representations than baselines on downstream tasks."
FedMRUR improves federated learning convergence and accuracy under data heterogeneity by using a hyperbolic manifold regularizer to reduce model inconsistency and a novel global optimizer to mitigate update norm reduction.
"We propose two data augmentation techniques, DP-Mix_Self and DP-Mix_Diff, designed for differentially private learning that outperform naive methods by using self-augmented data and diffusion-generated synthetic data, respectively."
"ExPT is a transformer foundation model pretrained on synthetic data to enable few-shot experimental design via in-context learning, outperforming existing methods with limited labeled examples."
"MeGraph integrates local and hierarchical graph information via a multi-scale mega graph with alternating message passing and fusion, achieving superior long-range interaction capture and strong performance on graph benchmarks."
"We derive a circuit-level Hebbian learning rule with recurrent inhibition that reconciles error-modulated and postsynaptic activity-based plasticity, matching backpropagation performance and predicting inhibitory modulation of excitatory plasticity."
"We present a general learning-augmented framework for energy-efficient scheduling that improves performance with accurate predictions while maintaining worst-case guarantees, and demonstrate its effectiveness on real and synthetic data."
"AMAG, a graph neural network-based latent variable model trained on neural activity forecasting with interaction priors, accurately recovers neural interactions and predicts future neural population dynamics from multiple recording modalities."
"We present a sublinear-time spectral clustering oracle for graphs with well-separated clusters that achieves consistent clustering with relaxed conductance gap assumptions but higher misclassification, and demonstrate robustness to edge deletions via experiments."
"Structure from Duplicates (SfD) is a single-image inverse graphics method that leverages multiple identical objects to jointly estimate their poses and reconstruct shared shape, material, and illumination with higher realism and detail than existing methods."
"We systematically analyze the expressive power of graph neural networks for link prediction on knowledge graphs, revealing theoretical justifications for common design choices and extending relational Weisfeiler-Leman algorithms."
"GEESE, a neural network-based method using hybrid surrogate models and generative distributions, detects and corrects failed machine learning state estimations in scientific and engineering inverse problems with fewer failures and fewer physical evaluations than state-of-the-art methods."
"We propose a dynamic timing representation with dilated convolutions and layer attention for efficient spike stream feature extraction and introduce an unsupervised spike-based optical flow method with a synthetic validation dataset, achieving up to 19% error reduction over prior spike-based methods on real and synthetic datasets."
"We propose H-LINUCB, a provably optimal distributed algorithm for linear contextual bandits that achieves minimal group regret when environment heterogeneity is known."
"We propose a curriculum-based method with a maximum entropy objective and mixture of experts to mitigate mode-averaging in imitation learning from diverse human demonstrations, outperforming state-of-the-art methods on complex control tasks."
"Landmark attention enables efficient processing of arbitrarily long contexts in Transformers by using landmark tokens to select relevant input blocks within the attention mechanism, extending context length without sacrificing random-access flexibility."
We introduce a convex approximation of individual fairness constraints and certify distributional individual fairness in large neural networks under distribution shifts using quasi-convex optimization.
"Dynamical Similarity Analysis (DSA) is a new metric that compares neural networks based on their dynamics rather than latent state geometry, accurately distinguishing conjugate and non-conjugate RNNs and learning rules."
"We model evaluation biases as a loss minimization process under information constraints and show that two parameters govern the emergence of bias, providing a framework to analyze and mitigate such biases in evaluation processes."
"Subsampling and ridge regularization for ensemble ridge estimators are asymptotically equivalent under specific regularization and subsampling parameter paths, implying monotonic prediction risk in the data aspect ratio under mild assumptions."
"In stochastic convex optimization, true risk minimization requires dimension-dependent mutual information, revealing that current information-theoretic generalization bounds cannot explain the dimension-independent generalization of SGD and regularized ERM."
"We propose a Fisher information-based measure, dFIL, to theoretically and empirically bound the invertibility of instance encodings and interpret their privacy."
"Eliminating specific linguistic information from language model representations significantly reduces alignment with human fMRI brain recordings during language processing, with syntactic properties having the largest impact."
"MIRL alleviates the degradation problem in deep Vision Transformers during masked image modeling, enabling significantly deeper models to be efficiently pre-trained with improved accuracy and generalization at lower computational cost."
"We propose a generative model based on the semi-dual Unbalanced Optimal Transport formulation that is more robust to outliers, trains more stably and converges faster, and achieves state-of-the-art FID scores on CIFAR-10 and CelebA-HQ-256."
"We introduce the λ representation, a novel state representation required for policy evaluation under diminishing marginal utility in sequential tasks, generalizing the successor representation and relevant for machine learning and foraging studies."
"Knowledge distillation exaggerates the teacher's implicit bias and confidence levels, which explains improved student generalization despite diverging probabilities."
"We empirically show that pre-training on high-resource tasks and fine-tuning on a mix of high- and low-resource tasks improves performance on imbalanced multi-task learning, particularly in NMT and multilingual language modeling."
ProST addresses time synchronization between agents and non-stationary environments by optimizing interaction timing to minimize dynamic regret in high-dimensional settings.
"We derive and approximate instance-specific regret lower bounds for multi-agent multi-armed bandits on factor graphs, and propose ESM, an algorithm whose regret asymptotically matches the approximation, with empirical validation in communication networks."
"We propose a dual self-awareness value decomposition framework that abandons the Individual Global Max assumption and introduces anti-ego exploration, achieving strong performance in cooperative multi-agent tasks."
"We show that DP-SGD's privacy parameters do not reliably control resistance to training data reconstruction, and provide a tight bound and matching attack to quantify reconstruction success, suggesting that DP guarantees alone are insufficient for this protection."
ReNO is a framework addressing errors and loss of continuous structure in neural operator learning caused by operator aliasing during discretization.
Computer vision models achieve better out-of-distribution generalization when their representations are factorized between object and background features and appropriately weigh these features.
"We formalize long-term fairness as an online reinforcement learning problem and propose an algorithm that, by sacrificing short-term incentives, drives the system toward equitable equilibria while maintaining bounded fairness violations and loss, outperforming myopic and distributionally robust baselines in human-population simulations."
"AbDiffuser is an equivariant, physics-informed diffusion model that generates antibody 3D structures and sequences efficiently and accurately, validated in silico and in vitro."
"We derive the limiting distributions of change point estimators in non-parametric multivariate time series with Hölder smooth densities and develop a sharp change point locator and a consistent long-run variance estimator, with theoretical and numerical support."
We propose a rotationally symmetric shared random codebook using a randomly rotated simplex and prove that its k-closest encoding achieves exact optimality for mean estimation under communication and local differential privacy constraints when shared randomness is available.
AVIS is an autonomous visual question answering framework that uses an LLM with tree search and user-derived decision models to achieve state-of-the-art performance on knowledge-based VQA benchmarks.
MeCo is a memory- and computation-efficient defense against data-free model extraction that randomizes inputs to mislead attackers and preserve model utility.
"We propose a joint maximum likelihood learning framework for energy-based models and generator models, using mutual MCMC-based teaching to improve EBM training accuracy and MCMC initialization."
"Language models exhibit significant multilingual tokenization disparities that unfairly affect processing costs, latency, and context length across languages."
"We propose IDEA, a model that decomposes image features into causal and non-causal components and uses causal features for hashing with domain invariance via synthetic intervention to improve unsupervised domain adaptive hashing."
"We propose fragment-level GNN pretraining using mined molecular fragments and contrastive and predictive tasks, improving performance on multiple molecular benchmarks."
"We propose a maximum average dot product agglomerative clustering variant that better recovers hierarchical structure under a probabilistic graphical model, outperforming UPGMA, Ward’s method, and HDBSCAN on real data."
"We propose an online projected gradient ascent algorithm that achieves no-regret learning and convergence to the unique stationary Nash equilibrium in a repeated dynamic pricing game with uninformed firms and MNL demand, even without strong monotonicity or variational stability, and show convergence at rate $\mathcal{O}(1/t)$."
"We close the gap in sample complexity for learning a single index model under isotropic Gaussian data by showing that online SGD with a smoothed loss achieves the lower bound of $n \gtrsim d^{k^\star/2}$, matching the CSQ lower bound for gradient-based methods."
We identify self-excitation as the root cause of Q-value divergence in offline RL and show that using LayerNorm in Q-networks effectively prevents divergence and achieves state-of-the-art performance even with limited data.
"We propose VPGTrans, a two-stage framework that efficiently transfers a visual prompt generator from a small to a large LLM, drastically reducing computational and data requirements compared to training from scratch."
"Numerical deviations in CNN inference arise mainly from hardware-specific optimizations such as SIMD use and GPU convolution algorithm selection, and these effects depend on model properties."
"We demonstrate that Weitzman’s rule for Pandora’s Box with independent value distributions remains optimal and provides significantly better approximation guarantees for correlated distributions, and show polynomial sample complexity for implementation in the correlated case."
"We propose a causal inference-based PU learning algorithm that uses normalized propensity scores and NIPW to improve classification accuracy under selection bias, even when the labeling mechanism is unknown."
"We propose a statistically guaranteed, low-computation method that characterizes and removes procedural uncertainty in over-parameterized neural networks using a single auxiliary network, enabling accurate uncertainty quantification with as few as four trained networks."
"We adapt the Shapley value framework to explain predictive uncertainty by quantifying each feature's contribution to conditional entropy, connecting it to information theory and providing efficient algorithms with theoretical guarantees for applications such as covariate shift detection and active learning."
"We systematically evaluate pre-trained visual representations for Embodied AI using a large benchmark and diverse pre-training data, finding that no single model universally dominates and that adaptation improves performance across tasks."
"We propose a no-regret Gaussian process upper confidence bound algorithm for Bayesian optimization with cost-varying variable subsets, demonstrating improved solution quality under budget constraints."
NeuralEF is a neural framework that accelerates large-scale efficient frontier portfolio optimization under heterogeneous constraints by reformulating it as a sequence-to-sequence problem.
"We propose MPVSS, a query-based mask propagation framework for efficient video semantic segmentation that reuses key frame predictions via segment-aware flow maps to significantly reduce computation while achieving state-of-the-art accuracy-efficiency trade-offs on VSPW and Cityscapes."
"Elastic Reset, a periodic reset method using an exponentially moving average, achieves higher reward with less language drift in reinforcement learning finetuning of language models than KL-penalty methods."
"We introduce a large-scale model that annotates hands, objects, and interactions in images with detailed outputs, trained on extensive new annotations."
"RMechRP is a radical reaction predictor using contrastive learning and mechanistic pathways for accurate, interpretable predictions and benchmarking in radical chemistry."
"We present a robust, optimal-communication distributed count tracking algorithm against adaptive adversaries using partial differential privacy, demonstrating that the √k advantage of randomized algorithms does not depend on the oblivious adversary assumption."
"Synthetic Experience Replay (SynthER) uses diffusion-based upsampling of collected experience to improve replay-based reinforcement learning from limited data, enhancing both offline and online training efficiency."
"We derive minimax optimal sample complexity bounds for invariant kernel ridge regression on compact manifolds under smooth Lie group actions, showing sample efficiency gains proportional to group size for finite groups and to quotient space volume and reduced manifold dimension for continuous groups, via a differential geometric approach."
"We prove that learning an unknown quantum state with quantum neural networks is fundamentally limited by an exponential vanishing of success probability with qubit count below a critical loss threshold, regardless of ansatz, initialization, or adaptivity."
"We introduce spanning capacity to characterize PAC learnability in agnostic reinforcement learning with generative access, but show it is insufficient for online access, where an additional sunflower structure is required for efficient learning via the POPLER algorithm."
"We develop a doubly-robust, constrained estimation and policy optimization framework for treatment recommendations under non-adherence, heterogeneity, and lack of positivity, and demonstrate improved, fairer recommendations in a pretrial risk-assessment case study."
"We propose a distribution-free, model-robust predictive inference method for matrix completion using conformal prediction, providing valid coverage guarantees regardless of model accuracy."
E3T is an efficient end-to-end training method for zero-shot human-AI coordination that uses a mixture of ego and random policies and a partner modeling module to achieve high performance without pre-trained populations.
"We propose an adaptive Bayesian algorithm for principled phased technology releases that determines optimal ramp-up sizes by inverting probability bounds using outcome means and variances, efficiently balancing risk and learning without rare-event simulation."
OILCA improves offline imitation learning generalization by generating counterfactual expert data via variational autoencoder-based counterfactual inference.
"We provide sharp injectivity and surjectivity conditions for neural operators using Fredholm and degree theory, and show their finite-rank implementations remain injective and universally approximative, with applications to uncertainty quantification and inverse problems."
"We analyze personalized and unknown strategic manipulations in classification, providing mistake and sample complexity bounds under different observation scenarios and showing that non-ball manipulations require at least Ω(|𝓗|) mistakes/sample complexity when the target is in a known class 𝓗."
"We propose HAP, a masked image modeling pre-training method with human part-based masking and structure-invariant alignment that achieves state-of-the-art results on multiple human-centric perception tasks using a plain ViT encoder."
DELTA is an unbiased client sampling scheme for federated learning that minimizes update variance and accelerates convergence by selecting representative clients based on their information content.
"ARMOR is a model-based offline RL framework that robustly improves upon a reference policy even with limited data coverage, guaranteeing no performance degradation and competing with the best covered policy under suitable hyperparameters."
"We propose an iterative method to align language models by resolving the granularity mismatch between sequence-level preferences and token-level training, using a framework for preference grounding and minimalist learning objectives that achieves competitive results on discrete-prompt generation and text summarization."
"We introduce robust algorithms for Lipschitz bandits under adversarial reward corruption with unknown budget, achieving sub-linear regret and optimality against strong adversaries."
"ComSL achieves a new state-of-the-art average BLEU score of 31.5 on multilingual speech-to-English translation by combining pre-trained speech and language models and using cross-modality, multi-task learning with limited data."
We propose a Bayesian neural network-based probabilistic weight-sharing quantization that leverages position-specific uncertainty to achieve superior compression and accuracy over state-of-the-art methods on ResNet and transformer models.
"TempME improves explainability of temporal graph neural networks by identifying pivotal temporal motifs that drive predictions, outperforming existing methods in accuracy and boosting TGNN performance."
"We propose a joint actor-critic objective aligned with policy improvement, prove its monotonic guarantees, and empirically show its advantage over standard methods in simple RL tasks."
"We propose a Polyhedron Attention Module (PAM) that creates interpretable, adaptive-order piecewise polynomial models by partitioning the input space into polyhedrons and using their boundary hyperplanes to generate feature interactions, outperforming ReLU networks in expressivity and classification tasks."
"IFactor is a framework that identifiably extracts and disentangles four categories of latent state variables for stable and compact reinforcement learning representations, outperforming baselines in synthetic and real environments."
Hierarchical randomized smoothing improves certifiable robustness-accuracy trade-offs in complex data by applying noise only to randomly selected subsets of entities.
"We propose a certified machine unlearning algorithm for minimax models using Hessian-based Newton updates and Gaussian noise, achieving generalization rates and deletion capacity comparable to standard models but with improved dependence on dimensionality."
"Scalarization is theoretically incapable of fully exploring the Pareto front in linear multi-task learning, especially for balanced trade-offs, and experiments confirm that specialized multi-task optimizers can find solutions unreachable by scalarization."
We present a 3D equivariant diffusion model that jointly predicts fragment poses and linker structures for targeted protein degradation without assuming known fragment positions.
"DDF-HO improves hand-held object reconstruction from a single RGB image by using Directed Distance Fields and a novel 2D-3D feature aggregation method, significantly outperforming baselines on both synthetic and real datasets."
Prompt Diffusion is a diffusion-based vision-language foundation model that enables in-context learning for image generation and editing using vision-language prompts.
We develop a causal framework and algorithm for achieving fair and equitable outcome control in automated decision-making systems by addressing and mitigating discrimination in individual benefit.
"We present an online quantum algorithm achieving $\widetilde O(1)$ regret and finding an $\varepsilon$-approximate Nash equilibrium in $\widetilde O(\sqrt{m+n}/\varepsilon^{2.5})$ quantum time for $m \times n$ zero-sum games, based on quantizing the optimistic multiplicative weight update method and a fast quantum multi-sampling procedure."
"We introduce counterfactual memorization to identify and measure how individual training documents affect neural language model predictions when omitted, revealing the source of memorization in standard datasets."
"We demonstrate that vanilla knowledge distillation, when evaluated on large-scale datasets with strong data augmentation, achieves state-of-the-art results, challenging the need for complex KD variants on small-scale benchmarks."
"We present a competitive active agnostic learning algorithm with an $O(\log H)$ query overhead over the optimal, and show that this overhead is NP-hard to improve."
"We show that depth-3 ReLU networks with Gaussian inputs are hard to learn even with non-degenerate weights and noise, and that smoothing both inputs and parameters makes depth-2 networks hard to learn under standard assumptions."
We propose an online mirror descent algorithm that achieves O(√T log T) expected regret for learning expert weights in logarithmic pooling aggregation under adversarial but calibrated expert forecasts.
"CFCQL is a multi-agent offline RL algorithm that separately computes and combines counterfactual conservative Q-value regularizations per agent to address distribution shift and value overestimation, outperforming existing methods in both discrete and continuous action settings."
"MosaicBERT is a BERT-style encoder with architectural and training improvements that achieves faster, cheaper pretraining while maintaining competitive performance."
"We introduce Clevr-4, a synthetic dataset with multiple valid label partitions to rigorously evaluate Generalized Category Discovery, and propose μGCD, a Mean Teacher-based method that outperforms prior work on both Clevr-4 and the Semantic Shift Benchmark."
"We propose a many-body approximation for non-negative tensor decomposition based on energy-based modeling, enabling global optimization via KL divergence minimization and interaction tuning, and demonstrate its effectiveness in tensor completion and approximation."
We propose a novel amortized reparametrization method for inferring latent linear SDEs that reduces model evaluations by over an order of magnitude without increasing time or memory complexity with data size or stiffness.
StreamNet accelerates patch-based TinyML inference on MCUs by using a stream buffer to reduce redundant computation and MAC operations while minimizing SRAM usage.
"TRGL adds a transport-inspired regularization to module-wise training, improving accuracy and mitigating stagnation with up to 60% less memory than end-to-end training."
"We propose linear arithmetic composition of parameter-efficient fine-tuning modules to enhance distribution generalization, multi-tasking, detoxification, and domain transfer without additional training."
"We propose injecting pairwise causal knowledge into transformer-based models for temporal event sequences, improving prediction accuracy and enabling causal inference in societal event sequences generated by large language models."
"We propose a variational Bayesian neural network with relative entropy coding and iterative prior learning for efficient, high-quality functional data compression."
"Cross-Scale MAE, a self-supervised model with cross-scale consistency constraints and xFormers acceleration, achieves superior remote sensing image representation learning compared to standard MAE and existing methods."
"LinGCN is a framework that reduces multiplication depth and improves the latency and accuracy of homomorphically encrypted Graph Convolution Network inference by introducing a differentiable linearization algorithm, a trainable compact activation function, and enhanced operator fusion."
"We introduce Continual Domain Generalization over Temporal Drift (CDGTD) to address gradual, sequential domain shifts and propose EvoS with a multi-scale attention module that standardizes features using evolving domain statistics for effective generalization to unseen future domains."
"We propose a method to identify and estimate the causal effect in linear structural causal models with a single non-Gaussian latent confounder proxy using cross moments, relaxing standard DiD assumptions and showing impossibility under joint Gaussianity."
"QUAM improves epistemic uncertainty estimation by identifying models that maximize the product of posterior and divergence, outperforming existing methods in vision tasks."
"We analyze algorithmic replicability and related properties in discounted tabular MDPs with a generative model, providing upper and lower bounds on sample complexity for replicable and approximately replicable policy estimation algorithms."
"Instruction-tuned LLMs significantly outperform other models at making binary pragmatic inferences (implicatures), indicating that certain fine-tuning strategies improve pragmatic understanding."
We derive a bias-variance tradeoff for graph neural networks under approximate graph symmetries via coarsening and show empirically that optimal generalization arises from choosing a symmetry group intermediate between the graph automorphism and the full permutation group.
We propose a self-adaptive motion tracking network with a learnable affine transformation layer and Fourier-encoded LSTM to robustly handle on-body sensor displacement in wearable flexible sensors.
"Contrastive learning demonstrates robustness to sampling bias via distributionally robust optimization, but suffers from over-conservatism and outlier sensitivity, which we mitigate with an Adjusted InfoNCE loss improving performance and convergence across image, sentence, and graph domains."
"Tree of Thoughts enables language models to explore multiple reasoning paths and make deliberate decisions, significantly improving performance on planning and search tasks compared to chain-of-thought prompting."
"We introduce characteristic circuits, a tractable probabilistic model that unifies spectral-domain representation of heterogeneous data distributions for efficient learning and inference without requiring closed-form densities."
"We present replicable approximation algorithms for statistical k-medians, k-means, and k-centers with provable sample complexity and experimental validation."
"We formalize and evaluate the invariance of interpretability methods for symmetry-invariant neural networks, deriving metrics, theoretical guarantees, and guidelines for robust explanations."
"We propose Dynamic Prompt Learning (DPL), which improves fine-grained image editing in diffusion models by refining cross-attention focus on noun tokens to prevent unintended changes outside the target region."
"Uni3DETR is a unified 3D detector that uses a detection transformer with point-voxel interaction and a mixture of query points, along with decoupled IoU loss, to achieve strong generalization across both indoor and outdoor scenes."
"Score-matching variational inference (GSM-VI), particularly for Gaussian variational families, matches posterior score functions iteratively and achieves comparable or superior approximation accuracy with 10–100 times fewer gradient evaluations than black box variational inference (BBVI)."
"Instability and inconsistency of model outputs, estimable on unlabeled data, are more predictive of generalization gap than loss sharpness, and reducing inconsistency improves generalization."
"We establish a reduction from smoothed sequential probability assignment to transductive learning, yielding optimal logarithmic fast rates and sublinear regret algorithms via an MLE oracle."
"We propose a two-stage probabilistic framework using optimal transport and diffusion models for statistical downscaling with unpaired data, enabling high-resolution output reconstruction and accurate physical statistics recovery without paired data or matched low-frequency content."
A path-following optimization scheme globally converges to the global minimum of the population loss for learning an acyclic directed graphical model in the bivariate setting.
"We propose a quasi-linear complexity algorithm for solving distributionally robust MDPs with Wasserstein ambiguity sets using L1, L2, or Linf norms, outperforming existing methods."
GeoTMI improves quantum chemical property prediction using GNNs by training on corrupted geometries and maximizing mutual information between correct and corrupted geometries and the property.
We propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters that converges for any regularization and extends to an inexact Monte Carlo variant with non-asymptotic convergence guarantees for discrete point clouds.
"We show that linear threshold functions can be efficiently learned from label proportions when features are Gaussian, by estimating covariance-based statistics and using them to identify the normal vector direction."
"Robustified artificial neural networks can generate small perturbations that strongly and precisely alter human object category percepts, demonstrating that human visual perception is more sensitive than previously assumed."
"DDMSL is a probabilistic discrete diffusion model for source localization and reconstruction of information diffusion paths on graphs, based on Markov chain modeling and a reversible residual denoising-diffusion framework."
"SRe²L is a dataset condensation framework that decouples model and synthetic data optimization, achieving state-of-the-art accuracy at low IPC with high-resolution synthesis and significantly lower training cost and memory usage."
"We propose a unified theoretical framework explaining feature learning in two-layer neural networks via gradients, demonstrating its effectiveness across tasks and illuminating phenomena like non-kernel feature learning and the lottery ticket hypothesis."
"SAN introduces slice-level adaptive normalization to better handle non-stationarity in time series by locally normalizing sub-series and modeling evolving statistical properties, improving forecasting accuracy as a model-agnostic plugin."
"We decompose neural prediction error via spectral and geometric analyses of model activations and neural responses, revealing multiple representational geometries that yield low error and offering insights for improving neural activity models."
"We propose a mathematically principled general theory of cooperative communication that defines a spectrum of common ground, beyond perfect knowledge sharing, and connects the process to variational autoencoding via empirical simulations."
"EMNH, a meta neural heuristic with parallel multi-task training and efficient hierarchical fine-tuning, outperforms state-of-the-art neural and traditional heuristics in solution quality and learning efficiency on multi-objective combinatorial optimization problems."
"We propose a Bayesian encoder for metric learning using Laplace Approximation over network weights, prove the contrastive loss is a negative log-likelihood on the sphere, ensure a positive definite covariance, and demonstrate improved calibration, OOD detection, and accuracy."
"We propose a method that uses a learned reward function to pseudolabel unlabeled data and trains a conditional diffusion model to generate samples targeting a specified reward value, theoretically and empirically showing that sample quality depends on reward signal strength, distribution shift, and off-support extrapolation cost."
"Prompt-based large language models can serve as weak learners in boosting algorithms for tabular data when provided with appropriately sampled text descriptions, sometimes outperforming few-shot and fine-tuned approaches, especially with limited data."
"GeoPhy introduces a differentiable, continuous geometric representation enabling scalable variational inference over full phylogenetic topologies without restriction, outperforming existing approximate Bayesian methods on real datasets."
"We introduce the Cost of Learning in Queueing (CLQ), a metric quantifying the early-stage performance degradation in multi-server queueing systems due to parameter uncertainty, and provide a unified analysis framework that bridges Lyapunov and bandit methods."
"Clustered Compositional Explanations generalize the original method by combining it with clustering and a novel search heuristic to better approximate a wider range of neuron behaviors, enabling broader and more complete explanations."
We propose an iteratively reweighted least squares algorithm that recovers row-sparse and low-rank matrices from linear measurements with minimal sample complexity and demonstrate its superior empirical performance.
"We propose a theoretically grounded inference-stage framework that significantly improves detection of backdoor attacks in both CV and NLP models, outperforming state-of-the-art methods by up to 300% AUCROC."
"We study proper PAC learnability under relaxed adversarial loss definitions, showing that some relaxations allow VC classes to be properly learned efficiently, while others do not, and provide new generalization bounds for adversarially robust empirical risk minimizers."
"Attention in Transformers can be interpreted as structural inference over implicit graphical model adjacencies, enabling novel extensions and bridging to Bayesian attention in neuroscience."
"Med-UniC, a cross-lingual medical vision-language pre-training framework using Cross-lingual Text Alignment Regularization, reduces community bias and improves performance on diverse medical image tasks across English and Spanish datasets."
"UBER improves offline reinforcement learning by using random reward priors to extract diverse, useful behaviors from reward-free data, enhancing sample efficiency and reducing reliance on human supervision."
"HiBug is an automated, interpretable debugging framework that leverages pre-trained vision-language models to identify and explain systematic model errors in critical data subsets without heavy human annotation."
"We propose to censor undesirable outputs of a pre-trained diffusion image generation model using a reward model trained on minimal human feedback, requiring only a few minutes of human labeling."
MPDR is an EBM training method using manifold-based perturbations and near-manifold negative samples that improves anomaly detection across diverse data types.
"We show that interpreting signed barcodes as signed Radon measures enables stable and computationally tractable vectorization of multiparameter persistent homology descriptors, improving performance over existing topology-based methods."
"We design online algorithms that achieve nearly-optimal revenue regret for selling multiple items to multiple users with unknown valuations, under three valuation models, with regret bounds of $O(NM\log\log(LT))$, $\widetilde{O}(\sqrt{NMLT})$, and $\widetilde{O}(\sqrt{NMLT})$ respectively."
MMICRL is a multi-modal inverse constrained reinforcement learning method that estimates multiple expert-specific constraints from mixed demonstration data and imitates diverse expert behaviors more effectively than existing baselines.
"CORAL is a coordinate-based neural operator that solves PDEs on arbitrary geometries without mesh constraints, outperforming state-of-the-art methods across resolutions and domains."
We propose and demonstrate that trading model weights as commodities in a parameter market enables mutually beneficial gains over isolated model training.
"We provide the first small-loss and gradual-variation regret bounds for non-Lipschitz, non-smooth losses in online portfolio selection using novel smoothness characterizations and FTRL variants."
"Embroid improves prompt-based learning without extra labeled data by leveraging consistency in model predictions across embedded sample neighborhoods and combining them using a latent variable model, significantly boosting performance across multiple LMs and tasks."
"AlpacaFarm is a low-cost LLM feedback simulator that enables reliable, reproducible instruction-following research by providing cheap human-like feedback, an evaluation dataset, and reference implementation of feedback learning methods."
"ICON improves unsupervised domain adaptation by enforcing prediction consistency with source labels and clustering in the target domain to remove spurious correlations, achieving state-of-the-art results on UDA benchmarks."
"We propose using an inverse dynamics model to guide state recovery in offline reinforcement learning, aligning the policy's state distribution with the offline data at unseen states without explicit distribution prediction."
"A-Crab is a new offline reinforcement learning algorithm that achieves an optimal $1/\sqrt{N}$ convergence rate and weaker average policy coverage assumptions while outperforming baseline policies, with theoretical guarantees and experimental validation."
"We propose a model-agnostic, any-time valid method for constructing certifiable adaptive uncertainty estimates using likelihood ratios, with theoretical guarantees and practical effectiveness demonstrated in generalized linear models and bandit problems."
"A data distribution is suitable for locally connected neural networks if and only if it has low quantum entanglement under canonical feature partitions, and this leads to a preprocessing method that improves neural network performance."
"We derive tight f-differential privacy bounds for discrete-valued data compression and privacy mechanisms and propose a ternary compressor that improves mean estimation accuracy without a three-way tradeoff between privacy, communication, and accuracy."
"We propose using large-scale pre-trained models to guide downstream training with sample difficulty-aware entropy regularization, improving both accuracy and uncertainty calibration."
We modify structured state space sequence models to enable fast parallel initialization and show that the resulting model outperforms RNNs and Transformers on reinforcement learning tasks with long-range dependencies.
The paper presents a label-efficient online binary sequence prediction algorithm with expert advice that achieves optimal regret guarantees in the worst case while requiring only O(√T) labels in benign settings.
"We propose a scaling factor that adjusts attention entropy to improve visual quality and text alignment in text-to-image diffusion models across varying image resolutions and aspect ratios, without additional training."
"We introduce a multimodal graph neural network that learns gene representations capturing functional similarity across diverse biomedical data, outperforming state-of-the-art methods by up to 100.4% and enabling analysis of pathways, regulation, disease genes, and species evolution."
"We introduce structure-preserving bracket-based GNN architectures that provably conserve energy or dissipate it, clarifying the role of reversibility and irreversibility in GNN performance."
"We introduce Reference-Based POMDPs, which use a reference policy to enable efficient computation of near-optimal policies in long-horizon, partially observable settings, outperforming standard POMCP."
"Cal-DETR calibrates detection transformers at train time by quantifying uncertainty and modulating class logits, improving calibration without sacrificing detection performance in both in- and out-of-domain settings."
"We propose a risk-consistent method for binary classification using only pairwise confidence differences as supervision, prove its optimal convergence rate and introduce a risk correction to mitigate overfitting, validated on benchmark and recommender datasets."
"We propose CRoSS, a diffusion-model-based image steganography framework that achieves superior controllability, robustness, and security without extra training."
We introduce a low-variance pairs estimator for causal error under inverse probability weighting that cancels IPW variance and achieves near-RCT evaluation accuracy without modifying the IPW estimator.
"We show that variance-dependent noise addition prevents overfitting in adaptive data analysis by bounding the covariance between new queries and past information leakage, without requiring worst-case scaling or complex algorithms."
"We introduce cyclic walks between vision transformer features and object slots to learn object-centric representations in an unsupervised manner, enabling disentanglement and segmentation without pixel-level reconstruction."
"We propose and analyze an active learning algorithm for multiclass logistic regression that minimizes the Fisher Information Ratio, theoretically bounding excess risk and outperforming existing methods on MNIST, CIFAR-10, and ImageNet."
"We propose SDMGrad and SDMGrad-OS, stochastic gradient methods for multi-objective optimization that achieve improved sample complexity for finding ε-accurate Pareto stationary points and match best-known rates for constant-level conflict-avoidant distance in multi-task and reinforcement learning."
"We theoretically characterize optimal deferral rules for cascades and show that post-hoc deferral outperforms confidence-based deferral when downstream models are specialists, labels are noisy, or there is distribution shift."
"ERM's suboptimality in mean squared error arises from its bias, not its variance, which achieves the minimax rate under mild assumptions."
"We propose an off-policy value estimation and classification-based method to address data non-stationarity, temporal credit assignment, and pessimism in learning initiation sets for hierarchical reinforcement learning, improving option discovery and task performance across multiple environments."
"We present a neural network architecture that efficiently approximates the p-th Wasserstein distance between point sets with model complexity independent of input size, leveraging symmetric and factor-wise group invariant functions and outperforming existing methods in generalization and training speed."
"Intervention-aware Concept Embedding models (IntCEMs) improve concept intervention receptiveness in Concept Bottleneck Models by jointly learning intervention policies during training, resulting in significantly better test-time performance when concepts are intervened upon."
"We propose a multi-task learning method that uses unbiased ratings to correct for hidden confounding in recommender system training, addressing the failure of existing debiasing approaches."
"We introduce the first cyclic coordinate-descent-like algorithm for sequence-form strategy spaces in extensive-form games, achieving faster convergence and empirical performance comparable to or better than state-of-the-art methods, with restarts yielding substantial speedups."
"We propose AutoPoison, an automated data poisoning pipeline that stealthily manipulates instruction-tuned LLMs' behavior by injecting targeted examples into their training data."
"We propose CDDB, a data-consistent Direct Diffusion Bridge method that outperforms prior diffusion-based inverse solvers on both perception and distortion metrics."
"FASA introduces a bidirectional, context-aware mechanism for local and global representation interaction in vision transformers, yielding a lightweight backbone (FAT) that achieves 77.6% ImageNet-1K accuracy with 4.5M parameters and 0.7G FLOPs, outperforming comparable models in accuracy and speed."
"We propose a unified randomized zeroth-order federated learning framework with communication complexity guarantees for nondifferentiable nonconvex, bilevel, and minimax optimization problems, empirically validating its effectiveness on nonsmooth and hierarchical machine learning tasks."
"We introduce NERE, an equivariant SE(3)-invariant rotation prediction network for unsupervised binding affinity estimation via energy-based modeling, outperforming both unsupervised and supervised baselines in antibody-antigen binding prediction."
AsyncFGD reduces memory use and improves hardware efficiency for on-device learning by decoupling layer dependencies via asynchronous forward gradient descent.
"We propose a theoretical framework for U-Net design that reveals their relationship to ResNets, introduces Multi-ResNets with wavelet-based encoders, and demonstrates improved performance in image segmentation, PDE modeling, and diffusion-based generative modeling."
"We introduce Pick-to-Learn, a meta-algorithm that embeds any learning algorithm to yield tight generalization bounds and improved post-training performance on MNIST and synthetic regression tasks."
"We present an uncoupled, convergent, and rational algorithm for learning in two-player zero-sum Markov games with bandit feedback that achieves non-asymptotic convergence rates to Nash equilibrium—$\tilde{\mathcal{O}}(t^{-1/8})$ in stateless games and $\tilde{\mathcal{O}}(t^{-1/(9+\varepsilon)})$ for irreducible games, and a path convergence rate of $\tilde{\mathcal{O}}(t^{-1/10})$ in the general case—without synchronization, prior knowledge, or coupled communication."
"We show that Wishart process models efficiently estimate neural noise covariance across smoothly varying conditions with few trials, providing reliable and smooth estimates of noise correlations and Fisher information in mouse and monkey cortex."
"The Quantization Model explains power-law loss scaling and sudden capability emergence by positing that neural knowledge is partitioned into use-frequency-ordered discrete quanta, whose power-law usage frequencies predict empirical scaling."
"We propose Projection Regret (PR), a diffusion model-based novelty detection method that measures perceptual distance between a sample and its recursively projected in-distribution counterpart to mitigate background bias and improve OOD detection."
"ReHLine is a linearly convergent, linear-time algorithm for regularized empirical risk minimization with convex piecewise linear-quadratic losses and constraints, outperforming both generic and specialized solvers on large-scale problems."
"We propose DisDiff, an unsupervised method to automatically discover and disentangle inherent factors in pre-trained diffusion probabilistic models without factor annotations."
"RECESS is a novel federated learning defense that proactively detects malicious clients and robustly aggregates gradients using trust scores based on multi-iteration performance, significantly reducing accuracy loss from model poisoning attacks compared to existing defenses."
"We propose a semantic-aware partitioning method that learns subset-specific calibration functions, showing that partition granularity directly impacts model calibration and accuracy."
"PERFOGRAPH is a novel graph-based program representation that incorporates numerical and aggregate data structure information, outperforming existing methods and setting new state-of-the-art results in program analysis and optimization tasks."
"Physical symmetry constraints in self-supervised learning yield interpretable, linear representations such as pitch from unlabelled audio and 3D space from video, and enable counterfactual augmentation to improve sample efficiency."
"We propose a discriminative particle filter using continuous mixture densities and importance sampling to unbiasedly estimate gradients, enabling accurate, robust multimodal uncertainty representation in high-dimensional observation tasks."
QuantSR improves accuracy and efficiency in low-bit image super-resolution by introducing a redistribution-driven learnable quantizer and a depth-dynamic quantized architecture that enables flexible accuracy-efficiency trade-offs during inference.
"NeuTRENO mitigates the over-smoothing issue in deep transformers by penalizing the difference between input and smooth attention outputs, improving representation fidelity and performance on vision and language tasks."
"SoTTA is a test-time adaptation method robust to noisy test samples, using high-confidence uniform-class sampling and entropy-sharpness minimization to outperform existing TTA algorithms under noise."
We propose a federated class incremental learning framework that uses a server-trained data-free generative model to mitigate catastrophic forgetting without requiring clients to store old data or models.
"We propose a volume feature rendering method for NeRF that reduces color network evaluations to one per pixel, improving rendering quality and speed while reducing training time."
"Deep reproducing kernel Hilbert $C^*$-modules with Perron-Frobenius operators yield a new Rademacher generalization bound and a benign overfitting interpretation, offering milder output dimension dependency and a natural link to convolutional neural networks via $C^*$-algebra."
ReTR introduces a transformer-based framework with a learnable meta-ray token and high-dimensional feature space rendering to improve surface reconstruction accuracy and generalization over existing methods.
"Graph Segment Training with Historical Embeddings and Stale Embedding Dropout enables memory-efficient, accurate large graph property prediction by training on graph segments and mitigating embedding staleness."
"We present a general equivariant neural network architecture and software library for reductive Lie groups, demonstrating its effectiveness on Lorentz and orthogonal group tasks."
"Transformers quickly learn global but slowly learn in-context statistical patterns, with weight matrices serving as associative memories shaped by gradients and data distribution."
"We derive high-probability PAC-Bayesian generalization bounds using Wasserstein distance instead of KL divergence that apply to unbounded losses and enable practical SRM optimization, yielding empirically superior learning algorithms."
"SAM improves generalization and prevents noise memorization in two-layer ReLU networks by mitigating early-stage overfitting, even in nonsmooth loss landscapes, with theoretical and empirical validation."
"We introduce DP-HyPO, the first framework enabling adaptive, differentially private hyperparameter optimization by providing a rigorous privacy analysis and empirical validation on real-world datasets."
We show that pixel-based pretrained agents can outperform humans on GUI instruction following using generic keyboard and mouse actions.
"We propose a flexible tensor decomposition framework using energy-based models and neural networks to learn joint probabilities without structural or distributional assumptions, enabling unified learning for various tensor types and demonstrating improved performance on synthetic and real-world datasets."
We introduce a role-playing framework using inception prompting to enable scalable autonomous cooperation among chat-based agents and release a library to study multi-agent instruction-following.
"DFRD is a federated learning method using a conditional generator and exponential moving average to robustly aggregate knowledge from heterogeneous local models without accessing client data, achieving superior performance on image classification tasks."
"AMPLIFY automates rationale generation for LLMs using post hoc explanations, improving prediction accuracy by 10–25% across reasoning tasks without human annotation."
"HUME is a model-agnostic, unsupervised framework that discovers human labelings by exploiting linear separability of classes across representation spaces, outperforming supervised baselines on STL-10 and matching or exceeding state-of-the-art on CIFAR-10 and ImageNet-1000 using only fixed pretrained representations."
"We analyze and mitigate negative transfer in diffusion models via multi-task learning and efficient interval clustering of denoising tasks, improving generation quality and training speed."
"We introduce Neural Frailty Machine, a neural framework for survival regression that extends the proportional hazards model via multiplicative frailty and demonstrates strong approximation and predictive performance with theoretical convergence guarantees and empirical validation."
"We propose Glime, a stable and high-fidelity extension of LIME that addresses LIME’s instability and low local fidelity via an equivalent, faster-converging formulation and a local, unbiased sampling distribution."
"We present a framework that provides formal privacy guarantees for cloud-based machine learning inference by linking neural network local Lipschitz constant with local sensitivity and extending the PTR framework, and show that adversarial representation learning improves the privacy-utility trade-off."
HotBEV is a hardware-oriented BEV detection framework that achieves significant accuracy improvements and up to 6.3× speedups on multiple GPUs by optimizing model design for real on-device latency.
"We analyze last-iterate convergence of EG, OGDA, and momentum in time-varying, unconstrained bilinear zero-sum games, finding EG converges in periodic settings while OGDA and momentum diverge, but all converge if the game stabilizes faster than 1/t in convergent perturbed games."
"We show that interpretable clone-structured causal graph models acquire in-context learning via template learning, context-sensitive retrieval, and token rebinding, suggesting similar mechanisms in LLMs and highlighting the role of overparameterization."
"We provide instance-dependent CDF bounds for running averaged conditional distributions that enable univariate distribution estimation from dependent data, including an importance-weighted extension for counterfactual reward estimation in randomized experiments."
"We investigate how compositionality in visual token representations affects in-context analogical reasoning and find that object-centric tokenizers with cross-attention best enable compositional generalization, while non-compositional representations excel at extrapolation to new domains."
FACTS is a model-agnostic framework for evaluating robust subgroup fairness using counterfactual explanations and refined notions of recourse difficulty.
"ExpGen improves zero-shot generalization in reinforcement learning by promoting exploration, achieving state-of-the-art results on challenging ProcGen tasks."
"We establish that attention layers outperform recurrent and feedforward networks on sparse averaging (scaling logarithmically with input size and requiring large embedding dimensions), but are outperformed on a triple detection task (scaling linearly), highlighting the utility of communication complexity and sparse averaging in analyzing transformer representation power."
"We propose ESTAG, an equivariant spatio-temporal GNN that uses past trajectory data to capture non-Markovian dynamics and improves simulation on molecular, protein, and macroscopic datasets."
"We propose an idempotent image compression codec using right-invertible blocked convolution and null-space enhancement, achieving state-of-the-art rate-distortion and superior near-idempotence after repeated recompression."
"The Evolving Connectivity framework enables gradient-free, hardware-friendly training of recurrent spiking neural networks that matches or exceeds standard methods on robotic tasks."
"MISA is a novel offline RL framework that constrains policy improvement to the data manifold using mutual information lower bounds, outperforming baselines on D4RL, including achieving 742.9 total points on gym-locomotion tasks."
"DJINN is a diffusion-based method that generates realistic, diverse traffic scenarios for autonomous vehicle simulation by jointly diffusing agent trajectories conditioned on flexible state observations and enables direct test-time sampling for scenario customization."
"We propose Compressed Video Prompt Tuning (CVPT), a novel prompt-based framework that adapts raw video models to compressed video understanding by re-parameterizing compressed modalities as conditional prompts and enhancing cross-modal interactions, achieving superior accuracy-efficiency trade-offs on benchmark datasets."
Training deep neural networks on images simulated with peripheral vision blur and reduced saturation increases their robustness to adversarial and non-adversarial perturbations.
We propose an analytic framework that interprets and quantifies the semantic features underlying deep saliency models and applies it to analyze their attention predictions across diverse scenarios.
"We propose bandit-feedback bidding algorithms for Double Auctions with unknown valuations on both sides, achieving O(log T/Δ) social regret and O(√T) individual regret, and show matching lower bounds."
"We propose DOSTransformer, a multi-modal transformer integrating crystal structure and energy information with prompts to accurately predict density of states for both phonon and electron systems."
PEQA combines parameter-efficient fine-tuning with quantization to efficiently fine-tune and deploy large language models at sub-4-bit precision while preserving or improving performance.
"We present the first parallel combinatorial algorithm for an additive ε-approximation of the optimal transport distance with O(log n / ε²) parallel complexity and O(log log(n/ε)/ε²) rounds in MPC frameworks, outperforming numerical solvers on GPUs for large n."
This paper introduces an end-to-end learning framework that integrates neural networks and logical reasoning via relaxed discrete constraints and trust region methods to enable effective neuro-symbolic system training.
PropCare is a new framework that estimates propensity and exposure from only interaction data to enable causality-based recommendation when exposure or propensity labels are unavailable.
"Minimizing the trace of the Hessian in deep linear networks with linear measurements approximately minimizes the Schatten 1-norm of the end-to-end matrix, improving generalization under standard RIP conditions."
"We prove RochetNet and its affine generalization for auction design are mode connected, linking local optima with near-optimal solutions and providing the first such analysis in differentiable economics."
"Topological Precision and Recall (TopP&R) is a robust evaluation metric for generative models that reliably estimates supports by retaining significant features with confidence, providing consistent and accurate sample quality assessment even under noisy and non-IID conditions, unlike existing metrics."
"We propose a positive mining method for targeted adversarial attacks that improves robustness in adversarial self-supervised learning, especially for non-contrastive frameworks."
"We propose EMMS, a fast and effective method for predicting the transferability of pre-trained neural networks to multiple multi-modal tasks without fine-tuning, outperforming state-of-the-art methods with significant speedups."
"We propose a nonparametric Nadaraya-Watson head that learns invariant representations by restricting the support set to a single environment, and validate its effectiveness for domain generalization in computer vision."
"We introduce L-stable probabilistic exponential integrators and their generalization to nonlinear systems, demonstrating improved stability and efficiency for stiff differential equations compared to existing probabilistic solvers."
We propose a bilevel-regularized formulation and an efficient algorithm for coreset selection in continual learning that achieves superior performance and faster convergence than greedy baselines.
We present the first polynomial-time online algorithm for Online $k$-Clustering with Moving Costs achieving $\mathcal{O}(\log n)$ total regret relative to the best fixed solution's time-averaged connection cost.
"We present a dimension-independent sample complexity algorithm for finding approximate second-order stationary points in the presence of outliers, with a lower bound showing that quadratic dependence on dimension is necessary."
We propose a backdoor-resistant fine-tuning method using a honeypot module to absorb backdoor features and significantly reduce attack success rates by 10–40% compared to state-of-the-art methods.
"We present a Bayesian, easy-to-implement algorithm using zero-shot predictors that efficiently selects training data in noisy, imbalanced settings, achieving strong performance with fewer iterations than prior methods on WebVision."
"Prompt tuning with zero-shot pseudolabels improves CLIP performance across semi-supervised, transductive zero-shot, and unsupervised image classification, with greater equity in per-class accuracy than conventional pseudolabeling."
"We propose using MLPs to parameterize G-steerable kernels in convolutional networks, generalizing steerable equivariance to any group G."
We propose an adaptive design that achieves near-optimal variance efficiency in two-treatment settings and provide a conservative variance estimator for valid inference.
"CAM addresses clustering collapse in heterogeneous federated learning by adding a globally shared model on top of cluster-specific models, improving performance across non-IID settings without requiring fixed clustering."
"Larger transformer-based language models more accurately predict fMRI brain responses to language, with performance scaling logarithmically with model and data size and approaching a noise ceiling in key brain regions."
"We propose a linear-time, scalable method for differentiating optimal trajectories in non-convex constrained discrete-time optimal control using the implicit function theorem and variable elimination, improving efficiency and stability over prior approaches."
"Data quantity, not the label space, label semantics, image diversity, data domains, or fixed total data amount when varying class composition, is the primary factor affecting the robustness of fine-tuned models after pre-training, as shown using iWildCam-WILDS shift."
DoWG is a parameter-free gradient-based optimizer that achieves optimal convergence rates in convex optimization and adapts to both smooth and nonsmooth problems by maintaining a distance-based weighted running average of gradients.
"We propose a bias-unsupervised debiasing method that uses pretrained models to extract bias information, enabling robust model training and validation without group labels, and demonstrates improved generalization over state-of-the-art label-dependent methods."
"CoLLAT is a framework that learns fine-grained audio understanding by locking a pretrained language model and using a novel audio-to-text grounding objective, achieving state-of-the-art performance in audio classification and related tasks while preserving language model capabilities."
"SA3D generalizes the Segment Anything Model to 3D segmentation by leveraging multi-view 2D prompts and a NeRF prior, enabling efficient 3D mask generation from a single-view segmentation without expensive 3D annotations."
"We introduce f-MIP, a practical privacy notion for membership inference that quantifies membership leakage under realistic attack settings and show that SGD-trained models inherently provide some privacy, which can be enhanced by gradient noise, with an analytically-derived attack enabling direct f-MIP auditing and revealing how hyperparameters and data characteristics affect membership inference."
"We propose a hybrid generative-sequential model combining an autoencoder and a transformer-normalizing flow to accurately predict both deterministic and stochastic dynamical systems on unstructured meshes, outperforming baselines and producing reliable uncertainty estimates."
"We propose a Non-isotropic Gaussian Diffusion Model that edits images by applying varying noise levels per pixel and using a pre-trained isotropic diffusion model with per-pixel diffusion times, achieving state-of-the-art performance in image editing."
"We analyze equivariant neural networks under partial domain symmetry and prove error bounds when symmetry is only partially correct or extrinsic, validating these results experimentally."
"We present the first 3D CNN-based video stylization method that explicitly disentangles and stylizes appearance while preserving motion, outperforming frame-wise 2D approaches in texture fidelity."
"We prove a $\widetilde{O}(H\sqrt{d_{l_1}T})$ Bayesian regret bound for Thompson Sampling in time-inhomogeneous reinforcement learning and compute $d_{l_1}$ for tabular, linear, and finite mixture settings."
CoDi is a generative model that can produce any combination of output modalities from any input modalities by learning a shared multimodal space and synchronizing their generation during diffusion.
"We introduce gisting, a method that trains LMs to compress prompts into reusable gist tokens, achieving up to 26x prompt compression, 40% FLOP reduction, and 4.2% speedup with minimal quality loss."
"We propose FedSTO, a semi-supervised federated object detection framework that enables effective training with only server-side labels and non-IID client data, achieving state-of-the-art results using only 20–30% of the available labels on prominent autonomous driving datasets."
"We propose a classification-based method for simulation-based calibration that overcomes SBC's limitations by providing interpretable divergence metrics and higher power, implemented using neural networks and validated on numerical and real data."
We derive information-theoretic lower bounds on the optimal robust loss for multi-class classifiers under adversarial attacks and quantify the gap to state-of-the-art methods on benchmark datasets.
DOSE is a model-agnostic method that improves speech enhancement using condition-augmentation and informative priors in denoising diffusion probabilistic models.
"We present a differentiable, parametric garment model using 2D panels with Signed Distance Functions and 2D-to-3D mapping that enables fast, high-quality reconstruction and editing of multi-layered clothing on arbitrary body poses."
"We present a diffusion-model-based method for solving inverse physics problems by recursively combining an approximate inverse simulator and a learned correction, achieving accurate, temporally stable solutions and posterior sampling."
"We propose a meta-learned transductive neural approach that efficiently approximates functions from few examples for both finite and infinite-dimensional problems, outperforming traditional methods in data efficiency and cost for modeling physical systems."
Sharpness-aware minimization reduces feature rank across various neural network architectures and tasks by pruning a significant number of activations.
"We propose PREGM, an end-to-end graph matching network that encodes affine-invariant node positions and reconstructs spatial relations to improve semantic keypoint correspondence by better utilizing spatial information."
"We introduce color-separating sets to characterize the expressive limits of persistent homology on attributed graphs and propose RePHINE, which integrates vertex- and edge-level topological features to enhance message-passing GNNs’ expressivity."
"Noether Embedding is an efficient event embedding method that encodes temporal regularities with time-translation symmetry, enabling data- and time-efficient detection and query of temporal regularities across multiple real-world datasets."
"We generalize policy optimization in reinforcement learning by replacing the standard KL divergence with Tsallis KL divergence, yielding a new algorithm that outperforms the original on 35 Atari games."
"We propose a diverse, high-dimensional benchmark for mutual information estimators and provide guidelines for selecting appropriate estimators for different data settings."
CAPro is a prototypical contrastive learning framework that leverages aligned text and visual prototypes to robustly learn semantic visual representations from noisy web data.
"DiffuseBot is a physics-augmented diffusion model that co-optimizes soft robot morphology and control for diverse tasks, bridging virtual design and physical performance."
"We clarify the relationship between Rubin causal models and structural causal models, showing that every RCM can be viewed as an abstraction of an SCM-representable RCM, even when violating SCM algebraic principles, and illustrate the role of SCM principles in RCM applications."
We propose a metric based on subjective logic theory to align recurrent vision models with human reaction times across multiple visual decision-making tasks.
"Hierarchical VAEs, when trained on ecologically relevant motion stimuli, better model brain responses and recover ground truth causes of motion compared to non-hierarchical models, supporting the role of hierarchical inference in aligning artificial and biological motion processing."
We discover that neural network architecture itself causes bias in face recognition and use neural architecture search to design models that outperform existing methods in both accuracy and fairness.
"LogEI reformulates classic Bayesian acquisition functions to alleviate numerical optimization pathologies, yielding improved and state-of-the-art optimization performance."
"$t$-AdaBoost generalizes AdaBoost by using tempered exponential measures with parameter $t$, achieves improved or comparable convergence rates for $t\in[0,1)$, ensures bounded leveraging coefficients, and enables new tempered losses for decision trees with tunable performance."
Incorporating biologically inspired architectural components into CNNs significantly improves their ability to explain primary visual cortex (V1) activity and tuning properties.
We theoretically and empirically identify obstructions to training geometric latent space encoders and propose a flow-based model with multimodal distributions to improve stability and convergence to homeomorphic encoders.
"Decorate3D is a method that models real-world objects as NeRFs, decomposes them into editable mesh and texture components, and enables 3D-consistent texture editing via prompt-guided, structure-aware generation and high-resolution UV texturing."
"EvalPlus enhances code synthesis evaluation by automatically generating extensive test cases for benchmarks like HumanEval, revealing significant inaccuracies in previous LLM performance assessments."
"We propose Energy Discrepancy, a new loss for energy-based models that efficiently interpolates between score matching and likelihood, enabling faster and more accurate training than existing methods, though with limitations in high dimensions."
"We propose Cola, a method that coordinates multiple vision-language models via a large language model to achieve state-of-the-art visual reasoning performance."
"We propose UIPS, an uncertainty-aware inverse propensity score estimator with theoretical convergence guarantees that improves off-policy learning by explicitly modeling uncertainty in the estimated logging policy."
"AltUp increases transformer model capacity with minimal compute and latency by widening token embeddings via subblock updates and predict-and-correct mechanisms, achieving up to 87% speedup on SuperGLUE and SQuAD at equivalent accuracy."
"We generalize online convex optimization to settings with long-term memory dependence, introducing the p-effective memory capacity Hp and establishing tight O(√HpT) regret bounds, with applications to online linear control and performative prediction."
"We propose a diffusion-based method (EGG) that generates more transferable and computationally efficient most exciting inputs for macaque V4 neurons than gradient ascent, enabling broader analysis of visual system coding."
"We propose Adjustable Robust Reinforcement Learning (AR2L) to balance average and worst-case performance in online 3D bin packing by optimizing a weighted sum of expected and worst-case returns, improving robustness without sacrificing nominal performance."
"We rigorously analyze and compare ODE-based and SDE-based diffusion models under zero and large diffusion limits, showing that ODE models outperform SDEs when perturbations occur late, while SDEs are more robust to early perturbations with error exponentially decreasing as diffusion increases, validated on synthetic and real datasets."
"We propose Laplacian Canonization with Maximal Axis Projection, a lightweight pre-processing method that restores sign and basis invariance in spectral graph embeddings for GNNs, improving performance on molecular datasets with minimal overhead."
We propose an exponentially convergent algorithm that reformulates supervised matrix factorization as a low-rank matrix estimation problem and demonstrate its ability to identify cancer-associated gene groups.
ITO Learning uses SE(3)-equivariant denoising diffusion models to accelerate molecular dynamics simulations across multiple time and space resolutions.
"We propose an iterative pruning and network analysis method to uncover hierarchical modularity in neural network solutions to tasks, and demonstrate its effectiveness on Boolean functions and MNIST-based vision tasks."
"We introduce a correlation loss that decorrelates latent features, improving performance of hyperprior-based learned image compression without increasing computational complexity, achieving up to 98% of AR-based BD-Rate gains at 30× faster speeds."
"We propose Demand-driven Navigation (DDN), which uses user demands rather than specified object names to enable agents to find suitable objects in scenes, leveraging LLM-derived textual attributes aligned with visual features to improve navigation performance over standard VON methods."
"We propose EF21-SGDM, which applies Polyak's momentum to EF21 to achieve convergence with smaller batch sizes in nonconvex distributed optimization under standard assumptions, improving communication and sample complexity and introducing a double momentum variant with even better guarantees."
"We present a provably optimal reliable learner with computationally feasible implementations that achieves strong guarantees against adversarial attacks and distribution shifts, including for linear separators under log-concave and smooth classifiers under smooth distributions."
We present exact verification conditions and algorithms for ensuring safety of ReLU-based neural control barrier functions by decomposing them into piecewise linear segments and verifying safety on each segment's boundary using a generalized Nagumo theorem.
"We propose a method for robust multimodal learning that generalizes to unseen modality combinations at inference by projecting modalities into a shared space and using pseudo-supervision to reduce overfitting, and demonstrate its effectiveness across video classification, robot state regression, and multimedia retrieval tasks."
"We propose an optimistic PPO variant for episodic adversarial linear MDPs with full information and achieve $\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret, improving upon previous policy-based algorithms and introducing a novel multi-batched update with new covering number arguments."
"We propose Inc-FedUCB, an incentivized communication protocol for federated linear bandits that achieves near-optimal regret with provable communication and incentive guarantees under self-interested clients."
"We present a unified minimax lower bound framework for distributed parameter estimation under local information constraints, which yields tight bounds for various distribution families and loss functions, and is complemented by matching upper bounds."
"ConvS5, a convolutional state space model combining ConvLSTM's tensor modeling with efficient long-sequence processing, outperforms Transformers and ConvLSTM on long-range spatiotemporal tasks with faster training and sampling."
We prove and demonstrate that using low-discrepancy completely uniformly distributed sequences for Gaussian perturbations in Langevin Monte Carlo reduces estimation error under smoothness and convexity conditions.
"Swapping batch normalization after a bounded activation function like Tanh, which induces asymmetric saturation and increases sparsity near zero, improves performance compared to the conventional order."
"We propose algorithms for parallel transfer learning in the Tiered Reinforcement Learning setting that achieve near-optimal regret for the source task and partial-state regret for the target task under a new ""Optimal Value Dominance"" condition, even without prior knowledge of task similarity or shared dynamics."
We propose a neural network architecture that adaptively learns an isometry-invariant filtration for point clouds to improve persistent homology-based machine learning performance on classification tasks.
"CommonScenes is a generative model that creates controllable, semantically realistic 3D scenes from scene graphs by modeling object and scene relationships, outperforming existing methods on generation consistency, quality, and diversity."
"VQLoC is a single-stage, end-to-end trainable framework for visual query localization in long-form egocentric video that improves accuracy by 20% and inference speed by 10× over prior multi-stage methods."
"We present an efficient, bias-corrected method for estimating partial information decompositions in high-dimensional Gaussian neural data and demonstrate its application in characterizing inter-areal redundancy in the mouse visual system."
We propose a method that bounds out-of-distribution actions during training using dynamic programming to improve performance in offline reinforcement learning on D4RL benchmarks.
"We propose a framework that leverages a discrete-continuous mapping to address over-smoothing in GNNs via total variation regularization and to model graph spreading flows with a tailored GAN objective, achieving state-of-the-art results on standard benchmarks."
"We propose Blurred-Dilated (BD), a model modification method that improves transferability of adversarial examples in black-box settings by reducing downsampling and using BlurPool and dilated convolutions to better preserve and disrupt image features."
"Banana is a Banach fixed-point network that achieves equivariant segmentation with inter-part equivariance by simultaneously co-evolving part assignments and SE(3)-equivariant transformations, ensuring robust generalization to unseen inter-part configurations."
"We analyze repeated multi-unit uniform-price auctions, designing efficient bidding algorithms with low regret and showing variant-specific susceptibility to collusion."
We present novel DP algorithms for efficiently computing pairwise statistics under the local model using techniques from DP for linear queries.
"CQD$^{\mathcal{A}}$ is a parameter-efficient adaptation model that recalibrates neural link predictor scores to improve complex query answering on incomplete knowledge graphs, achieving higher accuracy and data efficiency than previous methods."
"We propose a spectral optimization method that identifies a stable, architecture-invariant subnetwork in overparameterized student models, mirroring the teacher's complexity without performance loss upon pruning beyond the teacher size."
"Trained large language models exhibit hierarchical, structure-dependent integration windows over time that untrained models lack, and these can be measured using a black-box word-swap method revealing a shift from exponential to power-law dynamics across layers."
"HeadSculpt introduces a 3D-aware, coarse-to-fine pipeline with landmark control and identity-aware editing to generate and edit high-fidelity 3D head avatars from text, addressing inconsistencies and fine-grained editing limitations of prior diffusion-based methods."
"We introduce UCB-CCA and UCB-CCA+ for the cascading contextual assortment bandit, achieving improved regret bounds without dependence on cascade length or problem constants."
CLASH is a causal machine learning method for effective early stopping of randomized experiments when treatment harms a minority subgroup.
PoET is an autoregressive Transformer-based generative model that learns to generate protein families and predict variant functions across diverse MSA depths by modeling sequences-of-sequences with order-invariant inter-sequence attention.
"MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents, improves de novo drug design by enhancing molecular diversity and achieving promising results on the GuacaMol benchmark and against SARS-CoV-2 targets."
NDPCA is a distributed compression framework that dynamically adapts to variable bandwidth to improve multi-sensor task performance by learning task-relevant low-rank representations and allocating bandwidth efficiently.
We show that goal-conditioned RL agents can satisfy arbitrary ω-regular LTL specifications zero-shot without additional LTL-specific training.
"CaST is a causal graph neural network framework for spatio-temporal forecasting that addresses temporal out-of-distribution and dynamic spatial causation via back-door and front-door adjustments, outperforming existing methods on real-world datasets."
"We rigorously analyze Gaussian-SVGD, showing linear convergence under strong log-concavity and recovering several GVI algorithms as special cases, while introducing an improved particle-based variant."
We propose a probability tree state abstraction (PTSA) algorithm that reduces the search space of MCTS-based algorithms by 10–45% while maintaining theoretical guarantees and improving training efficiency.
"We establish generalization error bounds and refine convergence rates for federated zeroth-order optimization using on-average model stability, extending results to asynchronous settings and addressing theoretical gaps."
"We characterize the convergence of optimistic gradient descent in time-varying multiagent games, providing sharp, variation-dependent bounds and new insights for both zero-sum and general-sum settings."
"We propose a graph representation method based on the Lovász number, using neural networks to encode handle vectors and subgraph information, achieving competitive performance on graph classification tasks."
We characterize the set of surrogate loss functions consistent for robust binary classification under adversarial training and show it is smaller than in the standard setting.
"We introduce Two-Stage Predict+Optimize, a general framework and training algorithm for predicting unknowns in both the objective and constraints of mixed integer linear programs, outperforming prior methods."
Collaborative Score Distillation (CSD) improves consistency across multiple images in text-to-image diffusion models by synchronously distilling generative priors using Stein Variational Gradient Descent.
"TPSR integrates Monte Carlo Tree Search with transformer decoding for symbolic regression to incorporate accuracy and complexity feedback, outperforming state-of-the-art methods on multiple datasets."
"We propose policy gradient methods for solving continuous zero-sum Markov Stackelberg games using trajectory-based noisy gradients, proving polynomial-time convergence in the convex-concave case and demonstrating superior performance of Stackelberg over Nash equilibria in reach-avoid problems."
"Sp$^{2}$GCL introduces a stable, scalable spectral encoder invariant to eigenvector rotations and a contrastive framework fusing spatial and spectral graph information for faster, more effective representation learning."
"We introduce a model sparsification-based paradigm for machine unlearning that enhances unlearning efficacy and efficiency, with demonstrated gains up to 77% in fine-tuning and applications to backdoor defense and transfer learning."
"The paper introduces Described Object Detection (DOD), a unified task extending OVD and REC using a new Description Detection Dataset (D³), and proposes a baseline that improves REC by revising training and adding a binary classification sub-task."
"We propose a GNN method linking class imbalance to model variance via bias-variance decomposition and graph augmentation, outperforming state-of-the-art on imbalanced node classification benchmarks."
We propose a gauge equivariant message passing architecture for solving PDEs on discretized manifolds that outperforms standard convolutional and attentional methods on tasks with complex nonlinear dynamics.
SVGD achieves a $1/\sqrt{\log\log n}$ convergence rate in kernel Stein discrepancy for sub-Gaussian targets with Lipschitz score using $n$ particles.
"SAMoSSA is a two-stage algorithm that first uses multivariate Singular Spectrum Analysis to estimate non-stationary components and then fits an AR model to the residuals, providing finite-sample consistent forecasting with theoretically justified performance improvements over baselines."
"The Lévy-Itō Model (LIM) replaces Brownian motion with an isotropic α-stable Lévy process in score-based generative modeling, achieving faster, more diverse sampling with higher fidelity and improved metrics on both balanced and imbalanced image datasets compared to standard diffusion models."
"We propose algorithms achieving $\widetilde{\mathcal{O}}(\sqrt{T} + C^P)$ regret for adversarial MDPs with adversarial losses and transitions, where $C^P$ quantifies transition adversarialism, and extend this to handle loss corruption and adapt to stochastic environments."
"We present a frequency estimation algorithm that outperforms previous learning-augmented methods, both with and without using heavy-hitter predictions, and achieves superior empirical performance."
We propose a data averaging-based sketching method that achieves a faster convergence rate than optimal sampling methods for high-dimensional linear regression with large sample sizes under computational constraints.
"We propose Efficient Diffusion Policy (EDP), which accelerates diffusion-based offline RL policy training by orders of magnitude and achieves state-of-the-art performance across multiple algorithms on the D4RL benchmark."
"Quantization generally outperforms pruning for neural network compression, except at very high compression ratios where pruning may be preferable."
FedNPG-ADMM reduces communication complexity of federated natural policy gradient from O(d²) to O(d) while maintaining convergence and reward performance.
SaVeNet is an efficient framework for geometric molecular representation learning that achieves state-of-the-art performance while addressing computational inefficiency and limited generalizability.
Gaussian Mixture Solvers (GMS) improve sample quality in diffusion-based image and stroke synthesis by better approximating the reverse transition kernel using mixture models rather than a Gaussian assumption.
We theoretically explain UNet instability in diffusion models due to long skip connect coefficients and propose a coefficient scaling method (ScaleLong) that stabilizes training and accelerates convergence.
"We propose Graph Prompt Feature (GPF), a universal prompt-based tuning method for pre-trained GNNs that outperforms fine-tuning across diverse pre-training strategies."
"We present the first dynamic algorithms for non-monotone submodular maximization under a cardinality constraint by reducing it to the monotone case, achieving $(8+\epsilon)$-approximation with efficient expected amortized oracle query complexity and demonstrating practical benefits on video summarization and max-cut datasets."
"We prove that meta-learning accelerates convergence to game-theoretic equilibria in multi-agent reinforcement learning across related tasks, and develop initialization-dependent MARL algorithms with improved convergence guarantees."
"LIBERTY improves deep reinforcement learning exploration by using an end-to-end, potential-based bonus derived from bisimulation metrics to encourage discovery of novel states without manual reward shaping, and is shown to scale better and perform superiorly on MuJoCo and Arcade Learning Environments."
"AR-Diffusion improves diffusion-based text generation by enforcing left-to-right token dependency through variable denoising steps, outperforming prior diffusion models and achieving significantly faster generation."
"We propose a high-dimensional hypothesis testing framework for uncertainty estimation in deep neural networks that operates directly on latent representations, relaxes distributional assumptions, and enables out-of-distribution detection without requiring training on OOD data."
"The paper proposes an adversarial dual-label confidence and semantic dissimilarity learning method with kernel extension for partial label learning, outperforming state-of-the-art approaches on multiple datasets and providing theoretical guarantees."
"Gaussian process probes (GPP) measure concept representation and uncertainty in models using a Bayesian linear probing framework that requires only vector representations and enables data-efficient, uncertainty-aware evaluation without access to training data or gradients."
"We characterize the trade-off between expected regret and tail risk in stochastic multi-armed bandits, proposing a policy that achieves optimal regret tail decay for any regret threshold in both stationary and non-stationary settings."
"Task arithmetic improves model editing by leveraging weight disentanglement, which is amplified by tangent space fine-tuning and linked to the spatial localization of neural tangent kernel eigenfunctions."
"We propose a primal-dual online learning algorithm for dynamic lever optimization in ad procurement under bandit feedback and long-term uncertain constraints, achieving low regret across stochastic, adversarial, and non-stationary outcome models without prior knowledge of the outcome process."
"This paper introduces and evaluates textual outlier exposure for out-of-distribution detection, showing that generated textual outliers, especially those that are near-distribution, descriptive, and include visual semantics, can achieve strong performance comparable to image-based methods."
"We conduct a large-scale human evaluation of generative models and show that existing metrics, including FID, poorly correlate with perceived realism, while DINOv2-ViT-L/14 enables richer evaluation and current metrics fail to reliably detect data memorization."
"DiffKD improves knowledge distillation by denoising student features using a diffusion model trained on teacher features, achieving state-of-the-art performance across multiple vision tasks."
"PHGD is a first-order method that exploits hidden convex structures in non-cooperative games via gradient preconditioning, providing convergence guarantees without separability assumptions in both deterministic and stochastic settings."
"We analyze robust adversarial training of two-layer neural networks as a bi-level optimization problem, providing convergence guarantees and precise iteration complexity bounds for PGD-based attacks under linear separability, and support these with empirical evidence."
"We propose a method to optimize the latent space in generative models by minimizing a data-dependent distance that reduces generator complexity, improving sample quality and efficiency across models like VQGAN and Diffusion Transformer."
We propose sampling-based regression adjustment methods with finite-sample error bounds for estimating treatment effects while minimizing experimental exposure.
