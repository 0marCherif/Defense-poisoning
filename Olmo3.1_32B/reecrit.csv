abstract
"OCRA integrates object and relational abstraction using advanced neural architectures to achieve systematic generalization in complex visual tasks, including a new dataset called CLEVR-ART."
"We present a graph-based method that encodes molecules and decodes predicted mass spectra as multisets of subformulae using a prefix tree, enabling efficient and accurate mass spectrum prediction for large-scale metabolomics."
"MoSo is a data-pruning method that removes the least informative samples by approximating the impact of their exclusion using gradient alignment, outperforming state-of-the-art methods on dataset condensation benchmarks."
"Transformers inherently optimize a sparse rate reduction objective through incremental compression and sparsification, and simplified transformer-like networks achieve ImageNet classification performance comparable to ViT while offering mathematical interpretability."
"We propose a dynamic programming framework that leverages subtree separability to efficiently optimize separable objectives and constraints in decision trees, outperforming general-purpose solvers in scalability across multiple application domains."
"UNSSOR is an unsupervised neural speech separation method that uses over-determined microphone mixtures and deep learning to extract individual speaker signals in reverberant environments, with improved speaker differentiation via transcribed speech for applications like emotion detection."
"We propose methods to approximate and interactively explore the Rashomon set of sparse generalized additive models using ellipsoidal approximations and eigenvector analysis, enabling practical model selection and property enforcement in real-world applications."
We propose two efficient algorithms for generalized linear bandits with heavy-tailed rewards that achieve nearly optimal regret of $\widetilde{O}(dT^{1/(1+\epsilon)})$ and outperform existing methods by a logarithmic factor when $\epsilon=1$.
We propose a factor-graph-based framework for identifying expected outcomes of novel interventions using minimal assumptions and testable interventional factor models.
"We analyze the last-iterate convergence of EG and OGDA in time-varying bilinear zero-sum games, finding that EG converges in periodic settings where OGDA and momentum diverge, and that convergence for all algorithms in perturbed games requires game stabilization faster than $1/t$."
"We show that achieving O(√(K Ł T)) dynamic regret in K-armed dueling bandits with significant preference shifts is generally impossible under common preference distributions except within the SST∩STI class, where it is plausible."
We introduce algorithmic improvements to diffusion language models that surpass GPT-2 124M in likelihood and generation quality.
We derive sharp asymptotic results showing the Naive Mean Field approximation is suboptimal for estimating the log-partition function and quantifying uncertainty in high-dimensional linear regression without restrictive assumptions.
"We analyze the impact of Matérn kernel mixtures on smoothness and identifiability in single- and multi-output Gaussian Process models, finding that additive mixtures are limited by the least smooth component and multiplicative mixtures enable identifiable covariance in multi-output settings."
"We introduce a system that integrates sparse feature learning with differentiable decision tree construction to produce small, high-performing, and interpretable decision trees, validated on synthetic and real data."
We propose a meta-learning framework for privacy-preserving action recognition that leverages unlabeled data and controllable augmentation to improve generalization against novel privacy threats.
"This manuscript introduces a novel causal framework for non-parametric decomposition of spurious variations in Markovian and semi-Markovian models, providing identification guarantees and demonstrating applicability in explainable AI, fairness, epidemiology, and medicine."
"We investigate how to optimally request and use a limited number of predictions in online algorithms for ski rental, secretary, and non-clairvoyant job scheduling problems, applying bandit learning theory to determine optimal querying under budget constraints."
"This work provides the first comprehensive theoretical analysis of federated zeroth-order optimization (FedZO), establishing generalization error bounds and on-average model stability under adversarial corruption and heavy-tailed noise."
"We propose an interactive, task-driven evaluation protocol to close the dynamics gap in trajectory prediction for autonomous driving by accounting for predictor-specific interactive effects and the trade-off between computational efficiency and accuracy."
"NeuralEF is a neural network framework that efficiently approximates solutions to efficient frontier portfolio optimization problems under diverse constraints, significantly accelerating computation while preserving accuracy."
"We propose a neural network-based method using conditional normalizing flows to reliably estimate optimal personalized dosage combinations in cancer treatment, addressing sparse covariate-treatment regions and non-stationarity in patient responses."
"We propose sign equivariant neural network architectures for spectral geometric learning, demonstrating improved performance in graph classification tasks compared to sign invariant models."
"GuidedDiffTime is a constrained diffusion model that efficiently generates realistic time series under specified constraints without retraining, outperforming traditional methods and reducing computational cost by up to 92%."
"We propose Spiking Early-Exit Neural Networks (SEENNs), a novel SNN architecture that dynamically adjusts computation time per input to achieve high image classification accuracy with minimal timesteps."
"We rigorously unify Bayesian, variational Bayesian, and ensemble methods in deep learning by reframing non-convex optimization as convex optimization over probability measures, demonstrating convergence of interacting deep ensembles to a global minimizer and highlighting the benefits of ensemble-based randomness in classification."
"Fine-Grained Reinforcement Learning from Human Feedback (FG RLHF) improves language model outputs by using detailed sentence- or sub-sentence-level human feedback and multiple reward models, enhancing factuality, relevance, and robustness across diverse architectures."
We introduce operation-level early stopping (OLES) to enhance the robustness and calibration of DARTS by preventing overfitting of parametric operations.
"We propose an unsupervised video object-centric learning method using pre-trained self-supervised features and a temporal feature similarity loss, achieving state-of-the-art results on synthetic and real-world video datasets."
This paper introduces two new notions of model equivalence for distributional reinforcement learning to enable selective risk-sensitive planning and demonstrates their potential to enhance model-free risk-sensitive algorithms.
"We propose an anatomically interpretable brain age prediction framework using coVariance Neural Networks and cortical thickness features, enabling identification of brain regions linked to Alzheimer’s disease and integrating regression and classification for explainable medical image analysis."
"We propose a masked two-channel decoupling framework that addresses incomplete multi-view weak multi-label learning by decoupling shared and view-specific representations and employing cross-channel contrastive and label-guided graph losses, demonstrating superior performance under missing views and labels."
"We formulate combinatorial optimization problems as bisimulation-quotiented MDPs to exploit problem symmetries, enabling provably optimal and out-of-distribution generalizing policies learned via imitation."
"We propose a diffusion-based generative sequence model for long-term, vision-based, and multi-task decision-making that learns control-relevant latent representations and achieves state-of-the-art results on robotics benchmarks."
"MFLD provides a rigorous framework with convergence guarantees for finite-particle, time-discretized optimization under stochastic gradients, improving upon standard Langevin dynamics in SGD and SVRG settings."
"This study finds that while LLMs like GPT-4 show limited autonomous planning ability (~12% success), their generated heuristics can improve AI planner performance when combined with external verification."
We introduce a nearly guaranteed upper bound on deep neural network test error under distribution shifts using unlabeled data and improved sampling-based divergence metrics.
NAR-Former V2 integrates GNN inductive learning into a Transformer framework to surpass GNN-based methods in encoding neural network structures and predicting attributes such as latency on the NNLQP dataset and achieve competitive performance on NASBench datasets.
"CoPriv jointly optimizes 2PC inference protocols and DNN architectures to significantly reduce communication overhead in private inference, achieving up to 9.98× lower online communication than state-of-the-art methods while maintaining higher accuracy."
"We propose a stepwise self-evaluation mechanism integrated with stochastic beam search to improve multi-step reasoning accuracy in LLMs, achieving significant gains on GSM8K, AQuA, and StrategyQA benchmarks."
"Distributional reinforcement learning (DistRL) achieves faster convergence and improved resilience in low-optimal-cost and offline settings, with theoretical guarantees and empirical validation in bandit and MDP domains."
"UniHOI leverages vision-language foundation models and prompt-guided decoding with LLMs to detect and classify human-object interactions in open and unseen settings, outperforming prior methods under both supervised and zero-shot conditions."
"We introduce a method for imitation learning from vague feedback by modeling demonstrations as a mixture of expert and non-expert data and recovering the expert policy, enabling improved performance with broad, non-expert feedback."
"TSDiff is an unconditional diffusion model with self-guidance that enables flexible time series forecasting, refinement, and synthetic data generation without auxiliary networks."
"We present a joint caching and model selection strategy that achieves up to 50× efficiency gains and up to 4.3× and 1.8× improvements in FLOPs and latency, respectively, for large language model inference in resource-constrained settings."
"We propose Feedback-Feedforward Alignment (FFA), a learning algorithm inspired by hyperbolic visual representation and brain-like feedback dynamics, which jointly optimizes feedforward and feedback pathways with convolutions to improve robust visual inference, as demonstrated on MNIST and CIFAR10."
"We propose a method to detect hidden confounders in observational data across multiple environments by testing conditional independencies derived from independent causal mechanisms, and validate its effectiveness via simulations and real-world benchmarks."
"POMP is a memory- and computation-efficient prompt pre-training method for vision-language models that achieves state-of-the-art zero-shot performance across over 21 datasets, including 67.0% average accuracy on 10 image classification tasks and 84.4 hIoU on open-vocabulary segmentation."
"We propose the rewarded soup technique, which fine-tunes multiple foundation models on diverse proxy rewards and interpolates their weights to achieve Pareto-optimal generalization across a range of tasks and preferences."
"We propose a theoretical framework for unsupervised machine translation between dissimilar linguistic systems, with rigorously derived sample complexity bounds that reveal translation feasibility depends on the complexity and overlap between the communication systems, suggesting the potential to apply neural translation models to animal communication."
"We introduce CogEval, a cognitive science-inspired protocol to evaluate LLMs' abilities to form cognitive maps and perform complex planning, revealing significant limitations in handling latent relational structures despite apparent competence in simpler tasks."
"We propose unbiased, low-variance estimators for sub-sampling activations in transformer fine-tuning, reducing peak memory by up to 2.7× and enabling up to 6.4× larger batch sizes with minimal accuracy loss."
ELDEN is a reinforcement learning exploration method that leverages uncertainty in local dependencies of factored state spaces to efficiently discover new interactions and outperform existing methods across diverse environments.
"We analyze mode connectivity in neural networks for differentiable auction design, showing that locally optimal solutions are linked by simple paths and can be efficiently optimized, especially with pretrained networks."
TimeX is a novel interpretable surrogate method for time-series models that preserves the latent space relations of pretrained models and demonstrates superior or competitive performance and practical effectiveness on diverse datasets.
PLASTIC improves single-agent RL sample efficiency by maintaining both input and label plasticity through architectural modifications validated on Atari-100k and Deepmind Control Suite benchmarks.
TempBalance is a layer-wise learning rate optimization method based on Heavy-Tailed Self-Regularization Theory that improves neural network training and classification performance across multiple datasets and architectures.
We introduce a Pointnet++-inspired neural network with a novel sampling scheme and class-balanced loss to accurately distinguish leaf from wood points in sparse UAV lidar point clouds.
"The paper proposes Individualized DP-SGD, a variant of DP-SGD that assigns distinct privacy budgets to users, improving privacy-utility trade-offs compared to uniform approaches."
"A non-linear auto-regressive, Bayesian framework reduces off-target optogenetic stimulation by optimizing laser power and targeting, improving stimulation precision in neural circuits."
"Jigsaw is a weakly-supervised learning framework that reconstructs physically broken 3D objects by integrating geometric feature extraction, surface segmentation, and multi-part matching, outperforming existing methods on the Breaking Bad dataset."
"LEACE is a closed-form concept erasure method that removes specific features from representations with minimal distortion, improving fairness and interpretability in language models."
"ParaDiGMS accelerates diffusion model sampling by parallelizing denoising steps via Picard iterations, achieving up to 4× speedup without quality loss."
"SynGen improves text-to-image generation by aligning cross-attention maps with syntactic entities in prompts, outperforming state-of-the-art methods on multiple datasets."
"We analyze how two-layer ReLU networks trained with gradient descent and hinge loss on noisy, linearly separable binary data can exhibit benign overfitting, dual-phase learning dynamics, and robust discrimination of clean versus corrupt data points."
"L2Dive enhances diving heuristics for mixed integer linear programming by using graph neural networks to learn instance-specific strategies, improving solution quality and speed compared to traditional methods."
"We present an efficient algorithm achieving nearly tight regret bounds for contextual bandit problems with adversarial losses and unknown contexts, introducing a technique to reduce estimation-action correlation across epochs."
"We generalize ResNets to Riemannian manifolds using geometric deep learning insights, improving learning on non-Euclidean data and outperforming prior manifold-specific models."
"We introduce Deep Language Networks (DLNs) that optimize natural language prompts as learnable parameters, showing that stacking LLMs improves performance beyond single-layer models and approaches that of larger models like GPT-4."
"We propose NICE, a GAN regularization method that uses adaptive multiplicative noise and discriminator consistency to improve data efficiency and stability in image generation, especially for imbalanced datasets."
We show that simple randomized voting rules maintain high explainability while improving efficiency and fairness in decision-making compared to traditional voting systems.
"We propose a VAE with an asymmetric Laplace mixture decoder that relaxes Gaussianity assumptions, enabling broader distribution fitting and better causal quantile estimation for privacy-preserving synthetic data generation."
"Dynamic Prompt Learning (DPL) improves fine-grained, text-driven image editing by mitigating cross-attention leakage in diffusion models through dynamic tokens and specialized losses, resulting in more precise region-specific modifications."
"We introduce Nash Regret and achieve a regret bound of \( O\left( \sqrt{\frac{d}{T}} \log(T |X|) \right) \) for multi-agent linear bandits with shared information, addressing performance degradation of UCB by measuring collective welfare via the Nash social welfare function."
"IPMix is a label-preserving, multi-scale augmentation method that enhances convolutional neural network robustness to corruptions and adversarial attacks while maintaining clean data performance."
"We propose GALET, an alternating method that achieves $\epsilon$-stationarity in bilevel optimization with a convex Polyak-Łojasiewicz lower level in $\tilde{\cal O}(\epsilon^{-1})$ iterations, matching the complexity of gradient descent for single-level nonconvex optimization."
"We introduce OpenMask3D, a zero-shot, query-based method for open-vocabulary 3D instance segmentation that outperforms existing approaches on long-tail data by aggregating multi-view CLIP embeddings to segment novel objects using free-form descriptions."
"SELF-ALIGN is a minimally supervised method for self-alignment of large language models using synthetic prompts, concise human-written principles, and in-context learning to generate desirable responses with high performance and reduced human annotation."
"We present a simulation-based Follow-the-Virtual-Advice framework that achieves O(1/√N) optimality in infinite-horizon restless bandits without requiring the uniform global attractor property, leveraging insights from no-regret learning and full-feedback games."
"We propose a novel SVD-based algorithm that enables near-optimal recovery of unbalanced communities in the Stochastic Block Model without prior size assumptions, even when most clusters are small."
We introduce a Gini deviation-based policy gradient algorithm that overcomes the limitations of variance-based risk measures in reinforcement learning.
"FAMO is a dynamic, O(1)-complexity multitask optimization method that outperforms baseline techniques in both supervised and reinforcement learning by balancing task losses without extensive hyperparameter tuning."
"We systematically evaluate state-of-the-art Bayesian deep learning methods on WILDS benchmarks, finding that ensembling improves performance and calibration for most models but not for large transformer-based language models, where variational inference and SWAG excel in accuracy and calibration, respectively."
We introduce SCH stability and a neighboring-hypothesis matrix to derive tighter generalization bounds in SCO via information-theoretic methods.
"StateMask is a novel DRL explanation method that identifies critical states for reward achievement by masking state inputs, enhancing transparency in decision-making and revealing the role of factorized perception in agent behavior."
"We reformulate continual learning as a sequence modeling task using Transformers, enabling effective long-term credit assignment and improved performance across classification and regression benchmarks."
"We introduce energy-based normalizing flow (EBFlow), a generative model that uses score-matching to avoid Jacobian determinant computation, enabling faster training and improved performance on graph datasets using cluster permutations as input."
"MEMTO, a memory-guided Transformer with two-phase training and graph-based regularization, achieves a 95.74% average F1-score for anomaly detection in multivariate time series by leveraging memory updates and bi-dimensional deviation metrics."
We propose a reinforcement learning and gradient-based framework that reformulates discrete feature transformation as continuous optimization for efficient feature space exploration in machine learning.
"We propose an $(\epsilon,\delta)$-certified machine unlearning algorithm for minimax models that uses Hessian-based Newton updates with the Gaussian mechanism, achieves improved generalization and deletion capacity, and bridges results with standard statistical learning models."
"We propose Graph Mixture of Experts (GMoE), a GNN that adaptively selects node-specific information aggregation experts to improve handling of diverse and heterogeneous graph structures, yielding significant performance gains on OGB benchmarks."
"We introduce ActionBench and Paxion, which improve action knowledge in video-language models using a new DVDM objective, boosting action understanding from ~50% to 80% and enhancing downstream task performance."
"We introduce DPM-SNC, a diffusion probabilistic model framework for structured node classification that learns node label distributions and makes predictions under partial labeling by manifold-constrained sampling, with a new training algorithm and theoretical support for enhanced GNN expressivity."
"We propose a modified agglomerative clustering algorithm that recovers hierarchical structure using maximum average dot product, outperforming standard methods and adapting to online and high-dimensional data."
"We analyze student-teacher deviations in knowledge distillation, revealing that gradient bias and loss characteristics drive improved student generalization despite diverging from the teacher's outputs."
"We introduce Conditioned Constrained Policy Optimization (CCPO), a reinforcement learning framework that enables zero-shot adaptation to varying safety constraints using versatile value estimation and conditioned variational inference, outperforming baselines in safety and adaptability for dynamic real-world deployment."
"We propose and analyze adaptive explore-then-commit algorithms for high-dimensional linear contextual bandits without sparsity, demonstrating their effectiveness via spectral methods and simulations."
We propose a molecular representation learning framework that uses graph-based regularization and a first-encode-then-separate strategy with residual vector quantization and self-supervised learning to achieve robust and invariant representations under distribution shifts.
"We introduce ARTree, a deep autoregressive model using GNNs to represent tree topologies as sequences of leaf additions, enabling efficient and accurate phylogenetic inference without heuristic features."
"FastSA is a simulated annealing-based algorithm that efficiently reduces GPU memory usage by up to 73% in large neural network training with only a modest computational overhead increase, providing a scalable alternative to previous gradient checkpointing methods."
We propose an accelerated quasi-Newton proximal extragradient method with gradient-only information that achieves a convergence rate surpassing NAG in high-dimensional convex optimization.
"The authors propose a recursive criticize-and-improve (RCI) prompting method that, combined with chain-of-thought, outperforms existing large language model approaches on computer tasks without requiring extensive demonstrations or reward functions."
"CMMN is a normalization method that adapts biomedical signals’ power spectrum density to a Wasserstein barycenter, enabling efficient cross-subject, cross-session, and cross-hardware EEG adaptation without retraining and achieving performance comparable to more computationally expensive domain adaptation methods."
"We introduce Partitioned Calibration Error (PCE), a novel calibration metric based on semantic data partitioning, and show that jointly learning semantic-aware partitions and tailored calibration functions with deep features and logits significantly improves model accuracy and calibration across diverse datasets and architectures."
Neural-Q-Whittle achieves a finite-time convergence rate of $\mathcal{O}(1/k^{2/3})$ for Whittle index-based Q-learning with neural network approximation in restless multi-armed bandits with Markovian arms.
"We identify integration windows in large language models using a word-swap procedure and show they transition from exponential to power-law dynamics across layers, aligning more with structural boundaries at deeper layers, suggesting a learned mechanism analogous to human language processing."
"We propose a descent-style algorithm that robustly recovers a low-rank matrix from linear measurements even when some are adversarially chosen, without relying on convex optimization or RIP conditions."
"Vanilla knowledge distillation, when combined with advanced data augmentation on large-scale datasets, achieves state-of-the-art accuracy and demonstrates untapped potential in large-scale learning environments."
"We propose an optimal asymmetric graph construction method for graph-based semi-supervised learning that improves label prediction robustness via attention and data augmentation, outperforming existing approaches on benchmark datasets."
"The paper shows that the Greedy algorithm for determinant maximization achieves an almost optimal composable coreset approximation of $O(k)^{3k}$, surpassing previous guarantees and exhibiting strong practical performance supported by both theoretical local optimality bounds and empirical validation."
"We propose (pseudo)-Gibbs sampling with moment matching to enable effective sampling from clean Energy-Based Models when trained with noisy objectives like Denoising Score Matching, improving scalability and consistency for high-dimensional and matrix completion tasks."
"We propose a statistically consistent, asymmetric softmax-based surrogate loss that resolves the miscalibration and unboundedness issues in learning-to-defer frameworks, and empirically validate its improved calibration and performance on CIFAR-10."
"We propose a model and method that exploit latent low-rank and sparse structure to efficiently estimate all unit-specific potential outcomes under combinatorial interventions with a sample requirement of poly(r) × (N + s²p), enabling practical causal inference in high-dimensional settings."
MindEye is a new fMRI-to-image model that uses contrastive learning and diffusion priors to reconstruct and retrieve viewed images from brain activity with state-of-the-art accuracy and fidelity.
"We derive non-asymptotic and asymptotic bounds on the coverage probability of prediction intervals for random forests, showing stable performance even with heavy-tailed responses and minimal computational cost."
"We propose a contrastive split-latent permutation autoencoder framework for EEG signal conversion that disentangles subject (style) and task (content) representations, enabling generalizable and zero-shot transfer between subjects."
We propose and demonstrate a self-recalibrating method for intracortical brain-computer interfaces that maintains high decoding accuracy (93.84%) for over a year in a human user without interruption.
"The epinet is an innovative architecture that enables conventional neural networks to estimate uncertainty as effectively as large ensembles with far less computation, introducing a new paradigm for epistemic neural networks (ENN) beyond traditional Bayesian approaches."
"We prove that with a novel initialization, alternating gradient descent achieves provably fast convergence for asymmetric matrix factorization, with iteration complexity depending on the spectral condition number and independent of the target rank."
"We show that a path-following optimization method globally converges to the minimum population loss for learning bivariate acyclic directed graphical models, challenging the assumption that such non-convex problems are prone to spurious solutions."
"We introduce the ""weak discrete gradient"" to bridge the gap between continuous differential equation approaches and discrete optimization, enabling systematic analysis and the design of new optimization methods with strong convergence guarantees."
"We introduce a Discontinuous ReLU network that models the principal's utility as a piecewise affine function to enable machine-driven, risk-minimizing contract design with privacy and scalability."
"We present ATM, a gradient-based, Gumbel Softmax-driven method that efficiently generates adversarial perturbations to text prompts, exposing a significant vulnerability in text-to-image models by achieving up to 91.1% attack success on short prompts through subtle word modifications that preserve category keywords."
We propose a Bayesian framework that adaptively infers and regularizes VAE network structures to prevent overfitting and improve performance on image tasks like super-resolution and deblurring.
"Mechanic is a learning rate tuning method inspired by cyclical step-sizes and Hessian spectral gaps that adapts learning rates dynamically and outperforms manual tuning on large-scale deep learning tasks, especially for quadratic losses and cyclical schedules."
We propose a black-box membership inference attack using quantile regression that achieves competitive performance with shadow models while requiring only a single trained model and no knowledge of the target architecture.
We demonstrate that learning Kohn-Sham charge densities with density models enables combinatorial generalization and accelerates Density Functional Theory convergence by 13% on unseen catalysts.
We propose a causal transportability-based bandit algorithm that leverages batch data from related environments to achieve sub-linear regret with improved sample efficiency.
We propose delayed stochastic optimization methods for distributed networks that achieve tighter convergence rates dependent on expected delay and mitigate the impact of arbitrary communication delays via a safeguarding step.
We propose Low-Rank Decomposition-based GNNs that leverage low-rank matrix and tensor decompositions to improve self-supervised learning by preserving local class-specific properties and capturing long-distance relationships in graphs.
"ConRad is a new 3D reconstruction method that uses a single RGB image and pretrained diffusion models to generate realistic, viewpoint-faithful 3D models with superior quantitative performance on ShapeNet compared to existing approaches."
"We propose an outlier-robust Wasserstein DRO framework that accounts for both geometric and non-geometric data contamination, providing minimax optimal risk bounds and efficient computation with improved performance in low-dimensional feature settings."
Adversarial training with learnable graph diffusion and neural attention effectively defends GNNs against adversarial structure perturbations.
"$CoLLAT$ is a contrastive audio-to-text grounding framework that enhances fine-grained audio understanding in language models and achieves state-of-the-art performance across audio classification, cross-modal retrieval, and audio-guided image generation."
FdeHBO is a Hessian/Jacobian-free adaptive method for nonconvex-strongly-convex bilevel optimization achieving O(ε^{-1.5}) sample complexity with O(1) samples per iteration.
We introduce a min-max game-theoretic framework for responsible AI that offers two algorithmic solution classes and empirically demonstrates effectiveness against subpopulation shifts.
"We propose a structural and temporal cross-modal knowledge distillation framework (STXD) that significantly improves multi-view 3D object detection by transferring knowledge from LiDAR to image modalities, yielding up to 4.5% gains in NDS and mAP on nuScenes."
AMPLIFY is a novel framework that automates rationale generation using post hoc explanations to improve Large Language Model performance by 10–25% across multiple reasoning tasks without human annotation.
"Transition-Constant Normalization (TCN) is a parameter-free, computationally efficient normalization method that improves performance across diverse image enhancement tasks and can be easily integrated into various architectures."
"We rigorously analyze the convergence and robustness of local Bayesian optimization strategies in high-dimensional settings, demonstrating their rate-optimal performance and effectiveness in recovering data structure under streaming and noisy conditions."
"The Base-$(k+1)$ Graph is a novel decentralized learning topology that achieves fast consensus with low communication overhead, outperforming traditional topologies in decentralized stochastic gradient descent."
"RIVAL introduces a diffusion-based pipeline that aligns latent distributions and leverages cross-image attention to generate high-quality, high-resolution variations of real-world images, outperforming existing methods in semantic, perceptual, and structural metrics."
"We unify GANs and particle-based generative models through a framework that incorporates sparse representations and dispenses with explicit generators, validated empirically and leveraging distributed computing for high-dimensional data."
"Demo2Code is a novel framework that generates robot task code from demonstrations by recursively summarizing demonstrations and expanding concise specifications into code, evaluated on robot task benchmarks including a new grid-based kitchen simulation called Robotouille."
We show that self-supervised and language-supervised visual representations can outperform those trained on real images when using properly generated synthetic images with multi-positive contrastive learning.
"We compare S4-based and Transformer world models for model-based reinforcement learning, finding that S4-based models (S4WM) better handle long-term dependencies and improve training efficiency in high-dimensional sequence generation."
Topological parallax measures geometric similarity between deep learning models and reference datasets via topological data analysis to enhance robustness and generalization during training.
"We present a grid-independent, resolution-invariant neural PDE model with an efficient probabilistic framework that learns from noisy and partial spatiotemporal observations and outperforms prior methods on complex datasets."
"We propose dual pseudo training (DPT), a semi-supervised method integrating diffusion models that achieves state-of-the-art classification and generation results, especially under severe label scarcity."
We theoretically and empirically show that deep neural collapse occurs in earlier layers of overparametrized networks and is characterized by a unique global optimum in an unconstrained features model across multiple nonlinear layers.
"WireMask-BBO is a black-box optimization framework that improves VLSI chip floorplanning by using a wire-mask-guided greedy procedure to reduce wirelength and avoid macro overlap, demonstrating up to 50% improvement and showing the potential of BBO and bandit algorithms in EDA optimization."
"This paper provides nearly optimal VC-dimension and pseudo-dimension estimates for the derivatives of deep neural networks using randomized techniques, advancing theoretical foundations for physics-informed machine learning models."
We propose a risk-averse deep reinforcement learning framework using coherent distortion risk measures and kernel methods that achieves robust safety and performance across uncertain test environments without minimax optimization.
"We introduce efficient algorithms to approximate ℓ_p sensitivities of matrices, enabling scalable variable selection and revealing low intrinsic dimensionality in real datasets."
"We propose an online algorithm that enables two firms to learn and converge to the unique stationary Nash equilibrium in a dynamic price competition with opaque information, using diminishing step-sizes to achieve no-regret learning and \(\mathcal{O}(1/t)\) convergence under the multinomial logit demand model with reference prices."
"We analyze how data augmentation with out-of-distribution and in-distribution data, and robustness measures, affect the generalization gap in adversarially trained models across CIFAR and ImageNet."
This paper explains why deep neural networks struggle more to learn complex concepts by analyzing how automorphism-based graph neural networks process symmetrical patterns and variable interactions.
"Smaller batch sizes, when properly managed, can improve performance in value-based deep reinforcement learning with replay memories in complex environments."
"We introduce Greedy Rejection Coding (GRC), a relative entropy coding algorithm that achieves efficient sample encoding and compression with theoretical runtime guarantees for unimodal continuous distributions, outperforming prior methods like A* coding and enabling practical applications such as learnt compression and federated learning."
"We extend denoising diffusion models to infinite-dimensional non-Euclidean spaces by introducing geometric priors and equivariant score networks, enabling symmetry-respecting generation and accurate modeling of complex scalar and vector fields in scientific data."
"We propose an optimization-based model of evaluation bias in admissions and hiring, showing how information constraints and risk aversion shape biased outcomes and offering interventions to mitigate them."
"We propose Cola, a framework that coordinates multiple vision-language models via a large language model to achieve state-of-the-art performance on visual reasoning tasks."
"We show that passive data, combined with structural modeling and natural language explanations, enables agents to generalize causal intervention strategies, even for novel variables and in confounded environments."
We derive the first non-trivial finite-time error bounds for average-reward policy-based RL that converge to zero and enable scalable performance measurement in sparse-reward MDPs.
"We present HiP, a hierarchical planning model integrating language, vision, and control foundation models that achieves superior performance on long-horizon table-top manipulation tasks."
"We introduce TRIAGE, a model-agnostic data characterization framework for regression that uses conformal predictive distributions to score and filter training samples based on estimation quality, improving model performance and enabling new paradigms in dataset and feature selection."
"We propose a polynomial-time algorithm that approximates the size and escape probability of SafeZones in MDPs within a near 2-factor, addressing computational hardness and enabling safer, more efficient policy learning."
"We show that in an alternating two-player online linear optimization setting, the learner can achieve regret bounds of $\mathcal{O}((\log n)^{4/3} T^{1/3})$ against an adversarial opponent and $\mathcal{O}(\log T)$ when the opponent is restricted to two actions, demonstrating that the standard $\Omega(\sqrt{T})$ lower bound no longer holds."
"We generalize the Individual-Global-Max principle to risk-sensitive multi-agent reinforcement learning and propose RiskQ, a value factorization method that satisfies the new Risk-sensitive IGM principle for risk metrics like VaR and achieves strong empirical performance."
"We develop a robust, doubly-robust framework for fair, constrained policy optimization in heterogeneous domains, with an application to reducing surveillance disparities in pretrial release recommendations."
"Chain-of-Thought prompting enables autoregressive Transformers to solve mathematical and decision-making tasks step-by-step despite their inherent limitations in direct deduction, thereby explaining their improved performance."
"HUME is a model-agnostic unsupervised learning framework that leverages linear separability in learned representations to infer human-aligned labels without supervision, outperforming existing baselines on image classification datasets."
"G2MILP is a deep generative framework that synthesizes realistic mixed-integer linear programming instances by modeling them as bipartite graphs and using a masked variational autoencoder, enabling the creation of novel, structurally and computationally complex MILP problems for solver improvement when real data is scarce."
"This paper proves a no-go theorem showing that learning an unknown quantum state with quantum neural networks becomes exponentially hard in the number of qubits below a critical loss threshold, limiting the effectiveness of shallow QNNs and linking local minima curvature to quantum Fisher information."
"We introduce a self-supervised polar architecture for next-frame prediction in videos, inspired by biological vision and neural network fast weights, which outperforms traditional motion compensation and aligns with V1 cell models."
"We propose ESTAG, an equivariant spatio-temporal GNN that uses past trajectory sampling and discrete Fourier transforms to capture non-Markovian dynamics and outperforms existing methods on molecular, protein, and macro-scale physical simulation benchmarks."
"We propose DP-FEST and DP-AdaFEST, two algorithms that efficiently integrate differential privacy into large embedding model training while preserving gradient sparsity and maintaining accuracy."
"We systematically analyze graph neural networks for link prediction in knowledge graphs, showing how their expressive power and regularization relate to relational Weisfeiler-Leman algorithms and justifying non-random data splitting due to graph structure."
ConPreDiff enhances diffusion models by incorporating context prediction during pretraining to improve semantic coherence and achieve new state-of-the-art results in image generation tasks.
"We introduce coop-CBM, a Concept Bottleneck Model enhanced with fine-grained concept labels and a concept orthogonal loss to improve interpretability and classification accuracy under distributional shift."
"We introduce CLeAR, a novel continual learning algorithmic reasoning framework that enables near-zero forgetting and backward transfer on abstract logical tasks across the Chomsky hierarchy, addressing vanishing gradients via low-rank passthrough architectures."
"We analyze the limiting covariance structure of modified transformer attention under infinite depth and width, showing that architectural changes ensure stability and prevent rank degeneracy in deep models for image data."
"We propose online fine-tuning during search to improve heuristic accuracy for game solving, enabling efficient solution of challenging 7x7 Killall-Go problems with significantly reduced computation compared to AlphaZero-based baselines."
"We introduce Robust Graph Information Bottleneck (RGIB), an information-theoretic method for robust link prediction under bilateral edge noise, demonstrating improved representation resilience across multiple datasets and GNN architectures."
The Sparsified Online Newton (SONew) method is a scalable second-order training algorithm that leverages sparsity and LogDet divergence to achieve faster convergence and improved generalization in large-scale deep neural networks compared to existing methods.
"LVM-Med is a large-scale self-supervised model trained on 1.3 million medical images across multiple modalities and organs, introducing a graph-matching-based contrastive learning method that outperforms existing models on 15 downstream medical tasks."
"We empirically show that in shallow ReLU networks, explicit regularization with a vanishing weight decay drives empirical risk minimizers toward minimum norm interpolants, revealing the implicit bias of optimization algorithms as network width and data size increase."
"We introduce a learning-based method that computes optimal equilibria in extensive-form games by framing them as zero-sum games, demonstrating convergence and scalability on benchmark and auction design problems."
"We show that training a one-hidden-layer ReLU network with correlated inputs via gradient flow from small initialization converges to zero loss and implicitly minimizes parameter rank, with distinct behavior from norm-minimizing interpolators, as confirmed by numerical experiments."
"We propose Retrieval-Augmented MIL (RAM-MIL), which uses Optimal Transport-based retrieval and a memory of labeled instances to improve Multiple Instance Learning performance, especially out-of-domain, and enhance interpretability."
"KOSMOS-1 is a multimodal large language model trained on diverse web-scale data to perform language, visual, and cross-modal tasks, including OCR-free NLP and nonverbal reasoning, demonstrating strong transfer learning across modalities."
"We propose Penguin, a homomorphic encryption-based ciphertext packing technique that accelerates and reduces memory overhead of graph convolutional network inference by up to 10× and 79%, respectively, enabling efficient and private graph analytics on encrypted data."
We analyze how normalization in Sharpness-Aware Minimization stabilizes convergence and improves generalization by enabling exploration of effective minima.
"We introduce a novel training method for generative models that optimizes the precision–recall trade-off by minimizing a new family of PR-divergences, improving generative performance as measured by log-likelihood on benchmarks like ImageNet."
"We introduce memory-efficient, robust online learning algorithms for the experts problem under adaptive inputs and heavy-tailed noise, establishing tight space–regret trade-offs and showing optimality of a natural deterministic algorithm up to polylog factors."
We propose a Bayesian actor-critic algorithm using preference feedback that achieves near-optimal regret and minimizes expert queries in contextual bandits and imitation learning without direct reward signals.
"We present the first $(1+\varepsilon)$-approximation algorithm for $(k,z)$-clustering in the sliding window model, achieving near-optimal space and time efficiency and introducing an online coreset framework that reveals higher sample complexity than offline coresets."
"We propose viewing attention mechanisms as structural inference over implicit adjacency graphs, unifying them with Gaussian Mixture Models, alignment, and Hopfield networks, and extending them with gradient-based and geometric priors to better model hierarchical data and adapt context windows."
"Boundless DAS efficiently uncovers interpretable and regular causal structures in large language models like Alpaca, revealing that it uses a simple causal model for numerical reasoning."
"We introduce a quantum Gaussian process-UCB algorithm that achieves polylogarithmic regret for non-linear reward optimization, surpassing classical Bayesian optimization lower bounds and demonstrating practical quantum speedup in simulations and on real quantum hardware."
"We analyze the oracle complexity of the switching subgradient method for finding nearly stationary points in non-convex constrained optimization with weakly convex objectives and convex or weakly convex constraints, showing it achieves comparable efficiency to double-loop methods using a simpler single-loop approach."
"We empirically study catastrophic forgetting in reinforcement learning and propose Learning-to-Modulate, a method that preserves pre-training task performance during fine-tuning by modulating information flow in frozen networks, outperforming prior approaches on Meta-World and DMControl benchmarks."
"We propose an encryption method that preserves Gini impurity in random forests during training, enabling secure computation without decryption while resisting adversarial attacks."
"We derive closed-form expressions for the denoising mean-squared error of high-dimensional tied-weight autoencoders with skip connections and show their advantage over standard autoencoders, linking to principal component analysis and validating on real data."
"The paper introduces Continual Domain Generalization over Temporal Drift (CDGTD) and proposes EvoS with a multi-scale attention module to handle gradual, sequential domain shifts in time-series forecasting."
"We introduce Wasserstein Quantum Monte Carlo (WQMC), a novel method that reformulates Quantum Variational Monte Carlo as Fisher–Rao gradient flow in the space of Born distributions using the Wasserstein metric, achieving faster convergence than conventional QVMC for solving the quantum many-body Schrödinger equation."
"We analyze the worst-case query time of graph-based approximate nearest neighbor search algorithms—HNSW, NSG, and DiskANN—finding that only DiskANN's slow preprocessing variant achieves poly-logarithmic query time with constant approximation under bounded intrinsic dimension, while others require linear query time in worst cases."
"We introduce Bayesian optimization with cost-varying variable subsets (BOCVS), propose a no-regret Gaussian process algorithm for it, and show that selecting cheaper, informative variable subsets reduces regret and outperforms baselines."
"Three-layer neural networks achieve lower sample complexity and superior nonlinear feature extraction compared to two-layer networks, as theoretically established for hierarchical data structures."
GESS is a diffusion-based model that uses expanded semantics and structural fMRI information to reduce the semantic gap and improve fMRI-to-image reconstruction.
We propose a low-variance pairs estimator for causal effect estimation under inverse probability weighting that achieves near-RCT precision without complex modifications.
"We propose a diffusion-based low-light image enhancement method with global structure-aware and uncertainty-guided regularization that outperforms existing approaches in quality, noise reduction, and reflectance recovery."
"STAR introduces an efficient class-incremental segmentation method that uses compact prototypes and novel losses to mitigate catastrophic forgetting and class imbalance without storing raw images, outperforming existing approaches on Pascal VOC 2012 and ADE20K."
"RangePerception, a range view-based LiDAR 3D object detection framework featuring a Range Aware Kernel and Vision Restoration Module, achieves BEV-level accuracy with greater efficiency, outperforming previous RV methods and matching a leading BEV method at higher speed."
We introduce a combinatorial dimension that characterizes learnability in realizable regression and construct minimax instance optimal learners for both PAC and online settings.
"We introduce participatory systems, model-agnostic prediction models that enable individuals to opt into personalization at prediction time, improving privacy, consent, and fairness in clinical prediction tasks while enhancing user engagement."
We propose a method to learn optimistic symbolic world models for reinforcement learning that accelerates learning in sparse reward environments and outperforms existing RL approaches.
"We theoretically characterize the training dynamics of a two-layer ReLU network on linearly separable data from random initialization to convergence, revealing distinct phases and nonlinear behaviors influenced by learning rate decay."
"We analyze gradient flow training of two-layer linear neural networks and show that small initialization biases the optimization toward low-rank hidden layers, with the evolution of weight singular values explained by a hidden mirror flow."
"VisionLLM is an LLM-based framework that treats images as a foreign language to flexibly handle diverse vision tasks, achieving over 60% mAP on COCO and demonstrating competitive performance with detection-specific models."
"We introduce Structured Voronoi Sampling (SVS), a gradient-based text generation method using Hamiltonian Monte Carlo that outperforms existing approaches in aligning samples with target distributions and producing fluent, diverse, controlled outputs."
We introduce a generative subgame solving framework that uses a diversity-based generation function and reinforcement learning to efficiently solve large extensive-form games with proven performance gains over traditional methods.
"The Cross-Episodic Curriculum (CEC) algorithm enhances Transformer agents’ learning efficiency and generalization by structuring cross-episodic experiences as a curriculum, outperforming traditional methods in multi-task and imitation learning settings."
"We introduce a method using conditional GFlowNets trained on tailored MDPs to efficiently sample high-quality solutions for combinatorial optimization problems, with publicly available code and empirical validation on synthetic and real-world instances."
"We provide a PTAS for learning random constant-depth neural networks with unbounded loss using PAC-Bayesian techniques, achieving poly(1/ε) time and sample complexity on spherical input distributions."
"We present a $\tilde O(k)$ amortized-update, $\tilde O(k^2)$ query-time $O(1)$-approximation algorithm for the fully dynamic $k$-median problem in metric spaces, with a matching lower bound, advancing dynamic community detection."
PICProp is a physics-informed bi-level optimization method that estimates confidence intervals for deterministic PDEs with probabilistic guarantees and without heavy assumptions.
RECODE improves zero-shot visual relation detection by generating and weighting composite visual-textual prompts using large language models.
"GeoCLIP is a CLIP-inspired model that uses GPS positional encoding to enable image-to-GPS geo-localization with continuous, high-resolution representations, outperforming traditional cell-based methods even with limited data and supporting text-based queries."
"Barrier Hamiltonian Monte Carlo (BHMC) introduces an involution checking step in Hamiltonian Monte Carlo to enable unbiased, reversible sampling from Gibbs distributions on Riemannian manifolds defined by self-concordant barriers."
"We propose scalable algorithms that directly incorporate missing values into fair multi-label classification, outperforming imputation-based methods in both fairness and accuracy."
"We present transient NeRFs that integrate raw, time-resolved lidar photon counts to reconstruct scene geometry and appearance from sparse multiview lidar data at picosecond resolution, outperforming existing lidar-supervised NeRFs on a novel benchmark."
We introduce a contrastive loss-based fine-tuning method for diffusion models that reduces discretization error and improves sample quality and sampling efficiency.
"We propose detecting AI-generated text by measuring the intrinsic dimensionality of text embeddings, which is consistently lower in AI-generated texts and enables a model-agnostic detector outperforming SOTA methods."
"We propose a predict-then-calibrate paradigm for contextual optimization that decouples prediction from uncertainty calibration, enabling flexible model choices and providing risk and robustness guarantees with empirical performance gains."
"Scissorhands is a selective token retention method that reduces large language model KV cache memory usage by up to 5× during inference without loss of quality, and can achieve up to 20× memory compression when combined with quantization."
We propose using pre-trained large language models to generate PDDL world models and integrate them with domain-independent planners to improve compositional task planning efficiency.
"We show that learning exact Nash equilibria in zero-sum K×K matrix games requires linear queries in K, and derive a lower bound of Ω̃(log(1/(Kε))) for approximate equilibria, highlighting gaps between upper and lower bounds."
"We enhance compositional generalization in deep networks by combining iterated learning with simplicial embeddings, inspired by graph representation learning, and demonstrate improved performance on vision and molecular graph benchmarks."
"We present a point-based NeRF method with Linear Blend Skinning that learns dynamic 3D reconstructions and skeletal models from sparse multi-view video, enabling high-fidelity reanimation and pose synthesis with reduced computation time and without object-specific templates."
"We modify Structured state space sequence (S4) models to enable fast, parallelizable hidden state initialization and demonstrate their superior performance and speed over RNNs and Transformers in reinforcement learning, meta-learning, and multi-objective tasks."
"We provide a theoretical framework showing that multimodal learning achieves better generalization than unimodal learning by a factor of $O(\sqrt{n})$ due to sub-network interactions across modalities, especially when connectivity and heterogeneity are present."
"We analyze a generalized mixture labeling problem, deriving minimax sample complexity under MMD separation and empirically confirming the asymmetric trade-off between unlabeled and labeled sample sizes on Higgs boson and CIFAR-DDPM detection tasks."
"We introduce a multi-task representation learning model with action prediction and multi-scale architecture for forecasting complex, naturalistic behavior, demonstrating superior performance on the MABe 2022 Multi-Agent Behavior challenge and validating its robustness with large-scale batch training and parallelization."
"We introduce Successor Features Keyboard (SFK) with the Categorical Successor Feature Approximator (CSFA), a method that automatically discovers state-features and task encodings to enable efficient transfer learning and outperform baselines on long-horizon tasks in complex 3D environments."
HotBEV is a hardware-tailored camera-view BEV detector that achieves higher accuracy and lower latency on GPUs for autonomous driving than existing methods.
"We introduce a modularity-maximization-based method that automatically constructs multi-level skill hierarchies in reinforcement learning agents, revealing structured interactions and improving learning efficiency without manual intervention."
"ParaFuzz is an interpretability-driven framework that detects backdoor attacks in NLP models by identifying label discrepancies in paraphrased inputs, outperforming existing methods across multiple attack types and datasets."
We introduce no-linear-swap regret in extensive-form games and show that minimizing it efficiently reaches linear-deviation correlated equilibria.
"We introduce Continuously Weighted Contrastive Loss (CWCL), a novel contrastive loss that models continuous similarity for cross-modal 0-shot transfer, achieving 5–8% gains in image classification and 20–30% gains in speech-to-intent and keyword classification over existing methods."
"We show that stochastic gradient descent can recover the unknown direction in high-dimensional sparse single-index models with non-Gaussian data, under mild conditions, extending previous results based on Gaussian assumptions."
"SimMTM is a simple masked time-series modeling framework that preserves temporal continuity by reconstructing masked segments via weighted aggregation of neighboring points, improving performance on time series tasks including cross-domain fault detection."
"We propose a computationally efficient, robust algorithm for finding approximate second-order stationary points in nonconvex optimization under strong contamination, with sample complexity \(\widetilde{O}(D^2/\epsilon)\), and demonstrate its application to low-rank matrix sensing with near-optimal lower bounds."
"We propose a kernel-based framework using a confusion density matrix to classify UQ-identified uncertainties into OOD, boundary, and high-IDM categories, enabling systematic benchmarking of UQ methods."
"We propose a dual-level out-of-distribution detection framework that uses diverse low-level and high-level features to separately identify domain and semantic shifts in image segmentation, improving adaptation and detection performance over baseline models."
"We extend plug-and-play methods to Poisson inverse problems using a Bregman Score Denoiser as a proximal operator in a Bregman Proximal Gradient framework, achieving theoretically grounded convergence and improved restoration performance."
"We benchmark mutual information estimators across diverse domains and challenging distributions, revealing their strengths and limitations and providing guidelines for practical use."
"We propose Normalized Negative Loss Functions (NNLFs) as a robust alternative to MAE in the Active Passive Loss framework, yielding the Active Negative Loss (ANL) method that outperforms state-of-the-art approaches on noisy datasets by focusing on memorized clean samples and aligning with sparse learning principles."
"This paper introduces a novel analytic framework to interpret and quantify the semantic features learned by deep saliency models, enabling principled analysis of their predictions and applications in diverse attention-related tasks."
"We present a self-supervised deep learning framework that, without position supervision, induces multiple grid cell modules in recurrent-convolutional networks, offering insights into their emergence and providing a new SSL paradigm."
"We introduce a positional reconstruction encoder-decoder (PR-EnDec) for semantic keypoint matching that encodes affine-invariant spatial embeddings and reconstructs graph spatial structure, improving correspondence identification on keypoint matching datasets."
"EvoFed reduces Federated Learning communication costs by sharing distance-based similarity measures instead of model parameters, achieving performance comparable to FedAvg while increasing local computation."
Progressive Guidance enhances diversity and robustness in diffusion model image generation by progressively applying discriminative gradients during sample generation.
"We introduce a dynamic codebook neural representation that efficiently compresses high-fidelity volumetric videos by exploiting spatial and temporal redundancies, achieving state-of-the-art rendering quality and superior storage efficiency on NHR and DyNeRF datasets."
"We introduce private everlasting prediction, a framework that enables adaptive, privacy-preserving classification over infinite domains with quadratic sample complexity in the VC dimension, addressing out-of-distribution and extreme class imbalance scenarios where traditional private learning fails."
FFNet is a flow-based feature fusion framework for vehicle-infrastructure cooperative 3D object detection that predicts feature flows from infrastructure sensor sequences to mitigate asynchrony and reduce transmission cost while improving rotation and temporal adaptability.
"Contrastive learning inherently performs distributionally robust optimization over negative sampling distributions, and introducing the Adjusted InfoNCE loss enhances robustness and convergence across image, sentence, and graph domains."
"We introduce the Q-exponential process, a probabilistic prior for functions that corresponds to $\ell_q$ regularization and outperforms Gaussian and Besov priors in functional modeling and image reconstruction."
"We establish exponential separations and lower bounds in quantum learning, showing that entangled measurements can drastically reduce sample complexity compared to separable measurements within the QSQ model, and introduce quantum statistical query dimension to characterize these limitations."
"We introduce a causal space framework—combining probability spaces and causal kernels—to rigorously model causal dynamics, including cycles and interventions, and demonstrate its application in learning causal relationships."
"We present improved frequency estimation algorithms for data streams that outperform prior learning-augmented methods, both theoretically and empirically, with and without heavy-hitter predictions."
"We propose h-GPI, a multi-step policy improvement method leveraging approximate environment models and policy libraries that, with increasing reasoning steps h, achieves better performance bounds and greater robustness to approximation errors than standard GPI, as demonstrated empirically."
"We introduce RMechRP, a contrastive learning-based reaction predictor trained on the RMechDB radical reaction database, providing interpretable and accurate predictions for radical chemical reactions."
"The paper introduces improved algorithms for solving box-simplex games and their matrix generalizations using area convexity and relative smoothness, enabling faster solutions to maximum flow, optimal transport, and related combinatorial and semidefinite programming problems."
"Mirror Diffusion Models extend diffusion models to generate data on convex constrained sets via a dual mirror space, enabling efficient sampling and embedding invisible watermarks, with improved performance for recommendation and privacy applications."
"We present a method that uses minimal human feedback and strategic perturbations, including texture descriptor manipulation, to censor undesirable outputs in pre-trained diffusion models for image generation."
"TopoSRL is a self-supervised learning method for simplicial complexes that uses simplicial augmentation and a novel contrastive loss to better capture local and global topological information, outperforming existing graph and supervised simplicial methods."
"We introduce H-duality, a novel correspondence between methods that efficiently minimize function values and those that reduce gradient magnitudes, yielding new algorithms that outperform existing ones and bridge theory-practice gaps in convex optimization."
"We introduce quantum algorithms for stochastic optimization that achieve improved dimension versus accuracy trade-offs for convex functions and faster critical point identification for non-convex functions, leveraging quantum variance reduction and outperforming classical methods."
"We propose Energy Discrepancy, a new EBM loss that approximates score matching and negative log-likelihood without score computation or MCMC sampling, enabling faster and more accurate learning of complex data distributions and improving EBM training as priors in variational decoders."
"We introduce a memory-efficient, probably approximately fair and optimal (PAFO) learnable fair streaming PCA algorithm with statistical guarantees and demonstrate its effectiveness on high-dimensional data."
"We introduce MIM4DD, a dataset distillation framework that uses mutual information and contrastive learning to maximize information retention in synthetic datasets, offering a principled, theoretically grounded alternative to heuristic-based methods."
"We propose interactive global adversarial training (iGAT), a theoretically grounded ensemble method that improves robustness to adversarial attacks by optimally distributing challenging examples and targeting ensemble weaknesses, achieving up to a 17% performance increase on CIFAR10 and CIFAR100 under white-box and black-box attacks."
GlucoSynth is a privacy-preserving GAN framework for generating high-quality synthetic glucose time series that maintains temporal dynamics and outperforms existing methods on large datasets.
We present a decomposed in-context learning with self-correction approach that achieves new state-of-the-art text-to-SQL accuracy of 85.3% on Spider and 55.9% on BIRD by breaking tasks into sub-tasks and leveraging LLMs' reasoning.
We propose a score-based diffusion model for object pose estimation that outperforms previous methods on the REAL275 dataset and generalizes to unseen categories and tracking without fine-tuning.
"We generalize the online binary prediction with expert advice framework to multiple experts and adversarial, periodically structured misinformation, proposing incentive-compatible, no-regret algorithms that outperform existing single-expert methods."
"We propose latent variational diffusion models to correct detector effects in LHC data, achieving more accurate and physics-constrained reconstructions of theoretical kinematic quantities than existing methods."
"We introduce Feature-Level Self-supervised Learning (FLSL), a bi-level feature clustering method using Vision Transformers that improves dense prediction tasks like object detection and segmentation by better aligning feature clusters with image semantics."
"TWIST introduces a method to train speech language models using pretrained textual language models as a warm start, outperforming cold-start models and demonstrating the importance of multilingual and large-scale pretrained models for speech tasks."
"ScoreOpt, a score-based diffusion defense using test-time optimization, improves adversarial robustness and inference speed on CIFAR10, CIFAR100, and ImageNet while addressing neural network intellectual property theft concerns."
"We propose Skill-It, a data selection framework that leverages ordered skill sets and targeted sampling to improve pre-trained language model performance more efficiently than random sampling."
We propose a weakly-supervised concealed object segmentation method that uses multi-scale feature grouping and SAM-based mask generation with data augmentation and entropy-based weighting to achieve state-of-the-art results on WSCOS benchmarks.
"ProtoConcepts enhances interpretability in image classification by learning and visualizing prototypical concepts from multiple image patches, enabling richer and more nuanced visual explanations than single-patch comparisons."
"We introduce interactive estimation, a framework for learning via similarity queries, with a new Dissimilarity dimension measure and an algorithm providing regret and PAC bounds polynomial in this dimension, unifying statistical-query learning and structured bandits."
"We introduce permutation-equivariant neural functional networks (NFNs) for processing neural network weights and gradients under neuron permutation symmetry, demonstrating their effectiveness on tasks such as generalization prediction, sparsity mask generation, and implicit neural representation modification."
We prove that determining the feasibility of causal imitation learning with context-specific independence relations is NP-hard and provide a graphical criterion and algorithmic approach for addressing this challenge.
"pFedHR is a federated learning framework that addresses model heterogeneity by dynamically reassembling personalized model ensembles using knowledge distillation, improving recommendation performance under both IID and Non-IID settings."
We address view inconsistency in text-to-3D generation by introducing score and prompt debiasing methods that improve 3D view consistency and realism.
"We introduce ε-fractional core-stability for Hedonic Games, enabling efficient computation of approximately stable coalition partitions and analyzing its tractability and behavior under various sampling distributions."
