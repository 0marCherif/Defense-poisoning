abstract
OCRA is a novel model that integrates object-centric and relational abstraction for systematic generalization in complex visual tasks.
We present a method that predicts mass spectra by decoding molecular subformulae as multisets of atoms and predicting their intensities.
The MoSo method identifies and removes less informative training samples by assessing each sample's impact on empirical risk using a gradient-based approximation.
"This work shows that a mathematically interpretable, simplified transformer-like network, designed to optimize the sparse rate reduction objective, achieves competitive ImageNet classification performance."
This paper presents a scalable dynamic programming framework for the constrained global optimization of decision trees.
"UNSSOR is an unsupervised neural method for speech separation in reverberant conditions, validated on two-speaker mixtures."
"Our study introduces algorithms to approximate the Rashomon set of sparse generalized additive models, enabling interaction with near-optimal models for user-defined constraints."
"We propose two efficient algorithms for generalized linear bandits with heavy-tailed rewards, achieving near-optimal $\widetilde{O}(dT^{\frac{1}{1+\epsilon}})$ regret."
This paper introduces an interventional factor model to generalize causal effects from past experiments to novel conditions with minimal assumptions.
EG and OGDA exhibit divergent last-iterate convergence behaviors in time-varying bilinear games.
"We establish an impossibility result for achieving $O(\sqrt{K\tilde{L}T})$ dynamic regret under Condorcet and SST preferences, but show it is plausible for $\text{SST}\cap \text{STI}$ distributions."
"We introduce algorithmic improvements to diffusion language models, culminating in Plaid 1B, which outperforms GPT-2 124M in likelihood and fluency."
This paper demonstrates the sub-optimality of the Naive Mean Field approximation in high-dimensional linear regression.
This paper demonstrates that additive mixtures of Matérn kernels are governed by their least smooth component and that multiplicative mixtures are well-suited for multi-output Gaussian Process models.
"We introduce a method that combines sparse feature learning with differentiable tree construction to produce small, performant, and interpretable decision trees."
This study introduces a Meta Privacy-Preserving Action Recognition framework that uses meta-learning and controllable data augmentation to improve generalization against novel privacy threats.
This manuscript introduces a non-parametric framework to decompose spurious variations in causal models.
"We study how to optimally query and use a limited budget of predictions for classical online problems like ski rental, secretary, and job scheduling."
This study provides the first theoretical generalization bounds for Federated Zeroth-Order optimization (FedZO) under Lipschitz and smoothness conditions.
Trajectory predictors must be evaluated interactively to bridge the gap between static dataset accuracy and real-world driving performance.
"NeuralEF is a neural framework that approximates efficient frontier solutions for resource allocation problems, accelerating large-scale financial optimization."
"We propose a neural network and normalizing flow method to estimate optimal, individualized multi-dose treatments using off-policy evaluation."
"We propose sign equivariant neural networks for spectral geometric learning, demonstrating their advantages through synthetic experiments."
"GuidedDiffTime, a guided diffusion model, efficiently generates constrained synthetic time series without re-training."
"We propose Spiking Early-Exit Neural Networks (SEENNs), which dynamically adjust timesteps to improve the efficiency of Spiking Neural Networks on image tasks, achieving 96.1% accuracy on CIFAR-10 with an average of 1.08 timesteps."
"This study establishes a rigorous mathematical link between Bayesian, variational Bayesian, and ensemble methods by reformulating deep learning optimization as a convex problem in the space of probability measures."
Fine-Grained RLHF uses sentence-level human feedback and multiple reward models to improve language model output precision.
OLES prevents performance collapse in DARTS by stopping the optimization of overfitting parametric operations.
This work introduces an unsupervised video-based object-centric learning method that uses a temporal feature similarity loss to achieve state-of-the-art performance on synthetic and real-world video datasets.
Our paper introduces two new notions of model equivalence for distributional reinforcement learning to enable risk-sensitive planning.
We propose an anatomically interpretable brain age prediction framework using covariance neural networks on cortical thickness data to identify Alzheimer's-related changes.
A masked two-channel decoupling framework is proposed for incomplete multi-view weak multi-label learning.
This paper formulates combinatorial optimization problems as Markov Decision Processes using bisimulation quotienting to improve out-of-distribution generalization.
We propose a diffusion-based generative sequence model for long-term planning and vision-based control that achieves state-of-the-art results on robotics and offline RL benchmarks.
"MFLD achieves global optimization with quantitative convergence rates for mean-field neural networks and MMD minimization, accounting for finite particles, discretization, and stochastic gradients."
This study finds that LLMs have limited autonomous planning capabilities (~12% success rate) but can serve as useful heuristics to guide AI planners.
"This work introduces a novel method to derive a tighter, nearly guaranteed upper bound on deep neural network error rates under distribution shift using unlabeled test data."
"NAR-Former V2, a hybrid GNN-Transformer model, surpasses GNN performance on the NNLQP dataset and achieves competitive accuracy on NASBench benchmarks for neural network representation learning."
CoPriv reduces communication in secure private inference by jointly optimizing the 2PC protocol and neural network architecture.
"We introduce a stepwise self-evaluation mechanism that enhances multi-step reasoning in LLMs, achieving significant accuracy improvements on benchmark tasks."
Distributional reinforcement learning achieves faster convergence via small-loss bounds that scale with the optimal cost.
UniHOI uses vision-language models and prompt learning to detect human-object interactions in both predefined and unseen open-world settings.
We propose imitation learning using vague feedback that distinguishes expert from non-expert demonstrations to recover and imitate the expert policy.
"TSDiff, an unconditionally-trained diffusion model for time series, achieves competitive performance in forecasting, refinement, and synthetic data generation through a novel self-guidance mechanism."
"We propose jointly optimizing caching and model selection to reduce inference costs for large language models, achieving up to 50× efficiency gains."
FFA is a bio-plausible learning algorithm that co-optimizes feedback and feedforward pathways for visual tasks like denoising and occlusion resolution.
We propose a method to detect hidden confounders in observational data from multiple environments and validate it through simulations and semi-synthetic benchmarks.
POMP is a prompt pre-training method for vision-language models that achieves state-of-the-art zero-shot performance on 21 image recognition datasets.
"We propose ""rewarded soup,"" a method that interpolates between weights of models fine-tuned on different rewards to achieve Pareto-optimal generalization across diverse tasks and human preferences."
Neural translation models may decipher animal communication if it shares sufficient complexity and common elements with human language.
"CogEval, a cognitive evaluation protocol, finds that large language models exhibit significant deficiencies in complex planning tasks requiring cognitive maps."
"We introduce low-variance unbiased estimators for matrix production to reduce activation memory in transformers, enabling up to 2.7x memory savings and larger batch sizes with minimal accuracy loss."
"ELDEN, a model-based reinforcement learning method, improves exploration in sparse-reward environments by using uncertainty in learned local dynamics between entities as an intrinsic reward."
"Neural networks in differentiable auction design exhibit mode connectivity, linking optimal solutions via paths that maintain performance."
TimeX is an interpretable surrogate model for time-series that uses self-supervised training to faithfully mimic pretrained models.
"Loss of plasticity in off-policy RL is addressed by the PLASTIC algorithm, which improves sample efficiency on Atari-100k and Deepmind Control Suite benchmarks."
"TempBalance, a novel layer-wise learning rate method guided by Heavy-Tailed Self-Regularization Theory, improves neural network training and classification performance across multiple datasets and architectures."
This study introduces a neural network model based on Pointnet++ to accurately separate leaf from wood points in sparse UAV lidar point clouds for improved biosphere monitoring.
"This paper introduces Individualized DP-SGD, a variant of Differentially Private Stochastic Gradient Descent that assigns unique privacy budgets to users to improve the privacy-utility trade-off."
Bayesian target optimisation reduces off-target stimulation in optogenetics by optimising laser power and target locations.
Jigsaw is a weakly-supervised framework for automated 3D fracture assembly.
LEACE is a closed-form concept erasure method that removes specified features from a representation while minimally altering it.
"ParaDiGMS accelerates diffusion model sampling by parallelizing denoising steps via Picard iterations, achieving 2-4x speedups without quality loss."
SynGen improves text-to-image generation by aligning cross-attention maps with the syntactic structure of the input prompt.
"Benign overfitting occurs in two-layer ReLU networks trained on noisy separable data, enabling correct test classification despite zero training loss."
L2Dive is a graph neural network-based diving heuristic for mixed integer linear programs that uses generative models and duality to improve solution times and primal-dual integrals.
We resolve an open problem by Balseiro et al. with an efficient algorithm achieving $\widetilde{O}(\sqrt{TK})$ regret for contextual bandits with adversarial losses and stochastic contexts.
"Riemannian residual networks extend ResNets to manifold-valued data, outperforming existing geometric networks on hyperbolic and symmetric positive definite matrix spaces."
"We introduce Deep Language Networks (DLNs) by stacking two LLMs, using variational inference to optimize learnable prompts, and show that a two-layer DLN outperforms a single-layer one and approaches GPT-4 performance with smaller models."
"NICE introduces noise-modulated consistency regularization to GANs, enhancing data efficiency and stability in image generation on limited or imbalanced datasets."
Randomized voting rules balance explainability and efficiency in rankings.
We introduce a VAE using an infinite mixture of asymmetric Laplace distributions to enhance quantile estimation and synthetic data generation while preserving privacy.
"Dynamic Prompt Learning mitigates cross-attention leakage in diffusion models, enabling precise text-based image editing."
"We introduce Nash Regret, a new metric for multi-agent bandits, and achieve $O(\sqrt{d/T} \log(T|\mathcal{X}|))$ regret for stochastic linear bandits."
IPMix is a data augmentation method that enhances classifier robustness against corruptions and adversarial attacks while preserving performance on clean data.
"This paper introduces GALET, an alternating gradient method for bilevel optimization with convex Polyak-Łojasiewicz lower levels, achieving $\tilde{\cal O}(\epsilon^{-1})$ iteration complexity."
OpenMask3D is a zero-shot open-vocabulary 3D instance segmentation model that uses multi-view CLIP feature fusion to segment objects from novel queries.
SELF-ALIGN is a method for self-aligning LLMs with minimal human input by using principle-driven reasoning to generate synthetic data for fine-tuning.
"Follow-the-Virtual-Advice, a new simulation-based framework, achieves an $O(1/\sqrt{N})$ optimality gap for restless bandits without the uniform global attractor property."
"This paper presents a novel SVD-based algorithm for recovering unbalanced communities in the Stochastic Block Model, achieving near-optimal recovery and surpassing existing methods."
This study proposes the Gini deviation as a superior alternative to variance for risk-averse reinforcement learning and introduces a policy gradient algorithm to minimize it.
"FAMO is a multitask optimization method that balances task losses with O(1) complexity, matching the performance of existing O(k) techniques."
"This study evaluates Bayesian deep learning methods on WILDS benchmarks, finding that ensembling improves performance but variational inference excels when fine-tuning large transformers."
Novel information-theoretic generalization bounds are derived for stochastic convex optimization using a new sample-conditioned hypothesis stability notion.
StateMask is a method that identifies critical states for DRL agents' final rewards by using a mask net to force random actions without degrading performance.
Sequence models trained under a meta-continual learning framework serve as effective backpropagation networks for continual learning across seven benchmarks.
"EBFlow integrates flow-based and energy-based models using score-matching to bypass Jacobian calculations, improving training speed and performance."
"We propose MEMTO, a memory-guided Transformer for multivariate time series anomaly detection, achieving state-of-the-art performance."
We propose a continuous optimization framework for discrete feature transformation using reinforcement learning and gradient-based search.
"We propose a certified minimax unlearning algorithm using Hessian-based Newton updates and Gaussian noise, achieving improved generalization and deletion capacity."
A Graph Mixture of Experts model improves performance on molecular property prediction tasks.
"Paxion, a framework using Discriminative Video Dynamics Modeling and a concealed dataset, improves video-language models' action knowledge from 50% to 80%."
We propose a Diffusion Probabilistic Model for Structured Node Classification (DPM-SNC) that learns a joint distribution over node labels and demonstrates superior performance across various graph settings.
Our study proposes an agglomerative clustering algorithm that merges clusters based on maximum average dot product to recover hierarchical structures from data.
Student-teacher deviations in knowledge distillation paradoxically enhance generalization due to gradient descent's implicit bias along top data eigendirections.
CCPO is a safe reinforcement learning framework that enables zero-shot adaptation to varying safety constraints without retraining.
"We propose an adaptive explore-then-commit algorithm for high-dimensional linear contextual bandits without sparsity, achieving a near-optimal regret bound."
We propose a molecular representation learning method with improved out-of-distribution generalization via invariant feature separation and regularization.
"ARTree is a deep autoregressive model using graph neural networks to decompose tree topologies, enabling efficient sampling and improved estimation for phylogenetic inference."
"FastSA, a simulated annealing-based recomputation algorithm, reduces GPU memory usage by up to 73% with minimal computational overhead."
"This paper introduces a quasi-Newton method achieving a convergence rate of $\mathcal{O}\bigl(\min\\{\frac{1}{k^2}, \frac{\sqrt{d\log k}}{k^{2.5}}\\}\bigr)$ for smooth convex optimization, outperforming Nesterov's acceleration when $k = \Omega(d \log d)$."
"We introduce RCI prompting, which outperforms existing methods on the MiniWoB++ benchmark by recursively criticizing and improving its output."
"CMMN is a novel EEG normalization method that uses optimal transport to adapt signals to a Wasserstein barycenter, significantly improving cross-subject, session, and hardware performance without retraining."
"We propose Partitioned Calibration Error (PCE), a framework that improves model accuracy and calibration by learning semantic-aware data partitions and their tailored calibration functions."
"Neural-Q-Whittle, a two-timescale Q-learning algorithm for restless multi-armed bandits using neural network approximation, is shown to converge at a rate of $\mathcal{O}(1/k^{2/3})$."
"LLMs exhibit integration windows that transition from exponential to power-law dynamics across layers, aligning with structural boundaries in deeper layers."
A descent-style algorithm robustly recovers low-rank matrices from semi-random measurements with adversarial corruptions.
"Vanilla knowledge distillation achieves state-of-the-art results on ImageNet-1K with ResNet-50, ViT-S, and ConvNeXtV2-T."
"This study introduces an optimal asymmetric graph structure for semi-supervised learning, with an efficient learning algorithm and experimental validation."
Greedy yields composable coresets with an almost optimal $O(k)^{3k}$ approximation for determinant maximization.
We propose a pseudo-Gibbs sampling method with moment matching to sample from denoising energy-based models trained via score matching.
Our work introduces a statistically consistent asymmetric softmax-based surrogate loss to address the unboundedness and miscalibration in learning-to-defer frameworks.
We propose a method to estimate all unit-intervention potential outcomes from limited data by exploiting low-rank and sparse Fourier structure.
MindEye is a novel fMRI-to-image method that surpasses existing benchmarks for retrieving and reconstructing viewed images from brain activity.
Random forests achieve stable prediction intervals with reliable coverage even under heavy-tailed responses.
We propose a contrastive split-latent permutation autoencoder for subject and task disentanglement in EEG signals.
A self-recalibrating intracortical brain-computer interface using pseudo-labels from a language model maintained 93.84% decoding accuracy over 403 days in a clinical trial.
Epinets enhance neural networks to efficiently estimate uncertainty with minimal computational overhead.
Alternating gradient descent with our proposed initialization achieves an $\epsilon$-optimal factorization of a rank-$r$ matrix in $T = C (\sigma_1(A)/\sigma_r(A))^2 \log(1/\epsilon)$ iterations.
A path-following scheme achieves global convergence to the minimum population loss for bivariate DAG learning.
"This paper introduces a ""weak discrete gradient"" framework to systematically derive discrete optimization methods and their convergence rates from continuous differential equation insights."
This paper proposes a deep learning method using a Discontinuous ReLU network to design optimal contracts.
"ATM, a gradient-based method using Gumbel Softmax, efficiently generates adversarial text prompts that compromise text-to-image models with high success rates."
We propose a Bayesian framework that adapts VAE network depth to data and prevents overfitting via layer-wise dropout.
Mechanic dynamically tunes the learning rate scale factor to match or surpass manual tuning performance across diverse deep learning tasks.
A quantile regression-based membership inference attack achieves competitive performance with significantly reduced computational cost.
"Our approach to catalyst modeling learns Kohn-Sham charge-densities pointwise, enabling combinatorial generalization to new elemental structures and reducing DFT convergence iterations by 13%."
Our work develops a bandit algorithm with sub-linear regret by leveraging causal invariances and batch data from disparate environments.
This paper introduces delayed stochastic algorithms for weakly convex optimization that achieve convergence rates dependent on the expected delay rather than the maximum delay.
We propose Low-Rank Decomposition-based GNNs for self-supervised learning that capture local and long-distance relationships without label information.
"Our method, Image Constrained Radiance Fields (ConRad), generates realistic 3D objects from a single RGB image, outperforming existing approaches."
An outlier-robust Wasserstein distributionally robust optimization framework is proposed to handle both geometric and non-geometric data perturbations.
We demonstrate that adversarially trained graph neural networks with learnable diffusion and attention mechanisms are a state-of-the-art defense against structural perturbations.
CoLLAT is a framework that enhances audio understanding in language models through contrastive locked tuning and a novel pretraining objective for audio-to-text grounding.
FdeHBO is a Hessian/Jacobian-free bilevel optimization method achieving $\mathcal{O}(\epsilon^{-1.5})$ sample complexity for nonconvex-strongly-convex problems.
"We introduce ""Responsible AI games,"" a framework addressing worst-case loss via min-max optimization, and propose two algorithm classes for these problems."
We propose a cross-modal distillation framework that enhances multi-view 3D object detection by transferring knowledge from LiDAR to image-based models.
AMPLIFY automates rationale generation using post hoc explanations to improve LLM accuracy by 10-25% on reasoning tasks.
"Transition-Constant Normalization (TCN), a parameter-free method, improves performance across various image enhancement tasks."
Local Bayesian optimization strategies achieve rate-optimal convergence in high-dimensional spaces by leveraging data structure through matrix component decomposition.
The Base-$(k+1)$ Graph is a novel topology for decentralized learning that achieves a fast consensus rate with low communication overhead.
"RIVAL, a novel diffusion-based pipeline, generates high-quality, high-resolution variations from a single real-world image by aligning latent distributions during inference."
We propose a unified framework for generative models by reinterpreting GAN generator training through the lens of particle-based models.
Demo2Code generates robot task code from demonstrations via recursive summarization and code synthesis.
Training self-supervised methods on properly sampled synthetic images from a text-to-image model can outperform learning from real images.
"S4WM, a novel world model using Structured State Space models, outperforms Transformer-based models in long-term memory retention and training efficiency for model-based reinforcement learning."
Topological parallax uses topological data analysis to assess geometric fidelity between models and datasets for improved robustness and generalization.
"We introduce a grid-independent, resolution-invariant model for learning partial differential equations from noisy, partial observations on irregular spatiotemporal grids."
"DPT, a novel semi-supervised learning method using a classifier and conditional diffusion model for pseudo-labeling and synthetic image generation, achieves state-of-the-art FID scores and top-1 accuracy on ImageNet with limited labels."
Our study theoretically and empirically demonstrates that deep neural collapse occurs across multiple non-linear layers in neural networks during binary classification.
WireMask-BBO is a black-box optimization framework for VLSI macro placement that minimizes wirelength and prevents overlap.
This paper establishes nearly optimal VC-dimension bounds for the derivatives of deep neural networks and characterizes their generalization error in Sobolev space.
This work introduces a model-free deep reinforcement learning framework using coherent risk measures to guarantee robust and safe performance in uncertain environments.
"This paper introduces efficient algorithms to approximate $\ell_p$ sensitivities for matrices, reducing computational costs."
"Our proposed online projected gradient ascent algorithm enables two firms to learn the unique stationary Nash equilibrium in a dynamic price competition under opacity, achieving \(\mathcal{O}(1/t)\) convergence."
"This study evaluates how measures like margin, smoothness, and flatness correlate with robust generalization by analyzing over 1,400 models trained on CIFAR-10, CIFAR-100, and ImageNet."
Deep neural networks learn simpler concepts more easily than complex ones due to the challenges of learning interactions requiring broader collaboration among input variables.
Smaller batch sizes can significantly improve performance in value-based deep reinforcement learning with replay memory.
Greedy Rejection Coding (GRC) is a new relative entropy coding algorithm that offers improved runtime and codelength guarantees for encoding samples from a target distribution.
Our work introduces a geometric diffusion model that integrates symmetry-respecting priors for generative modeling on non-Euclidean domains.
Our model explains evaluation bias in admissions and hiring as arising from a loss minimization problem under information constraints.
"Cola, a method using a large language model to coordinate multiple vision-language models, achieves state-of-the-art performance on visual reasoning tasks."
"Passive data enables agents to deduce and apply causal strategies through structural modeling and imitation learning, with language models extrapolating interventions from explanatory prompts."
We develop the first non-trivial finite-time error bounds for policy-based average-reward MDPs.
"HiP integrates language, vision, and action models into a hierarchical controller for long-horizon manipulation tasks, demonstrating superior performance."
TRIAGE is a model-agnostic framework that uses conformal predictive distributions to characterize data for regression tasks.
SafeZones are near-optimately approximated by a bi-criteria algorithm with polynomial sample complexity.
"We extend online linear optimization to alternating play and show that the standard Ω(√T) regret lower bound no longer holds, achieving Õ((log n)^(4/3) T^(1/3)) and O(log T) regret for the simplex and a ball."
"RiskQ, a value factorization method satisfying the proposed Risk-sensitive Individual-Global-Max principle, minimizes regret for risk-sensitive multi-agent reinforcement learning under common risk metrics."
"We propose a doubly-robust, constrained optimization framework for fair algorithmic recommendations under heterogeneous treatment effects and low take-up, applied to pretrial risk-assessment."
This paper theoretically demonstrates that Chain-of-Thought prompting enables constant-size transformers to solve complex mathematical and decision problems they cannot solve directly.
"HUME, a model-agnostic framework, infers human-like labels by finding linearly separable classes in a pretrained representation space, achieving state-of-the-art unsupervised classification."
G2MILP is a deep generative framework that synthesizes novel mixed-integer linear programming instances using a masked variational autoencoder on bipartite graphs.
"Learning an unknown quantum state with quantum neural networks faces fundamental limits, as the probability of avoiding local minima decreases exponentially with qubit count and only increases polynomially with circuit depth."
"A self-supervised model for next-frame prediction, inspired by the Fourier shift theorem, matches deep network performance while aligning with V1 cell structure."
"ESTAG, an equivariant spatio-temporal graph neural network, improves physical dynamics simulation by capturing non-Markovian interactions."
"DP-FEST and DP-AdaFEST are novel algorithms that maintain gradient sparsity and ensure differential privacy in large embedding models, achieving a $10^6 \times$ gradient size reduction while retaining competitive accuracy."
"This research investigates how Graph Neural Networks perform link prediction on knowledge graphs, analyzing model expressiveness and validating design choices like regularization through empirical testing.
備考：原文の冗長な部分や繰り返しを排除し、研究の核心（GNN による知識グラフのリンク予測、表現力の分析、正則化の検証）を簡潔にまとめています。"
ConPreDiff enhances diffusion models by incorporating context prediction during pre-training to improve semantic coherence and set new state-of-the-art performance in image generation tasks.
Coop-CBM improves classification accuracy over traditional models by using concept orthogonal loss to enhance concept separation.
CLeAR introduces a continual learning framework for abstract logical reasoning that achieves near-zero forgetting and enables backward transfer.
This paper models infinite-depth transformers via a stochastic differential equation derived from a modified attention mechanism.
"AlphaZero-based heuristics fail on out-of-distribution positions in game solving, a problem addressed by online fine-tuning, which reduces computation time by 76%."
Robust Graph Information Bottleneck (RGIB) improves link prediction in GNNs by enhancing representation robustness against edge noise.
The Sparsified Online Newton (SONew) method is a scalable second-order algorithm that achieves faster convergence and improved generalization for training large neural networks.
"LVM-Med, a self-supervised model pre-trained on 1.3 million medical images, outperforms existing models across 15 medical imaging tasks."
"Shallow ReLU networks converge to a minimum norm interpolant, a bias modulated by weight decay and observed in common optimizers."
Our study introduces a method for computing optimal equilibria in extensive-form games by training agents using zero-sum learning techniques.
Gradient flow training of a one-hidden layer ReLU network on correlated data induces an implicit bias towards low-rank solutions.
Retrieval-Augmented Multiple Instance Learning (RAM-MIL) uses Optimal Transport for nearest neighbor retrieval to improve performance and interpretability on in-domain and out-of-domain WSI classification tasks.
"KOSMOS-1 is a multimodal large language model trained on web-scale corpora to perform tasks including language understanding, generation, OCR-free NLP, and visual perception."
"Penguin, a novel homomorphic encryption ciphertext packing technique, reduces computational and memory overhead for secure graph convolutional network inference."
Normalization stabilizes SAM's convergence and enables efficient navigation of the loss landscape.
"We introduce PR-divergences, a new family of $f$-divergences, to train generative models by directly optimizing a chosen trade-off between precision and recall."
We introduce memory-efficient robust algorithms for online learning with experts under adaptive inputs and establish near-optimal space-regret trade-offs.
We propose an actor-critic algorithm for preference-based learning that achieves competitive regret bounds in contextual bandits and imitation learning while minimizing expert queries.
"We present the first near-optimal $(1+\varepsilon)$-approximation algorithm for $(k,z)$-clustering in the sliding window model, requiring $\frac{k}{\min(\varepsilon^4,\varepsilon^{2+z})}\,\text{polylog}\frac{n\Delta}{\varepsilon}$ space."
"Attention mechanisms in Transformers are re-interpreted as structural inference over implicit graphical models, unifying diverse architectures and suggesting new gradient-based and geometric generalizations."
"Boundless DAS, an evolution of Distributed Alignment Search, uncovers interpretable causal structures in the 7B-parameter Alpaca model."
"We propose a quantum-GP-UCB algorithm that achieves a poly-logarithmic regret bound, outperforming the classical square-root lower bound."
This paper analyzes the oracle complexity of the switching subgradient method for finding stationary points in weakly convex problems with convex or weakly convex constraints.
"L2M, a novel neural modulation method, mitigates catastrophic forgetting in pre-trained RL agents during fine-tuning on new tasks."
This work introduces an encryption method that preserves Gini impurity for private random forest training without decryption.
This study derives the denoising error of a two-layer autoencoder with a skip connection in the high-dimensional limit.
This paper introduces Continual Domain Generalization over Temporal Drift (CDGTD) and an Evolving Standardization (EvoS) method to generalize across sequentially arriving domains with gradual distribution shifts.
Wasserstein Quantum Monte Carlo accelerates convergence in quantum many-body ground-state calculations compared to traditional variational methods.
"Our analysis reveals that leading graph-based approximate nearest neighbor search algorithms, including HNSW, NSG, and DiskANN, exhibit linear worst-case query time for reasonable accuracy."
Bayesian optimization with cost-varying variable subsets is addressed via a no-regret Gaussian process algorithm that selects informative variable subsets to minimize cumulative cost.
"Three-layer networks learn hierarchical features more efficiently than two-layer networks, achieving superior sample complexity for non-linear functions."
GESS reconstructs images from fMRI data by integrating expanded semantics and structural information within a diffusion process.
A novel low-variance pairs estimator for causal model evaluation reduces the high variance associated with inverse probability weighting.
We introduce a diffusion-based method for low-light image enhancement that uses global structure-aware and uncertainty-guided regularization to improve image quality and detail recovery.
"STAR, a class-incremental semantic segmentation method using prototypes and statistical replay, sets a new state-of-the-art on Pascal VOC and ADE20K benchmarks."
"RangePerception, a range-view LiDAR detection framework using a Range Aware Kernel and Vision Restoration Module, matches the accuracy of leading bird's-eye-view methods while being 1.3x faster."
A novel combinatorial dimension characterizes learnability in realizable regression for both PAC and online learning.
"Participatory systems, a model-agnostic algorithm allowing individuals to opt into personalization at prediction time, improve performance, privacy, and consent in clinical prediction tasks."
We introduce a method that learns optimistic symbolic world models to accelerate reinforcement learning in sparse-reward environments.
"We theoretically characterize the full training process of a two-layer ReLU network on separable data, revealing a four-phase learning trend from simple to complex."
Gradient flow training of two-layer linear networks with small initialization favors low-rank solutions.
"VisionLLM, an LLM-based framework that treats images as a foreign language, achieves over 60% mAP on COCO, rivaling task-specific detection models."
"Structured Voronoi Sampling (SVS), a novel gradient-based MCMC method, more accurately samples complex text distributions for controlled generation."
We introduce a generative subgame solving framework that uses a diversity-based generation function and reinforcement learning to improve strategy performance in large imperfect information games.
The Cross-Episodic Curriculum (CEC) algorithm improves Transformer agents' efficiency and generalization in RL and imitation learning.
"GFlowNets are trained to efficiently sample diverse, high-quality solutions for combinatorial optimization problems."
We present a PTAS for learning random constant-depth neural networks with unbounded losses.
"We present a fully dynamic $O(1)$-approximation algorithm for $k$-median and $k$-means with $\tilde O(k)$ amortized update time and $\tilde O(k^2)$ query time, and provide a matching update time lower bound."
Physics-Informed Confidence Propagation (PICProp) introduces a method for estimating confidence intervals in deterministic PDEs using bi-level optimization.
RECODE improves zero-shot visual relation detection by decomposing predicates into descriptive components and using a weighted chain-of-thought approach.
"GeoCLIP introduces a CLIP-inspired method for image-to-GPS retrieval by modeling the Earth as a continuous function, establishing a new form of supervision for geo-localization."
Barrier Hamiltonian Monte Carlo (BHMC) is introduced for unbiased sampling from Gibbs distributions on constrained manifolds using a novel involution step.
Training classifiers on imputed data worsens fairness and accuracy; we propose adaptive algorithms that use missing data patterns to improve both.
Transient NeRFs use raw lidar photon count histograms to render novel views and capture transient light transport.
"We introduce a contrastive sampling method that reduces discretization error in diffusion models, improving sample quality and efficiency."
"Human and AI-generated texts are distinguished by a lower intrinsic dimensionality of text embeddings in AI content, enabling a detector that outperforms SOTA methods across domains and models."
We propose a generic predict-then-calibrate algorithm for contextual optimization that decouples prediction from calibration to provide model-agnostic risk guarantees.
Scissorhands reduces KV cache memory usage by 5× at inference by selectively retaining key tokens.
"Our method uses LLMs to generate PDDL domain models, which are then used with domain-independent planners to solve tasks."
This paper characterizes the query complexity for learning exact ($\epsilon=0$) and $\epsilon$-approximate Nash equilibria in $K \times K$ matrix games.
Iterated learning and simplicial embeddings improve compositional generalization in deep networks.
"Our method uses point-based representation and Linear Blend Skinning to learn a dynamic NeRF and skeletal model from sparse multi-view video, enabling reposable 3D reconstructions without object-specific templates."
"We present a modified Structured state space sequence (S4) model that enables fast parallel hidden state reset, achieving superior performance and speed over RNNs and Transformers in reinforcement learning tasks."
Multimodal learning achieves a generalization improvement of \(O(\sqrt{n})\) over unimodal methods when modalities are connected and heterogeneous.
"We generalize a known trade-off between labeled and unlabeled data for binary classification, extending it to mixed-class samples and demonstrating it empirically on Higgs boson detection and image generation tasks."
We introduce a multi-task representation learning model with action-prediction and multi-scale architecture that achieved top performance in the MABe 2022 behavior challenge.
We introduce the Successor Features Keyboard (SFK) and Categorical Successor Feature Approximator (CSFA) to automatically discover state-features and task encodings for improved transfer learning.
HotBEV is a hardware-optimized camera-view BEV detector that improves both accuracy (NDS/mAP) and speed on automotive GPUs.
Our framework uses modularity maximization to automatically generate multi-level skill hierarchies that enhance reinforcement learning.
"ParaFuzz, an interpretability-driven framework using semantic discrepancies in paraphrased inputs, outperforms existing methods in detecting diverse NLP backdoor attacks."
"This paper introduces a polynomial-time algorithm for minimizing linear-swap regret in extensive-form games, leading to efficient convergence to linear-deviation correlated equilibria."
"CWCL, a novel contrastive loss function that leverages continuous pairwise similarity, improves zero-shot cross-modal transfer performance by 5-30% on benchmark tasks."
Gradient descent with shallow neural networks can identify the unknown direction in high-dimensional single-index models under non-Gaussian distributions.
SimMTM is a masked time-series pre-training framework that reconstructs masked points via weighted aggregation of neighbors to preserve temporal fidelity.
"We present a robust framework for finding approximate second-order stationary points in nonconvex optimization with outliers, with applications to low-rank matrix sensing."
"We propose a confusion density matrix to classify uncertain examples as out-of-distribution, boundary, or high in-distribution misclassification."
A dual-level framework concurrently addresses domain and semantic shifts for improved out-of-distribution segmentation.
"This work introduces a Bregman Plug-and-Play method with a novel Bregman Score Denoiser to solve Poisson inverse problems, providing convergence guarantees and superior restoration."
Mutual information estimators are rigorously benchmarked across diverse distributions to guide their effective application.
Replacing MAE with normalized negative loss functions in robust learning frameworks improves performance on noisy datasets.
We present a novel analytic framework to interpret and quantify the implicit features learned by deep saliency models.
Self-supervised learning with convolutional networks produces multi-modular grid cells.
PREGM introduces a positional reconstruction encoder-decoder for semantic keypoint matching by leveraging spatial attributes and geometric context.
EvoFed reduces Federated Learning communication costs by sharing fitness-based similarity measures instead of model parameters.
Progressive Guidance improves diffusion model image diversity and robustness by strategically applying discriminative gradients during generation.
We introduce a dynamic codebook neural representation that reduces model size while maintaining rendering quality for volumetric videos.
"We introduce private everlasting prediction, a framework for answering an unbounded stream of adaptive queries while protecting the privacy of both the training data and the queries, with sample complexity quadratic in the VC dimension."
FFNet is a feature flow-based cooperative 3D detection framework that compensates for sensor asynchrony and reduces transmission costs.
"This study demonstrates that contrastive learning inherently performs distributionally robust optimization, reveals the temperature parameter's role as a Lagrange multiplier, and proposes a novel Adjusted InfoNCE loss to improve performance."
"The Q-exponential process provides a probabilistic framework for L_q regularization, offering a sharper penalty than Gaussian processes."
We show entangled measurements exponentially reduce quantum sample complexity compared to separable or statistical queries.
"We introduce a causal space framework, combining probability spaces and causal kernels, to rigorously model interventions and address challenges like cycles and latent variables."
"We present improved algorithms for frequency estimation in data streams that outperform prior learning-based methods, with further gains when augmented by learned predictions."
"We propose $h$-GPI, a multi-step successor feature method that improves zero-shot transfer in reinforcement learning with a performance guarantee."
RMechRP is a contrastive learning-based system that provides accurate and interpretable predictions for radical chemical reactions.
"We develop efficient algorithms for box-simplex and matrix games, advancing optimization in areas like maximum flow and optimal transport."
"Mirror Diffusion Models enable tractable diffusion-based generation on convex constrained sets, improving performance in recommendation systems and enabling watermarking applications."
We introduce a method using minimal human feedback to censor undesirable outputs in pre-trained diffusion models.
TopoSRL is a self-supervised learning method that uses simplicial augmentation and contrastive loss to learn representations of simplicial complexes.
H-duality establishes a one-to-one correspondence between methods that minimize function values and those that reduce gradient magnitudes.
We introduce quantum algorithms for stochastic convex and non-convex optimization that achieve superior dimension-accuracy trade-offs compared to classical methods.
We propose a novel Energy Discrepancy loss function for training energy-based models that avoids MCMC sampling and approximates both score matching and negative log-likelihood.
This work introduces a memory-efficient fair streaming PCA algorithm with statistical guarantees.
"We propose MIM4DD, a mutual information-based framework for dataset distillation that enhances existing methods through contrastive learning."
"We propose interactive global adversarial training (iGAT), a new ensemble defense method that increases robustness by up to 17% on CIFAR benchmarks."
GlucoSynth is a privacy-preserving GAN framework that generates synthetic glucose traces.
This work introduces a decomposed in-context learning and self-correction method that achieves state-of-the-art text-to-SQL accuracy on the Spider (85.3%) and BIRD (55.9%) datasets.
We propose a diffusion-based generative model for category-level object pose estimation that achieves state-of-the-art accuracy on the REAL275 dataset and generalizes to unseen categories.
Our work extends online binary prediction to selecting multiple experts while ensuring incentive compatibility and sublinear regret.
We introduce a latent variational diffusion model that outperforms existing methods in reconstructing theoretical particle kinematics from detector data at the LHC.
"FLSL, a feature-level self-supervised learning method using Vision Transformers, improves dense prediction task performance like object detection and segmentation."
TWIST improves speech language models by warm-starting from pretrained text models.
ScoreOpt is a test-time optimization defense using score-based diffusion models to enhance adversarial robustness and inference speed.
"Skill-It, a data sampling algorithm for language models, improves training efficiency by selecting data in a deliberate skill order."
We propose a weakly-supervised method for concealed object segmentation that uses a multi-scale feature grouping module and leverages SAM to generate training masks from sparse annotations.
ProtoConcepts enhances interpretable image classification by using multiple prototypical patches to clarify conceptual similarities like color or shape.
"Interactive estimation learnability is captured by the Dissimilarity dimension, for which we provide a polynomial-regret algorithm that unifies statistical-query learning and structured bandits."
"Neural functional networks, designed with permutation equivariance, effectively process other networks' weights for tasks like predicting generalization and generating sparsity masks."
Incorporating context-specific independence relations into causal imitation learning renders the feasibility problem NP-hard and necessitates new graphical criteria and algorithms.
pFedHR introduces a personalized federated learning framework that reassembles heterogeneous client models to improve recommendation systems.
We propose two debiasing methods to mitigate view inconsistency in score-distilling text-to-3D generation.
We introduce $\varepsilon$-fractional core-stability for Hedonic Games and provide efficient algorithms for two fundamental classes.
